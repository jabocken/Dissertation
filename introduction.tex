\chapter{Introduction}
% topic/basic motivation
Proving that a non-trivial program has no bugs is not an easy task.
As technology continues to improve, software will continue to increase in complexity.
Providing methods to ease the work of reasoning over programs is a necessity in the modern world.
This is particularly important for programs that are intended for high-reliability applications,
such as avionics, medical equipment, or other safety-critical systems.
Of course, software bugs have existed ever since the creation of non-trivial programs.
For at least as long, researchers have been working on methods of bug-free programming and sound program analysis.

\section{Formal Verification of Software}
Many have dipped into the field of \emph{formal methods} \autocite{butler:fm} to do this.\index{formal!methods}
For our purposes, the subfield of \emph{formal verification}\index{formal!verification} specifically.
Formal verification allows reasoning over programs with a high degree of assurance.
This assurance is provided by sound mathematical representations and logical reasoning.
Even pen-and-paper proofs can provide guarantees that purely informal analysis does not.

% work context
Initial formalisms developed by \textcite{floyd1967assigning,hoare1969axiomatic} have provided a basis for the formal verification of software for over fifty years.
\todo{what more to say here?}

\todo{need another thesis sentence to build up a paragraph for that leads into the challenges?}

% basic challenges
Despite this, formal verification of software is still a difficult task.
Many useful properties are \emph{undecidable}\index{undecidable} \autocite{rice1953classes,horspool1980approach,ouimet2008formal}.
There are no single, fully-automated methods that are guaranteed to determine them for all possible programs.
One alternative is to use \ac{itp} \todo{want a citation here}.
However, that does not scale well due to the amount of intricate user interaction and development involved.

Furthermore, source-code-level formal verification requires a large \emph{\ac{tcb}} \todo{citation?}.
Without a verified compiler \autocite{leroy:compcert}, one must trust the semantics of a binary reflect its original source.
Even then, one must assume that the \ac{isa} in use has been correctly specified, that all libraries work correctly, and even that there are no hardware flaws in the physical computer being used to execute the program.
Ultimately, it is not possible to eliminate the \ac{tcb} completely.
However, it is possible to \emph{reduce} it, and not just via the usage of non-mainstream compilers that can only handle a subset of programs for their given language.

\section{Static Binary Analysis}
As an alternative to top-down software analysis, the field of \emph{binary analysis} has taken root in the research community.
We focus here on \emph{static} binary analysis specifically.
That is, analyzing programs ``offline'' with the usage of external tools rather than instrumenting or hooking into a running program.
This automatically eliminates the compiler and any optimizations it may do from the \ac{tcb}.

% work context
\todo{What are some big/seminal works in (static) binary analysis?}

% basic challenges
\todo{lack of abstraction}

\todo{issues with disassembly}

\section{Motivation}
\todo{paragraph(?) on memory preservation}

\todo{paragraph(?) on control flow recovery}

This dissertation consists of two main contributions: approaches to \emph{memory preservation} in \cref{memory-preservation} and approaches to \emph{control flow recovery} in \cref{hg,eicfg}.
\todo{more?}

\subsection{Memory Preservation}

\subsection{Control Flow Recovery}

\section{Challenges}
The properties described above both suffer from the problem of undecidability\index{undecidable}.

\subsection{Memory Preservation}
\todo{anything more here?}

\subsection{Control Flow Recovery}

\section{Contributions}

\subsection{Control-Flow-Driven Memory Preservation}
This dissertation therefore proposes \emph{semi-automated} approaches.
They use \iac{itp} environment, but with automatic generation to produce as much of the theorem and proof code as possible.

\subsection{Syntax-Driven Memory Preservation}

\subsection{Hoare-Logic-Based Control Flow Recovery}
\subsection{Exceptional Interprocedural Control Flow Recovery}

\section{Summary of Related Work}
\todo{should this come before contributions?}
A full overview can be found in \cref{ch:related}.

In 2014, \textcite{goel2014syscalls,goelphd} produced formal semantics
for most user-mode \gls{arch} instructions as well as for commonly-used system calls.
That work allows mechanized reasoning over compiled programs in the ACL2 theorem prover \autocite{ACL2}.

Soon after, \textcite{tan2015auspice} introduced a logic framework called AUSPICE for automated verification of safety properties on the assembly level.
AUSPICE took six hours to execute on \num{533} instructions, but was applicable to unmodified code.
Our methodology in \cref{ch:syntax} is also applicable to unmodified code, as long as that code is assembly.
The contributions in \cref{hg,eicfg} go even further by being directly usable with existing binaries.

More recently, \textcite{baumann2016high} provided \pgls{arm}[v8]-based hypervisor that was formally verified on the machine code level to ensure isolation of guest \acp{os}.
That work was based on an earlier one for \pgls{arm}[v7] separation kernel, PROSPER \autocite{dam2013hypervisor,dam2013formal}.

Additionally, a few years ago, \textcite{fromherz2019verified} embedded a subset of the \gls{arch} \ac{isa} in the functional, verification-oriented language F$^*$ \autocite{fstar}.
This was done in order to perform a proof of correctness over the commonly-used cryptographic routine AES-GCM.\index{AES-GCM}
Their usage of \iac{vcg} is similar to ours in \cref{ch:syntax}, but ours did not need to be separately formally verified as we implemented it with proven-true Hoare rules.\index{Hoare!rule}

\subsection{Control Flow Recovery}
\todo{still working here to combine old/new text, rewrite as needed too! And add more!}
At the time this work was written, no other tool operating on binaries
%answered the two base questions above.
provided \todo{what?}
The bulk of existing methods are either known to be unsound (they either  misidentify code as data or are underapproximative) \autocite{schwartz2002disassembly} or are speculative or learning-based \autocite{wartell2011differentiating,khadra2016speculative}.
The strength of those tools is their universality: they typically provide output for any binary, even in cases where guesses and non-validated assumptions have to be made.
Their weakness is that their outputs are untrustworthy; thus, any analyses built on top of them are untrustworthy as well.

% EXISTING TOOLS DO NOT SOLVE THE PROBLEM (eicfg)
Existing state-of-the-art disassemblers/decompilers, such as IDA Pro, Ghidra, and Binary Ninja, do not provide sufficient information on exceptional control flow.
Typically, they are able to extract exception information statically available in binaries (e.g., landing pad locations).
They can even provide interprocedural control flow graphs.
However, they do not perform the interprocedural static analysis required for reconstructing \emph{exceptional} control flow.
That is, they cannot trace the path from an exception throw site to the landing pad instructions that exception goes to in the process of unwinding.
\Acp{cfg} under such analyses will have \emph{throw sites as terminating locations} with no outgoing edges.
%Tracing an exception from throw site to catch is not a feature of such tools.
%In order to analyze the stack unwinding caused by an exception being thrown, one must typically utilize a runtime debugger such as GDB.
%In fact, many state-of-the- art disassembly tools, IDA Pro and Ghidra included, rely on GDB to perform dynamic analysis via debugging.
%That is, they require runtime information in order to trace exceptional state.


---------------------------

To the best of our knowledge, we are the first to provide a generator for
\ac{eicfg}-like constructs on the binary level.
Many state-of-the-art disassemblers/decompilers, such as IDA Pro, Ghidra, and Binary Ninja,
can extract exception information from binaries, including landing pad locations.
They can even provide interprocedural control flow graphs.
However, they do not provide features for interprocedural exception analysis.
Tracing an exception from throw site to catch is not a feature of such tools.
In order to analyze the stack unwinding caused by an exception being thrown,
one must typically utilize a runtime debugger such as GDB.
In fact, many state-of-the-art disassembly tools, IDA Pro and Ghidra included,
rely on GDB to perform dynamic analysis via debugging.
That is, they require runtime information in order to trace exceptional state.
In contrast, our \ac{eicfg} generator does not.

\section{Limitations and Scope}

\subsection{Memory Preservation}

\subsection{Control Flow Recovery}

\section{Impact}
\todo{What are the results? What can these contributions be used for? Seems kind of redundant with the contributions subsection though.}

\subsection{Memory Preservation}

\subsection{Control Flow Recovery}





\chapter{Introduction (old)}

To fill the former gap, we introduce the property of \emph{memory preservation}.
This property indicates that the memory a program writes to is bounded by prespecified regions.
To the best of our knowledge, there are currently no state-of-the-art methods that specifically aim to formally verify that property.

Complicating matters, memory preservation cannot be proven fully automatically as it is an undecidable property.
This mandates \emph{interactive} but \emph{scalable} approaches.
It also must be proven at the assembly level as it relies on concrete memory layout.
We therefore provide formal verification methods for real-world programs that are specifically tailored to memory preservation.
This allows more automation and scalability.
For example, we have shown that we can formally verify approximately \num{12000} lines of assembly code obtained by disassembling binaries of the Xen hypervisor \autocite{xen} with minimal user interaction.

To fill the latter gap, we provide binary-level approaches to dealing with complex, interprocedural control flow in the presence of indirect branches and even exceptions.
First, a more general \emph{\ac{hg}} approach, followed by one focused specifically on \emph{exceptional control flow}, \emph{\acp{eicfg}}.
We have found no other approaches that lift binaries to \ac{eicfg}-like constructs.
\Ac{hg} lifting does have a similar work, Jakstab \autocite{kinder2010static,kinder2012alternating,kinder2012virtualization}, but it is targeted at a different set of applications from our work and we found it to be rather limited practically.
Both methods are fully automated and scalable, with the \ac{hg} work lifting \glssymbol{inst-total-lifted} instructions and the \ac{eicfg} work \glssymbol{eicfg-inst-total}.

\section{Motivation for Memory Preservation}
As a basic property, memory preservation has potential applications to security analyses, compositional reasoning, and even concurrency.
These potential applications are described in more detail below.

\subsection{Security}
Unbounded memory usage can lead to vulnerabilities
such as buffer overflows and data leakage.
One example of such a vulnerability would be 2014's Heartbleed \autocite{heartbleed}.
Heartbleed was caused by a lack of bounds checking on a string array
requested as output as part of a ``heartbeat'' message.
This, combined with a custom memory manager
that also had no security protections against out-of-bounds memory accesses,
lead to potential leakage of sensitive data such as passwords and encryption keys.
Memory preservation could serve as a foundation for formal security analyses
that could be used to expose vulnerabilities involving malicious writes.

Another important property that memory preservation could help with
is \ac{cfi}. \Ac{cfi} ensures that software execution
follows a predetermined \ac{cfg} using static analysis and runtime checks.
At a minimum, this requires proving that a program cannot overwrite its stack pointer
or that a called function does not overwrite local variables of its caller.
In other words, it must be proven that the memory writes of a program
are confined to prespecified regions, which is exactly what memory preservation states.
This can aid in avoiding \ac{rop} attacks without excessive runtime overhead.

The property of \emph{noninterference} is also a useful one for security.
On a high level, it states that a group of users using a certain set of commands
\emph{does not interfere} with another group of users if the the first group's actions
have no effect on what the second group of users can see
\autocite{goguen1982security,rushby1992noninterference}.
On a functional level, that could be interpreted as a statement that a non-interfering function does not modify any memory that is accessed
by the function not being interfered with.
Memory preservation is specifically about showing that all memory outside of
specific regions is not modified by the function
or functions associated with those regions, so proving that the region sets
for two functions are disjoint
would essentially prove noninterference for those two functions.\footnote{%
  A weaker property would be showing that one of the functions does not write
  to any of the memory regions read by the other, but that would actually be harder to prove
  as we do not currently differentiate between regions that are read and written.%
}

\subsection{Composition}\label{sse:composition}
Scalability in verification is only feasible with composition;
proofs of functional correctness or some other property over a large suite of software
require decomposing that suite into manageable chunks.
Separation logic provides a \emph{frame rule} that supports such%
\index{separation logic!frame rule}
decomposition \autocite{o2001local,reynolds2002separation,krebbers2017essence}.
In words, the frame rule states that,
if a program or program fragment can be confined to a certain part of a state,
properties of that program or program fragment carry over
when used as part of a larger system involving that state.
Memory preservation allows for discharging the most involved part of the frame rule,
at least in terms of individual assembly functions.
That is, it shows that the memory preservation of those functions is constrained
to specific regions in memory.
This could then serve as a basis
for a larger proof effort over multi-function assembly programs.

\subsection{Concurrency}
Reasoning over concurrent programs is complicated
due to the potential interactions between threads.
While there are ways of handling such interactions in a structured manner
via kernel- or library-provided \ac{ipc},
one method commonly used for the sake of efficiency is \emph{shared memory}.
Shared memory, in the context of this work,
refers to threads or processes sharing either a full memory space
or portions of one (via memory mapping)
that can be written to and read from freely by any thread or process with access to it.
Usage of shared memory can result in \emph{unintended} interactions between threads.
Memory preservation could be adapted to show the absence of such interactions
by proving that multiple threads only write
to specifically-allowed regions of shared memory.
Doing so would, of course, require a proper model of concurrency,
which is out of scope of this dissertation.

\section{Motivation for Trustworthy Control Flow Lifting}
Every analysis or transformation technique applicable to binaries, whether it be decompilation \autocite{brumley2013native,dinaburg2014mcsema}, binary verification \autocite{goelphd,brumley2011bap,tan2015auspice}, binary patching \autocite{wartell2012binary,kim2017revarm}, or security analysis \autocite{kruegel2005automating,song2008bitblaze,davi2009dynamic,wang2017angr}, needs to start with some form of binary \emph{lifting}.
That is, raw unstructured data needs to be lifted to a form where one can reason over behavior and semantics.
Typically, binary lifting requires answering \emph{at least} the following base questions:
\begin{description}
  \item[Disassembly] \emph{What} instructions are potentially executed within the binary?
  \item[Control Flow Recovery] \emph{In what order} can these instructions be executed?
\end{description}
Those two questions are mutually recursive; they cannot be isolated from each other.
Once analyzing more than one instruction, disassembly\footnote{%
  Specifically for our case, recursive descent disassembly.%
}
requires knowledge of which instructions are to be executed next.
Such information is not necessarily statically available. For example, jump targets may need to be dynamically computed, the stored return addresses for \inlineasm{ret} instructions change based on context, and even the bounds on jump table indices themselves may not be fixed.
Determining the target of a \inlineasm{ret} can be non-trivial even when the call graph is nominally static as well, because there is always the possibility of an instruction within the returning function having overwritten the return address.\footnote{When used purposely, this is the core of the previously-mentioned \ac{rop}.}

Overapproximativity is necessary in order to provide complete coverage.
Alternate methods of uncovering exceptional state generally rely on unsound heuristics

Interprocedural analysis is necessary as exception handling is inherently interprocedural. While

\todo{needs more work}

The information necessary for stack unwinding can be embedded within a program.
Debugging builds have it to provide support for unwinding during debugging.
Programs with structured exception handling require it even when debugging information is excluded.
However, even when such information exists, its support for interprocedural
analysis applies solely to the act of stack unwinding.
Investigating the behavior of exception handling in such scenarios requires the implementation of handling for multiple runtime library functions.
These include \inlineasm{__cxa_throw}, \inlineasm{__cxa_begin_catch},
and \inlineasm{__cxa_end_catch}.

\section{Challenges}
Here are some basic challenges for the concepts presented by my dissertation.

\subsection{Control Flow Lifting}\label{challenges-cfg}
Because of the potentially dynamic nature of a program's control flow, determining where to go next requires knowing how the program got there.
This produces the ``chicken-and-egg'' problem of disassembly \autocite{schwartz2002disassembly}.
At a minimum, a disassembler that supports call and indirect jump traversal needs to ensure the following properties:
\begin{description}
  \item[Return Address Integrity] Functions cannot overwrite their own return addresses.\footnote{assuming a standard structured programming methodology} This requires the absence of stack overflows or similar inappropriate stack manipulation.
  \item[Bounded Control Flow] All indirect, or non-immediate, branches transfer control flow to fixed, statically-calculated, bounded sets of addresses. This requires the ability to determine upper bounds on array indices.
  \item[Calling Convention Adherence] All called functions properly restore the set of registers the 64-bit System~V \ac{abi} considers non-volatile.
\end{description}

% CHALLENGE: EXCEPTIONAL CONTROL FLOW
This problem is exacerbated when dealing with \emph{exceptional control flow}, induced mainly by the \Cpp{} \lstinline{throw} and \lstinline{catch} statements.
The target of a throw, i.e., to which instruction address the control flow is transferred after execution of a throw statement, is decided dynamically at runtime.
It is based on, among other things, the current call/return stack, the current caught exception stack, and low-level information pertaining to rethrows and catch statements.
Moreover, it inherently requires \emph{interprocedural} analysis, as the function call history is relevant for assessing the throws’ targets.
Interprocedural analysis is challenging, as one cannot simply isolate a function and do analysis, but most consider the binary as a whole.

--------------------------------

Static program analysis is a difficult task, made even harder when the well-established
stack-based function call paradigm is not respected. In particular,
exception handling often includes violations of program control flow not visible
from within the source code or binary of the program.
Even structured exception handling, such as that provided by \Cpp,
requires information external to the binary in order to properly analyze it.
Modeling this information requires, at a minimum,
semantics for the library functions that perform the exception handling process.

\section{Assembly-Level Verification}
Properties that reason over the concrete memory used by a program,
such as memory preservation, cannot be satisfactorily expressed on the source-code level.
This is because even programs in a relatively low-level language like C
have abstractions on memory for local variables and function calls.
How and where that memory is allocated may be compiler, \ac{abi}, and \ac{isa}-specific.
It can even depend on what compiler options are in use,
including the level of optimization.
While one way of resolving that issue would be to choose a specific compiler
and provide a formal analysis of how it arranges memory (or write a compiler to do so),
that method places restrictions on the build process.
Targeting assembly or machine code directly, as done in this dissertation,
allows bypassing the build process,
which also opens the door for verification of legacy code.
\begin{example}\label{ex:rop}
  As a further illustration, consider formulating a property
  that a function cannot overwrite its own return address.
  Doing so would require knowledge of the layout of the stack,
  including the values of the stack and frame pointers,
  thus making it an \emph{assembly-level} property.
\end{example}
As a side benefit,
targeting assembly means that there is no need to trust all the steps between
writing source code and obtaining a binary from it.
Doing so reduces the \ac{tcb} without needing to use a compiler
that has been formally proven to maintain
the semantics of the source code in the binaries it produces.

\subsection{Challenges}\label{asm_challenges}
The biggest challenge in assembly-level verification is
the semantic gap between compiled and source code.
Higher-level languages hide details of their implementation
behind layers of abstraction, which makes it easier to reason about them on that level
but makes it harder to formally show equivalence with the semantics of
to lower abstraction levels.
Meanwhile, assembly languages are close to direct interfaces
with their corresponding \acp{isa},
having minimal differences in semantics but not being easy to reason about directly.

As an example of the semantic gap,
assembly code generally lacks the structured control flow
found in languages on a higher level of extraction.
Instead, all control flow on the assembly level is performed using conditional
or unconditional branches, either to a predetermined location or to a calculated label.

A further example would be source code containing division operations
being compiled to run on a processor that does not provide hardware division.
Many \acp{cpu} for embedded systems lack support for hardware division
as efficient division algorithms require a lot of circuitry.
For such processors, runtime division must be calculated using an algorithm
implemented in assembly rather than via a specific instruction.

Even the basic concept of numeric types is minimal on the assembly level,
much less more abstract data types like lists or trees.
While most \acp{isa} do have different instructions
for signed versus unsigned integer arithmetic,
as well as distinct instructions for floating-point operations,
individual values in memory have no type.
They are merely lists of bytes starting at some address,
and even the number of bytes and the address to read from or write to can be variable.
A user could go as far as supplying the result of a floating-point computation
as the address operand of an instruction that loads or stores memory.
Historically, there have been computers that associated type information
with memory locations in hardware
\autocite{feustel1972rice,feustel1973advantages,thornton2008rice},
but we do not have that luxury on typical modern systems.

An additional issue with assembly,
and the one most significant for this dissertation,
lies in the simplicity of the user-exposed memory model.
The vast majority of high-level, structured languages with scoping
prevent function calls from accessing the local variables of other calls
without significant effort or explicit notation, but the same is not true for assembly.
An assembly instruction that operates on memory can refer to any
address within range of its address operands even if it is not supposed to.
Most modern \acp{isa} do provide some form of memory protection,
but those generally rely on runtime detection of invalid accesses
and are often not fine-grained enough for reasoning about individual stack frames
or local variables.
Any verification effort that wishes to reason about low-level memory properties
must provide its own abstractions and assumptions on layout.

\section{Control Flow Recovery}
To account for some of those issues\todo{rewording!}, we here provide an approach to \emph{trustworthy binary lifting} that simultaneously performs
\begin{enumerate}
  \item disassembly,
  \item control-flow recovery, and
  \item generation of proofs that are required to provide \emph{assurance} of the output.
\end{enumerate}
Due to the aforementioned undecidability, our approach is not universal.
It may fail on certain binaries or need to annotate certain instructions
with unsoundness warnings.
However, our approach provides \emph{assurance} that \emph{if} unannotated output is produced, that output is a sound representation of the binary.
To the best of our knowledge, at the time of writing,
\emph{no existing work could provide scalable, formally overapproximative
  assurance between a binary and its lifted representation.}

\subsection{Assumptions and Scope}\label{hg-assumptions}
In order to reduce state space complexity and provide scalability, we did reduce our complexity.
Specifically, we:
\begin{enumerate}
  \item target potentially stripped \ac{cots} \gls{arch} \ac{elf} binaries compiled with various levels of optimization,
  \item do not deal with multithreaded code,
  \item do not deal with destructors executed after an exit, and
  \item assume that all memory regions accessed by the binary are either aliasing, separate or enclosed \autocite{balakrishnan2004analyzing,balakrishnan2005codesurfer}.
\end{enumerate}
In general, we assume that the per-instruction semantics in use and the state changes they express are sound
(for example, semantics that have been machine-learned from actual hardware \autocite{heule2016stratified,roessle2019formally}).
We also assume the existence of \pgls{fetch} function that, given an address, soundly retrieves the corresponding instruction from the binary.
\todo{Maybe add a discussion of this to the conclusion, as we did in fact encounter a bug with this in the implementation used for HGs requiring changing to a different library.}
Experimental results show that the majority of unsoundness annotations concern function callbacks.
In order to gain scalability, we treat function calls as context free.
That allows us to reduce the state space and also reuse previous executions.
However, it also means that if a function pointer is passed as a parameter, its concrete value will be unknown.

External functions also require some assumptions.
Specifically, we assume that external calls properly follow the System~V \ac{abi} and do not interfere with their parent stack frames.
This means that those registers considered volatile are invalidated when we encounter such calls, but we do not have to invalidate most other components of the state.
An in-depth exploration of function call handling comes later in \cref{function-call-extension}.

Finally, to deal with the final assumption regarding aliasing/separation/enclosure, we again utilize overapproximation.
In cases where we cannot obtain clean arrangements of regions and would instead require overlapping ones, we merely invalidate the regions under consideration.
This ensures that the result of accessing such regions will just be \gls{bot}, allowing us to avoid engaging in generating innumerable states for all possible overlapping arrangements.
From our experiments, this did not result in significant loss of the information necessary to successfully complete our analyses.


\section{Contributions}
This dissertation presents two formal approaches to per-function verification
of the assembly-level property we call memory preservation:
\emph{control-flow-driven} verification and \emph{syntax-driven} verification.
Both approaches use some form of control-flow analysis over functions in \gls{arch} assembly to generate incomplete proofs.
Those proofs are then loaded into the interactive theorem prover Isabelle/HOL
and completed there. The proof strategies for both approaches involve
\emph{symbolic execution} of the underlying assembly code \autocite{king1976symbolic}, albeit in different ways.

The main differences between the two approaches
lie in their degrees of automation, the strengths of their invariants,
and how they perform symbolic execution.
The first approach, control-flow-driven verification,
requires significantly more user user input but has the potential for much stronger invariants.
Meanwhile, the second approach, syntax-driven verification,
has a significantly higher level of proof automation via the generation of \acp{fmuc}
but is not as suited for stronger invariant production.
Symbolic execution is also more efficient in the control-flow-driven approach
as it more closely follows the structure of the function's \ac{cfg}.
In contrast, the syntax-driven approach must deal with
operating on a restricted set of control flow constructs,
which can result in extra symbolic execution.

There are also two approaches to control flow lifting: \ac{hg} generation and \ac{eicfg} generation.
Both approaches \todo{some more here}

\subsection{Control-Flow-Driven Verification}
This methodology for verification of memory preservation relies on treating function bodies
as \acp{cfg} with basic blocks as the nodes,
much as compilers do when performing their analyses.
In order to reason about the \acp{cfg},
they are annotated with predicates on state at specific locations,
between which the program will be symbolically executed.
While it is possible to reason about full functional correctness with this methodology,
doing so takes a significant amount of effort due to the very low level of abstraction
assembly provides, even with proven-correct formal simplification rules in Isabelle.
Because of this, we focused on the aforementioned property of memory preservation.

%Formal Definition of Memory Preservation
In our model, memory usage is formulated as a set of \emph{regions}
that start at some address and have a specific size in bytes.
We do not currently differentiate between regions for writes and regions for reads,
though doing so is a possibility in the future.
Proving memory preservation requires performing symbolic execution
on the underlying assembly instructions
and showing that no regions beyond those needed to complete the proof are modified.

%Semi-Automated Formal Verification for Memory Preservation
In order to reason about that memory usage
so we can prove memory preservation in a theorem prover,
the structure of the proof must be extracted from the assembly programs.
For that purpose,
our code generation tool for this work produces the skeleton of a proof
based on the control flow of the analyzed programs.
This is achieved using off-the-shelf tools.

That proof skeleon specifies where the program should be annotated
and provides some initial conditions based on register values.
It also provides the proof steps to properly perform symbolic execution
and starts the user off with a basic set of regions
determined from variables in the stack frame.
The two steps remaining, however, are up to the user.
Those steps are formulating any remaining memory regions
to successfully complete symbolic execution
and fleshing out the annotations on state so that the symbolic execution of later blocks
can continue from that of earlier ones.

%Analysis of HermitCore Functions
The control-flow-driven methodology was applied to \num{63} functions
extracted from the HermitCore  \autocite{lankes2016hermitcore}
unikernel library \autocite{madhavapeddy2014unikernels},
covering \num{760} \ac{sloc} or over \num{2379} assembly instructions.
Of those functions,~\num{18} had loops and~\num{33} had subcalls.
Optimized variants were also verified for~\num{12} of the functions involved,
resulting in \num{75} functions verified.
There was even one function that featured recursion,
which turned out to be the most challenging function to handle.
Other than the recursive function, the most challenging ones to handle
were the ones with loops. Formulating annotations that must hold for all loop iterations
is not easy when a significant amount of memory operations are performed.

The closest related work to this, that of \textcite{matthews2006verification},
resulted in the verification of only \num{20} functions,
with \num{631} assembly-level instructions in total.
That is only \SI{26.67}{\percent} of the functions,
or under \SI{26.5}{\percent} of the instructions, that we verified here.
On top of that, the \acp{isa} they worked with are not as low-level as the \gls{arch} \ac{isa}.
While they verified functional correctness
instead of a weaker property like memory preservation,
they also specifically reduced the complexity
of the most complicated set of functions they verified
by using a simple \inlineasm{xor} cipher instead of a proper block cipher.

\subsection{Syntax-Driven Verification}
Taking our experiences from the control-flow-driven verification work into account,
we chose a slightly different path for the second verification work
presented in this dissertation.
This approach focuses on relating symbolically-executed basic blocks
with a syntactic representation of program control flow.
It also involves significantly more information generation
than the previously-discussed approach.

%Mostly-Automated Formal Verification for Memory Preservation
Abstracting away from the concrete control flow to a more structured syntax
increases the capacity for automation
as it allows for the development of a set of \emph{Hoare rules}
over the syntactic control flow \autocite{hoare1969axiomatic}.
By developing and using a set of such formal rules, we were able to restrict symbolic execution
to the level of individual basic blocks and then use those rules to do the rest of the work.
This greatly simplified our proof strategies for proving memory preservation.

The change in methodology alone would not have been enough, however.
As stated, we also generate much more information.
That additional information consists of the full set of memory regions
for each basic block, the corresponding \acp{mrr},
and the block's preconditions and postconditions.
Having that information generated for them greatly reduces the work an end user
must put in compared to our initial approach.

%Analysis of Xen Binaries
Unlike the previous work, this one was applied to assembly obtained
by running \texttt{objdump} on three \emph{unmodified} binaries resulting from the
Xen Project hypervisor build process \autocite{chisnall2008definitive}.
Of the \num{352} functions present in those binaries,
\num{251} or \xenpercentage\ were verified.
Ultimately, over \num{12252} optimized instructions were covered
with only \num{1047} manual lines of proof required.
That is an approximate ratio of one manual line of proof
for every \num{12} instructions handled,
or an average of \num{16} manual lines of proof for every loop handled,
of which there were \num{65}.

To the best of my knowledge, this is the first work to achieve
that degree of coverage for optimized \gls{arch} binaries produced by production code.
While \textcite{tan2015auspice} produced a fully-automated methodology
for binary analysis, it was much slower than our approach here,
meaning they would take longer to cover the same amount of functions
even though they had more automation.
Under normal circumstances, this approach can complete the proofs for two functions
with a total of \num{97} assembly instructions in less than ten minutes.
That is \SI{9.7}{insts\per\minute} compared to \SI{1.48}{insts\per\minute}
for AUSPICE, \num{6.55} times as fast. We did have some functions that took
an overly long period of time due to the suboptimality of \acl*{scf}
with respect to minimizing symbolic execution, but those were atypical.

\subsection{Hoare Graphs}
The core contribution of this \namecref{hg} is, again, an algorithm (and implementation) for extracting \iac{hg} from \pgls{arch} binary.
The vertices of that \ac{hg} are symbolic states that consist of the following:
\begin{enumerate}
  \item \emph{predicates} containing information on registers, memory locations and flags and
  \item \emph{memory models} that provide pointer aliasing information.
\end{enumerate}
Each \ac{hg} edge is labeled with the corresponding assembly instruction.
Our key intuition here is that those edges are \todo{one-step?} \emph{inductive}:
each edge forms a Hoare triple \autocite{hoare1969axiomatic},
much like those we have discussed before.
Every vertex/state contains enough information to prove that its outgoing edges are overapproximative,
even in the case of non-trivial control flow.
This includes indirect branches, jump tables, and function calls/returns.

Importantly, an overapproximative relation will model not just the ``normal'' behavior, but also any ``weird'' behavior \autocite{shapiro2013weird,dullien2017weird}.
Normal behavior consists of the intended and expected control flow of the program.
``Weird'' behavior is a term of art indicating control-flow transfers not intended by the program designers.
In particular, it is behavior that results because of lower-level semantics providing more functionality than more restricted higher-level abstractions.
\Cref{hg-example} below contains an example where instructions are potentially overlapping, which is often an arrangement found in obfuscated code.
That example exhibits a \iac{rop} gadget that depends on whether two pointers alias or not, with the aliasing case resulting in an unexpected \inlineasm{ret} being executed. That aspect of the example is discussed further in \cref{weird}.

The soundness of this overapproximation is shown with pen-and-paper proofs in \cref{ch:hg-formulation}.

\subsection{Exceptional Interprocedural Control Flow Graphs}
This paper provides a tool for static, interprocedural, automated \Cpp-exception-aware control flow analysis on the binary level.
That tool produces \acp{eicfg}, that document the direct exception-handling-related components of the state of the program, such as exception objects currently allocated, number of uncaught exceptions, and which exceptions are currently in a caught state.


The tool itself targets binaries using \Cpp\ exception handling%
\fturl{https://itanium-cxx-abi.github.io/cxx-abi/abi-eh.html}
that were compiled for the \gls{arch} \ac{isa} and System~V \ac{abi}.
Symbol-stripped binaries are supported, but may not perform well if they contain unresolvable indirections or unmodeled callbacks.
Non-\Cpp\ binaries, or \Cpp\ ones that do not use exception handling, are supported as well for flexibility.
We assume that all external calls do not produce exceptions themselves.
We also do not model \lstinline{setjmp}/\lstinline{longjmp}, as they do not interact well with structured exception handling \autocite{using-setjmp-and-longjmp}.

% VALIDATION
A desired characteristic is that the produced \acp{eicfg} are \emph{overapproximative}: every concrete path, i.e., every path possible during dynamic execution, is included in the \ac{eicfg}.
We informally argue that the abstract semantics that are symbolically executed overapproximate the concrete semantics \autocite{cousot1996abstract}.
To strengthen this claim, we validated our abstract transition rules for exception handling against the concrete implementations of the corresponding library functions.
This was done by fuzzing abstract states, concretization to a concrete CPU state, running the function under validation in an instrumented binary, observing the CPU state, and verifying that the concrete transition is correctly contained in the abstract transition.
%This was done by \emph{fuzzing} dynamically instrumented test binaries using generated abstract start and end states.
We informally argue that our approach is overapproximative unless an indirection cannot be resolved, which is clearly marked in the \ac{eicfg}.
%The methodology is informally based on abstract interpretation \autocite{cousot1996abstract}, where an abstract transition relation overapproximates concrete transitions.

% RESULTS
We applied the tool to \totalbins\ off-the-shelf binaries compiled from \Cpp, C, and Fortran source code.
%The implemented tool was able to reach an average of \coveredpercent\ of the instructions from the \satisfactorybins\ binaries we examined that had satisfactory analyses (for our purposes, \cutoffpercent).
The implemented tool was able to identify \uniquethrows\ unique throws and successfully trace the exceptional control flow for every one of them.
On average, dealing with exceptional control flow can increase coverage by \avgdiffinst\ per unique throw, with each throw averaging \avgunwinds\ unwind edges.
Those edges are ones tools such as Ghidra do not produce.

-------

  This work provides a tool for static, interprocedural,
\Cpp-exception-aware control flow analysis.
Specifically, the production of \acp{eicfg}.
It can do this even in the presence of recursion and some forms of indirect control flow.
The tool is guaranteed to provide overapproximative results for those paths of execution it can resolve, with those paths it cannot resolve clearly marked in its output.
The resulting \ac{eicfg} documents the direct exception-handling-related components of the state of the program, such as exception objects currently allocated, number of uncaught exceptions, and which exceptions are currently in a caught state.
This is done using an abstract interpretation-based methodology in a restricted domain that only concerns itself with the necessary components of program state required to perform function calls and exception handling.

\todo\dots

It operates using a recursive-descent rather than linear sweep methodology.


\section{Organization of Dissertation}
Following this introduction in \cref{ch:related} is a review of tools and work related to the field of assembly-level verification, control-flow lifting, and software correctness in general.
Domain-specific information necessary to understand the work and terminology can then be found in \cref{ch:background}.
For an in-depth exploration of the basis for the symbolic execution engines and formal memory reasoning used by the contributions of this work,
see \cref{ch:symbolic_execution}.

After that, the control-flow-driven approach to verification of memory preservation mentioned above is presented in \cref{ch:cfg}
while the syntax-driven approach is presented in \cref{ch:syntax}.
I then have the aforementioned work for the lifting of formal control flow graphs in \cref{hg}
followed by the interprocedural, exception-aware \ac{cfg} extractor in \cref{eicfg}.
Finally, my dissertation wraps up in \cref{ch:conclusions}.
