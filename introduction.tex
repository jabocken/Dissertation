\chapter{Introduction}
% TODO?

The approaches to assembly-level verification detailed in this dissertation%
\index{assembly!verification}
are \emph{semi-automated}.
They combine \ac{itp} with automated generation
of at least some components of formal proofs.
This reduces the amount of manual proof effort required by varying degrees
while still providing flexibility in the proving process.

%TODO

As per Rice's theorem~\citep{rice1953classes},
the non-trivial memory usage-related properties presented here are undecidable.
This is why the approaches presented in this dissertation
use \iac{itp} environment, allowing user interaction when necessary.

\section{Levels of Verification}

\section{Challenges of Assembly-Level Verification}
The biggest challenge of assembly-level verification is the lack of abstraction.
Higher-level languages hide details of their implementation
behind layers of abstraction, which makes it easier to reason about them
on that level.

\section{Memory Usage}\label{memory_usage}
The main property targeted for verification in this dissertation
is referred to here as \emph{memory usage}.%
\index{memory!usage}
It characterizes the parts of memory that are read and written by a program.


\todo\dots



\subsection{Memory Regions}\label{memory_regions}
To reason about memory usage, we must have some way of describing it.
For this purpose, we have the concept of \emph{memory regions}.%
\index{memory!region}
A memory region $\region{a}{s}$ is defined to have type $W\times\nat$;%
\nomenclature{$W$}{Type of 64-bit words}%
\nomenclature{$\nat$}{Type of natural numbers}
that is, its starting address~$a$ is a 64-bit word
and its size in bytes~$s$ is a natural number.

\todo\dots

\subsection{Memory Preservation}
An application of memory usage analysis,
memory preservation shows that the values written by a program%
\index{memory!preservation}
are constrained to specified regions in memory.
Those regions cannot be fully identified when working with source code alone,
particularly when the end result is optimized.
Memory may be laid out differently depending on the \ac{isa} and \ac{abi} targeted,
as well as on the compiler used.
This can include positioning of global variables
as well as the layout of stack frames.\index{stack!frame}
While one way of resolving that issue would be to choose a specific compiler
and provide a formal analysis of how it arranges memory, that method is not flexible.
It may instead be better to target assembly or machine code directly,
as done in this dissertation.

\subsubsection{Usefulness}
The following paragraphs elaborate on the usefulness of memory preservation
as a platform for further verification efforts.

\paragraph{Security.}
Unbounded memory usage can lead to vulnerabilities
such as buffer overflows and data leakage.
One example of such a vulnerability would be 2014's Heartbleed~\citep{heartbleed}.
Heartbleed was caused by a lack of bounds checking on a string array
requested as output as part of a ``heartbeat'' message.
This, combined with a custom memory manager
that also had no security protections against out-of-bounds memory accesses,
lead to potential leakage of sensitive data such as passwords and encryption keys.
% TODO: need another, better example that involves data modification too
Memory preservation could serve as a foundation for formal security analyses
that could be used to expose vulnerabilities involving malicious writes.

\paragraph{Composition.}\label{sse:composition}
Scalability in verification is only feasible with composition.
Proofs of functional correctness over a large suite of software
require decomposing that suite into manageable chunks.
Separation logic provides a \emph{frame rule} that supports such%
\index{separation logic}%
\index{separation logic!frame rule}
decomposition~\citep{o2001local,reynolds2002separation,krebbers2017essence}.
In words, the frame rule states that,
if a program or program fragment can be confined to a certain part of a state,
properties of that program or program fragment carry over
when used as part of a larger system involving that state.
Memory preservation allows for discharging the most involved part of the frame rule,
at least in terms of individual assembly functions.
That is, it shows that the memory usage of those functions is constrained
to specific regions in memory.
This can then serve as a basis
for any larger proof effort over multifunction assembly programs.

\paragraph{Concurrency.}
Reasoning over concurrent programs is complicated
due to the potential interactions between threads.
While there are ways of handling such interactions in a structured manner
via kernel- or library-provided \ac{ipc},
one method commonly used for the sake of efficiency is \emph{shared memory}.
Shared memory, in the context of this work,
refers to threads or processes sharing either a full memory space
or portions of one (via memory mapping)
that can be written to and read from freely by any thread or process with access to it.
Usage of shared memory can result in \emph{unintended} interactions between threads.
Memory preservation could be adapted to show the absence of such interactions
by proving that multiple threads only write
to specifically-allowed regions of shared memory.
Doing so would, of course, require a proper model of concurrency,
which is out of scope of this dissertation.

\subsection{Overapproximation}\label{mem_use_over}
As a formal property, memory usage has been proven to never miss any memory regions
written to, assuming the correctness of the semantics and model it is applied
to~\citep{bockenek2019preservation,popl2019underreview}.
Put another way, however,
this means that the methodology \emph{must} be conservative.
If it cannot make a determination about the usage status of some part of memory,
either due to an underdeveloped state or too large of one to easily reason about,
it must assume that that region is used. It must \emph{overapproximate}.%
\index{overapproximation}
This sort of false positive can be an issue in the field of formal verification,
as it can make the property under consideration weaker despite being correct.

One way to shrink such overapproximations is to increase the
\emph{context sensitivity}\index{context sensitivity}
of the approach, such as performing the analysis over the full program at once
rather than individual components, but that can involve
a significant increase in time and verification effort.

\todo{Want talk about the usage of SMT solvers/etc.,
  increase in automation in general
  as one way of mitigating increased verification effort somewhere}

\section{Summary}
