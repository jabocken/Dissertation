\chapter{Control-Flow-Driven Verification}\label{ch:cfg}
As described in the preface,
my two main contributions to the memory preservation work presented here
were the development of a tool for producing proof skeletons
for memory preservation, interfacing with the analysis tool \texttt{angr},
and the development of structured proof strategies
to flesh out and verify those skeleton proofs (\cref{se:cfg_overview}),
along with the development of guidelines
for invariants that provide for function-level composition
(\cref{se:cfg_composition}).

\todo{describe layout? move stuff from overview maybe}

\section{Overview}\label{se:cfg_overview}
The memory usage analysis approach presented in this chapter
features a Floyd-style methodology as described in \cref{se:cfg_invariant}.
This methodology allows for minimization of symbolic execution%
\index{symbolic execution} while still ensuring correctness.
It also provides proper loop handling.

An overview of the methodology can be seen in \cref{fig:cfg_overview}.
The main goal of the approach is to prove the property of \emph{memory preservation},%
\index{memory!preservation}
fully described in \cref{se:memory_preservation}.
In short, this property ensures only documented regions of memory are modified;%
\index{memory!region}
anything outside of those regions remains unchanged.
This property has the potential to be used
to prove the absence of common memory-related issues,
such as buffer overflows or some forms of data leakage.

By building on the binary analysis tool angr~\citep{shoshitaishvili2016state},%
\index{angr}
the work of abstracting from binary to \ac{cfg} is handled with minimal user input.
The first step is disassembly of an x86-64 binary using%
\index{x86-64}
a tweaked version of the angr
\lstinline|reassembly| analysis~\citep{wang2017ramblr}
provided by \citet{roessle2019}.%
\index{angr!reassembly}
More detail on why this specific disassembler was chosen
can be found previously in \cref{sse:loading_assembly}; in short,
the Isabelle parser relies on specific size annotations the tweaked analysis generates.

To achieve minimal symbolic execution,
it features automatically-selected \emph{cutpoints}%
\index{cutpoint}
identified by a Python script that relies on
angr's \lstinline|CFGEmulated| control flow analysis.%
\index{angr!CFGEmulated}
The cutpoints are described in \cref{se:cfg_invariant}.
Basic starting predicates for those preconditions and postconditions%
\index{precondition}%
\index{postcondition}
as well as the cutpoints are generated,
but the bulk of the information must be added manually.
Larger-scale scalability is achieved by using function-level compositionality.
Even recursive functions are supported, albeit with a caveat.%
\index{recursion}

The caveat on recursion is that it requires a significant amount of work,
much greater than that needed for loops or function calls alone.
Recursive functions are equivalent to one or more loops that utilize a stack,
which significantly complicates the necessary invariants
due to the fact that program stacks, when used with most modern \acp{isa},
are stored in regular memory.
Even when recursive code primarily uses registers
to hold most of the intermediate values and local variables,
those locals must be stored on the stack with each recursive call%
\index{stack}
and potentially restored on return.
Tail call recursion~\citep{clinger1998tail} can help with that,%
\index{recursion!tail call}
but only when it is supported by the compiler in use\footnote{%
  \acs{gcc}, Clang, and \acs{icc} all support it
  via the \texttt{-foptimize-sibling-calls} flag%
} and is applicable to the functions under consideration
(which is not the case for the examples presented in this paper).

The process that selects cutpoints%
\index{cutpoint}
also generates skeleton memory preservation theories for every function analyzed.
The theory files can then be opened in the theorem prover Isabelle
and the assembly loaded using the methodology described in \cref{sse:loading_assembly}.
Once that is done, a user can flesh out the invariants (\cref{se:cfg_invariant})%
\index{Floyd!invariant}
and add the necessary sets of memory regions%
\index{memory!region}
that the functions write to (\cref{memory_regions})
in order to complete the proofs of memory preservation.%
\index{memory!preservation}
Defining the necessary invariants for functions with complex control flow
is generally a hard task,
but targeting a property such as memory preservation
does reduce the amount of work required
as seen in \cref{se:cfg_examples,se:cfg_application}.
The work is still not trivial, however.

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance=4.5cm, auto, ->, >=stealth, myblock/.style={
    draw,
    text width=2.5cm,
    text centered,
    rounded corners,
    minimum height=2cm
  }]
    \node[myblock] (binary) {x86-64 binary};
    \node[myblock, right of=binary, minimum height=1cm] (cfg) {Python package};
    \node[myblock, right of=cfg] (isa) {Isabelle};
    \node[myblock, right of=isa] (mem) {Formal Proof of Memory Preservation};

    \draw[transform canvas={yshift=1.5em}] (binary) -- node[above] {\footnotesize disassemble} (isa);
    \draw (binary) -- (cfg);
    \draw[transform canvas={yshift=-.5em}] (cfg) -- node[above] {\footnotesize generate} node[below] {\footnotesize theories}(isa);
    \draw[transform canvas={yshift=1em}] (isa) -- node[above] {\footnotesize invariant} (mem);
    \draw[transform canvas={yshift=-1em}] (isa) -- node[above] {\footnotesize regions} (mem);
  \end{tikzpicture}
  \caption{Overview of methodology}\label{fig:cfg_overview}
\end{figure}

This methodology was applied to two example functions
as well as to multiple functions
from the HermitCore unikernel library~\citep{lankes2016hermitcore}.%
\index{HermitCore}%
\index{unikernel}
HermitCore is \iac{os} kernel library aiming to provide real-time guarantees for high-performance computing.
Documentation of the two example functions can be found in \cref{se:cfg_examples},
while the HermitCore function work can be found in \cref{se:cfg_application}.
These functions have a variety of features such as loops,%
\index{loop}
non-trivial data structures, pointers, subcalls, and even recursion in some cases.%
\index{function!call}%
\index{recursion}

\section{Formal Definition}
A formal definition of memory preservation requires a formal basis to work with,
and this definition started with the machine model from \cref{se:machine_model}.

\subsection{Symbolic Execution for \acs*{cfg}-Driven Verification}\label{cfg_symb_exec}
While \cref{ch:symbolic_execution} provided a general overview of symbolic execution,
this chapter requires a more specific look.
The step function for this methodology takes the form
$\step:A\times S\rightarrow(S\mid\bot_E)$.%
\nomenclature{$A$}{Type of assembly instructions}%
\nomenclature{$S$}{Type representing program state; an Isabelle record}%
\nomenclature{$\bot_E$}{Indicates exceptional state}
It requires a tuple of instruction to execute and current state~$\sigma$,
returning the state~$\sigma'$ after execution of that instruction.
If some sort of exception, such as a divide by zero, occurs,
the function returns $\bot_E$ instead.

From the machine model, we manually derive a run function%
\index{symbolic execution!run function}
$\run:(S\rightarrow\mathbb{B})\times S\rightarrow(S\mid\bot_E\mid\bot_\var{NT})$.%
\nomenclature{$\bot_\var{NT}$}{Indicates non-termination}
This partial function takes as input a state predicate~$H$ and a state~$\sigma$.%
\index{state!predicate}
Predicate~$H$ denotes the \emph{halting condition}.%
\index{symbolic execution!halting condition}
Typically, the halting condition instructs the run function to stop
at a certain instruction, such as \lstinline{ret}.
The run function iteratively fetches the current instruction
via the current value of the instruction pointer
and uses the machine model to execute it.
Whenever an exception occurs, it stops and returns~$\bot_E$.
If the execution were to continue forever
without an exception or reaching the halting condition
(e.g.\ due to an infinite loop),
the function returns $\bot_\var{NT}$.
Formally, this is achieved by a standard least-fixed-point construction.

% TODO!!!











\subsection{Floyd Invariant Foundation}\label{se:cfg_invariant}
Loops pose a significant problem when using symbolic execution to analyze code.%
\index{loop}
One of the major issues is that they result in significant path explosion.
While there are methodologies to reduce the number of paths to execute
when using loops~\citep{saxena2009lese,obdrzalek2011efficient},
those methods are not currently formally verified
and therefore not usable within Isabelle/HOL.%
\index{Isabelle/HOL}
Additionally, deciding the loop condition on a symbolic state
may involve non-determinism (such as an event loop dependent on user input to exit),%
\index{non-determinism}
which can cause infinite execution.

Breaking up symbolic execution of loops is one method of resolving those issues.
With the right annotations,
it is possible to only need to symbolically execute one iteration per loop.
This eliminates the above-mentioned loop issues.
That breaking up of loops can be accomplished using a control-flow-based approach
akin to \emph{Floyd verification}~\citep{floyd1967assigning}.%
\index{Floyd!verification}
A state predicate assigned to an instruction within a loop%
\index{state!predicate}
functions as an \emph{invariant} for that loop.%
\index{loop!invariant}
Such invariants hold for every iteration of the loop,
allowing a symbolic description of the loop's behavior.
When combined with a general methodology
of structured preconditions and postconditions that show,%
\index{precondition}%
\index{postcondition}
for each annotated state, the succeeding annotated state satisfies its state predicate,
a Hoare triple can be inferred for the program as a whole.%
\index{Hoare!triple}

Taking this approach also allows minimizing symbolic execution
even in non-loop situations, a form of internal compositionality.
Consider the following pseudocode,
which sequentially executes an if-statement and some program~$P$:
\begin{lstlisting}[
  escapechar=|,
  gobble=2,
  caption=Simple Pseudocode,
  label=compositional_pseudocode
]
  if |$b$| then |$x$| else |$y$|; |$P$|
\end{lstlisting}
The assembly corresponding to this code can be verified using symbolic execution.
If executed in full, the symbolic execution engine
would require first considering the case where~$b$ is true,
executing~$x$ and subsequently symbolically executing program~$P$.
It would then consider the case where~$b$ is false, executing~$y$ followed by~$P$.
Program~$P$ would thus be symbolically executed twice.
This repetition can be avoided
by placing a cutpoint at the start of each block where control flow converges,%
\index{cutpoint}
resulting in all instructions being symbolically executed only once each.
Each cutpoint, however, requires a state predicate contained
in a \emph{Floyd invariant}.

The Floyd invariant for a function is a partial function
that takes the form $I:L\rightharpoonup(S\rightarrow\mathbb{B})$.%
\nomenclature{$L$}{Type of instruction addresses in a program; a 64-bit word}%
\nomenclature{$\mathbb{B}$}{Type of boolean values, True and False}
This function maps from instruction addresses with invariants
to the corresponding state predicate that is the invariant.
As a technical detail, some function proofs require additional arguments to $I$
that represent the arguments passed to the function.
\begin{definition}\label{def:floyd_inv}
  A Floyd invariant~$I$ \emph{holds} if and only if, for any state~$\sigma$,
  \begin{equation}
    I(\loc\sigma)(\sigma)\longrightarrow
    \sigma'\neq\bot_E\wedge(\sigma'=\bot_{\var{NT}}\vee I(\loc\sigma')(\sigma')),
  \end{equation}
  where
  $\sigma'=\run((\lambda\sigma\cdot I(\loc\sigma)(\sigma)\neq\bot),\sigma)$%
  \nomenclature{$\bot$}{Used here to represent an empty value,
    such as the result of calling a partial function with a value it does not have an actual result for or in general the \texttt{None} value of an optional type}
  and $\loc\sigma$ is the current program location,
  stored in \inlineasm{rip} on x86-64 systems.
\end{definition}
In words, if the Floyd invariant holds on the current state~$\sigma$,
then running to the next annotated location does not produce an exception.
If it terminates, the produced state~$\sigma'$ satisfies the Floyd invariant.

The following theorem states that a Floyd invariant
can be used to prove properties over its corresponding program or function
as a whole:
\begin{theorem}
  Assume that Floyd invariant~$I$ holds and provides annotations for locations~$l_0$ and~$l_f$ (the initial and final location).
  Let halting condition~$H$ stop at location~$l_f$;%
  \index{halting condition}
  that is, $H(\sigma)\longrightarrow\loc\sigma=l_f$.
  Then $\htriple{I(l_0)}{H}{I(l_f)}$.
\end{theorem}
\begin{proof}
  Remember from \cref{def:htriple} that
  \begin{equation*}
    \htriple{P}{H}{Q}\equiv\forall\sigma\cdot
    P(\sigma)\wedge\sigma'\neq\bot_{\var{NT}}\longrightarrow
    \sigma'\neq\bot_E\wedge Q(\sigma').
  \end{equation*}
  Though there could be any number of additional annotations between~$l_0$
  and~$l_f$, Floyd~\citep{floyd1967assigning} showed that a Floyd invariant that holds
  starting from some initial condition to an intermediate annotation at~$l_1$
  will also hold starting from that annotation.
  % TODO: double-check this with Freek, Floyd didn't actually prove that in the paper.
  Thus, as $I$ holds, we can substitute in~$I(l_0)(\sigma)$ for~$P(\sigma)$
  and~$I(l_f)(\sigma')$ for~$Q(\sigma')$ without issue,
  resulting in the following statement:
  \begin{equation*}
      I(l_0)(\sigma)\wedge\sigma'\neq\bot_{\var{NT}}\longrightarrow
      \sigma'\neq\bot_E\wedge I(l_f)(\sigma').
  \end{equation*}
  As we have already assumed that~$I$ holds,
  we can substitute in the right side of the implication from \cref{def:floyd_inv}
  to obtain
  \begin{equation*}
    \sigma'\neq\bot_E\wedge(\sigma'=\bot_{\var{NT}}\vee I(l_f)(\sigma'))\wedge
    \sigma'\neq\bot_{\var{NT}}\longrightarrow
    \sigma'\neq\bot_E\wedge I(l_f)(\sigma').
  \end{equation*}
  This then simplifies to 
  \begin{equation*}
    \sigma'\neq\bot_E\wedge I(l_f)(\sigma')\longrightarrow
    \sigma'\neq\bot_E\wedge I(l_f)(\sigma'),
  \end{equation*}
  which is trivial.
\end{proof}

Intuitively, Floyd-style verification allows a program to be modeled as \iac{cfg}.%
\index{Floyd!verification}
In that \ac{cfg}, each edge can be seen as an implication.

Unlike the usual formulation of
Hoare logic~\citep{hoare1969axiomatic,myreen2007hoare},\index{Hoare!logic}
Hoare triples as defined for this work take a \emph{halting condition}%
\index{Hoare!triple}%
\index{symbolic execution!halting condition}
as their middle input rather than a program statement.
The result is that the program statement is
characterized by the addresses of its initial
and ending instructions, defined in~$P$ and~$H$, rather than a specific syntax.
Thus, we have the following definition:
\begin{definition}\label{def:htriple}
  $\htriple{P}{H}{Q}$
  denotes that, for any state~$\sigma$, assuming precondition~$P$ and termination,
  $\run(H,\sigma)$ produces a non-exceptional state that satisfies postcondition~$Q$.
  
  Formally,
  \begin{equation}
    \htriple{P}{H}{Q}\equiv\forall\sigma\cdot
    P(\sigma)\wedge\sigma'\neq\bot_{\var{NT}}\longrightarrow
    \sigma'\neq\bot_E\wedge Q(\sigma'),
  \end{equation}
  where $\sigma'=\run(H,\sigma)$.
\end{definition}
% TODO: may not need the explicit $\sigma'\neq\bot_E$

Standard composition does not apply to such Hoare triples.
Consider a symbolic run that executes until halting condition~$H'$.%
\index{symbolic execution!halting condition}
It is possible to break this run into two parts
by first running until a halting condition~$H$ and then until~$H'$.
This requires that~$H'$ is \emph{stronger} than~$H$; that is, $H'$ implies~$H$.
This ensures that the run first stops at~$H$ before it stops at~$H'$.
%\refstepcounter{equation}
\begin{theorem}\label{thm:comp}
  Hoare triples are compositional with respect to stronger halting conditions:
  %  \def\labelSpacing{24.574 pt}
  \begin{prooftree}
    \AxiomC{$\htriple{P}{H}{Q}$}
    \AxiomC{$\htriple{Q}{H'}{R}$}
    \AxiomC{$\forall\sigma\cdot H'(\sigma) \longrightarrow H(\sigma)$}
    %    \LeftLabel{\hphantom{\textnormal{(\theequation)}}}
    %    \RightLabel{\textnormal{(\theequation)}}
    \TrinaryInfC{$\htriple{P}{H'}{R}$}
  \end{prooftree}
\end{theorem}

\section{Function-level Composition}\label{se:cfg_composition}
As stated above, composition is crucial for scalability.%
\index{composition}
On the function call level, compositionality ensures that, when a function is called,
a successful verification effort over that function can be reused
if preexisting or developed later if need be.

Generally, compositionality over function calls requires proving
that the stack pointer remains unchanged after execution of every function call.%
\index{stack!pointer}
There are some exceptions for optimized tail calls
in which a called function returns to the caller of its callee,
but those are not the norm.
\begin{example}
  Consider a function~$f$ starting in a text section at location~$l_0$.
  The function is called from a different text section
  by the instruction \lstinline{call f} at location~$l_\var{call}$.
  This means the return address for the call is $l_\var{call}+5$%
  \footnote{the \texttt{call} instruction is five bytes long}.
  After execution of \lstinline{call f}, the program will be at location~$l_0$
  and the stack pointer will have some value $\rspo$.
  In order to apply compositionality to function calls,
  the pre- and postcondition have to meet the following requirements.
  First, the precondition must imply that the return address is pushed on the stack%
  \index{precondition}
  (a task performed by \lstinline{call}):
  $\readmem{\rspo}{8}=l_\var{call}+5\wedge\mathrsp=\rspo$.
  Second, the postcondition must imply that after \lstinline{ret},
  the net effect of the function body
  is that the stack pointer has been incremented by 8:
  $\mathrsp=\rspo+8\wedge\loc=l_\var{call}+5$.
  Note that \lstinline{call} itself decrements the stack pointer by~8,
  so this implies the net effect, from the point of view of the caller,
  is that the stack pointer is unchanged.
  The postcondition must also show
  that the location has been set back to the return address, $l_\var{call}+5$.
\end{example}
Besides the stack pointer, modern calling conventions have other \emph{callee-saved}%
\index{register!callee-saved}
registers, such as \lstinline{rbp} and \lstinline{r12-r15}.
It is generally assumed that the net effect of a function call
does not touch these registers.
Consider a situation in which \lstinline{rbp} contains an address,
which will be used as the target of a write after a function call.
In order to prove memory preservation, \lstinline{rbp} must be shown to be preserved.
Generally, this is easy to prove
by strengthening the pre- and postcondition with a conjunct $\mathrbp=\rbpo$.
The proof is generally not complicated,
as these callee-saved registers are pushed onto the stack
at the beginning of functions that use them and popped off at the end.

In many cases, users of a verification methodology over functions
will encounter calls to functions that are not included in the verification effort.
These may be system calls or simply functions not currently under consideration
due to unsupported features or lack of time.
External functions are simply assumed to have correct behavior
and are thus left out of the existing analysis,
leaving those functions in the \ac{tcb}.

\section{Examples}\label{se:cfg_examples}
The following sections present some basic explanation
of the procedure used in this chapter, 
\subsection{Non-recursive Loop Example: \texttt{pow2}}\label{sse:pow2_example}
This simple loop-based function, shown in \cref{lst:pow2},
raises two to the power of its argument.
The assembly code was obtained by compiling a C program containing the function
with \ac{gcc} 7.2.0
and disassembling it using the modified \lstinline|reassembly| analysis
mentioned in \cref{se:cfg_overview}. The input is stored in \lstinline{edi}.
\begin{lstlisting}[
  gobble=2,
  float=*,
  caption=\texttt{pow2} in C,
  label=lst:pow2
]
  unsigned long pow2(unsigned exponent) {
      unsigned long a = 1;
      
      for (unsigned i = 0; i < exponent; ++i) {
          a += a;
      }
      
      return a;
  }
\end{lstlisting}
\begin{lstlisting}[
  gobble=2,
  float=*,
  caption=\texttt{pow2} in Assembly,
  label=lst:pow2a,
  firstnumber=0
]
  pow2:
      push	rbp # Size:1
      mov	rbp, rsp # Size:3
      mov	dword ptr [rbp - 0x14], edi # Size:3
      mov	qword ptr [rbp - 8], 1 # Size:8
      mov	dword ptr [rbp - 0xc], 0 # Size:7
      jmp	.label_10 # Size:2
  .label_11:
      shl	qword ptr [rbp - 8], 1 # Size:4
      add	dword ptr [rbp - 0xc], 1 # Size:4
  .label_10:
      mov	eax, dword ptr [rbp - 0xc] # Size:3
      cmp	eax, dword ptr [rbp - 0x14] # Size:3
      jb	.label_11 # Size:2
      mov	rax, qword ptr [rbp - 8] # Size:4
      pop	rbp # Size:1
      ret	 # Size:1
\end{lstlisting}
The function uses memory in five places, all on the stack.
These are expressed relative to the original value of the stack pointer $\rspo$:
\begin{enumerate*}%[label=\alph*.)]
  \item the caller's \lstinline|rbp| at $\mathrbp=\rspo-8$ (eight bytes);
  \item the argument to the function at $\mathrbp-\mathtt{0x14}$ (four bytes);
  \item the accumulation variable and return value at $\mathrbp-8$ (eight bytes);
  \item the counter variable at $\mathrbp-\mathtt{0xc}$ (four bytes);
  \item and the address of the location to return to at $\rspo$.
\end{enumerate*}
The memory region for this function is thus $r_s=\region{\rspo-28}{36}$.
Assigning region~$r_s$ with ID~$i_s$
and the untouched region $\region{a}{1}$ with ID~$i_a$,
parent relationships can then be established as shown below:
\begin{align*}
  & \parent{\region{\rspo}8}{i_s}{r_s} && \parent{\region{\rspo-20}4}{i_s}{r_s} \\
  & \parent{\region{\rspo-8}8}{i_s}{r_s} && \parent{\region{\rspo-28}4}{i_s}{r_s} \\
  & \parent{\region{\rspo-16}8}{i_s}{r_s} && \parent{\region{a}1}{i_a}{\region{a}1}
\end{align*}

For the memory preservation proof of this function, we chose to associate annotations
at the start of the function, an instruction that broke the loop,
and the return address of the function
(a logical variable, as the caller of the function is unspecified for this proof).
\Cref{fig:pow2cfg} shows the Floyd invariant in \ac{cfg} form for this function.
The invariant carries through the preservation of memory,
showing that region $\region{a}{1}$ maintains its value throughout.
The equalities over \inlineasm{rbp} and \inlineasm{rsp}
are used by the memory region reasoner.
For compositional purposes as described in \cref{se:composition},
the function is also shown to preserve the value of the stack pointer.
\begin{figure}
  \centering
  \begin{tikzpicture}[>={stealth}]
    \graph[math nodes, grow down=2.2cm]{
      "1:\begin{array}{l}
        \readmem{a}{1}=v_0\wedge\mathrsp=\rspo\wedge{} \\
        \readmem{\rspo}{8}=\retaddr
      \end{array}" ->[
        align=left,
        "$\mathrsp\coloneqq\mathrsp-8$\\
        $\mathrbp\coloneqq\mathrsp$"
      ] b/"12:\begin{array}{l}
        \readmem{a}{1}=v_0\wedge~\\
        \mathrbp=\mathrsp=\rspo-8
      \end{array}" ->[
        "$\mathrsp\coloneqq\mathrsp+16$"
      ] "\retaddr:\readmem{a}{1}=v_0\wedge\mathrsp=\rspo+8";
      b ->[out=-16, in=16, looseness=3] b;
    };
  \end{tikzpicture}
  \caption{Floyd invariant for \texttt{pow2}}\label{fig:pow2cfg}
\end{figure}
Given the parent regions presented above, the proof that the Floyd invariant holds
is executed automatically using the symbolic execution engine
described in \cref{ch:symbolic_execution}.

\subsection{Recursion: Factorial}\label{sse:cfg_factorial}
The factorial operation provides a simple example of recursion.%
\index{factorial}%
\index{recursion}
The basic definition of factorial is $n!=\prod_{i=1}^n i$.%
\nomenclature{$\prod$}{Product of a sequence of terms;
  multiplication equivalent of $\sum$}
This results in a number that is the product of the numbers from~$1$ to~$n$.
Expressed in recursive form, that definition is:
\begin{equation}
  n!=\begin{cases}
    n * (n - 1)! & \text{if }n > 0 \\
    1 & \text{if }n = 0
  \end{cases}
\end{equation}
The C equivalent of that function is shown in \cref{factorial-c}.
\begin{lstlisting}[
  gobble=2,
  float=*,
  caption=Factorial in C,
  label=factorial-c
]
  uint64_t factorial(uint8_t n) {
      if (n) {
          return n * factorial(n - 1);
      }
      return 1;
  }
\end{lstlisting}
The assembly snippet shown in \cref{lst:factasm} is again
the result of a function compiled with \ac{gcc} 7.2.0
and disassembled by the tweaked \lstinline|reassembly| analysis~\citep{wang2017ramblr}.
In this case, the function performs a recursive factorial calculation on the value~$n$
stored in~\lstinline|dil| (the lowest eight bits of~\lstinline|edi|/\lstinline|rdi|).
\begin{lstlisting}[
  caption=Assembly of Factorial Example,
  label=lst:factasm,
  float,
  escapechar=|,
  firstnumber=0
]
  factorial:
      push rbp|\label{factasm:precondition}|
      mov rbp, rsp
      push rbx
      sub rsp, 0x18
      mov eax, edi
      mov byte ptr [rbp - 0x14], al
      cmp byte ptr [rbp - 0x14], 0
      je .label_12
      movzx ebx, byte ptr [rbp - 0x14]
      movzx eax, byte ptr [rbp - 0x14]
      sub eax, 1
      movzx eax, al
      mov edi, eax
      call factorial
      imul rax, rbx|\label{factasm:resume}|
      jmp .label_13
  .label_12:
      mov eax, 1
  .label_13:
      add rsp, 0x18
      pop rbx
      pop rbp
      ret
\end{lstlisting}
It essentially consists of two loops,
one loop to perform storing the integers from~$n$ to 2 on the stack as the function
is called recursively
and the second to multiply all those values together as each call returns.

As with \lstinline|pow2|, the proof for this function relies on an~$\rspo$,
though in this case that value specifically refers to the value of \lstinline|rsp|
for the first/topmost call to the recursive function.
The memory locations operated on by this function
are similar to those of \lstinline|pow2|,
but the memory locations themselves cannot be directly offset
from~$\rspo$ due to the function's recursive nature.
\begin{figure*}
  \centering
  \begin{tikzpicture}[>=stealth]
    \graph[math nodes, grow down=3.5cm]{
      a/"1:\begin{array}{l}
        \readmem{a}{1}=v_0\wedge\mathdil\le n\wedge{} \\
        \mathrsp=\rspo-48*(n-\mathdil)\wedge{} \\
        (\mathdil\not=n\longrightarrow\mathrbx=\mathdil+1)\wedge{} \\
        \rbxpops(32,\rspo,n,\mathdil)\wedge{} \\
        \retsites(0,\rspo,n,\mathdil)
      \end{array}" ->[
        align=left,
        "$\begin{array}{l}
          \text{if }\mathdil = 0 \\
          \mathrsp\coloneqq\mathrsp+8
        \end{array}$"
      ] b/"15:\begin{array}{c}
        \readmem{a}{1}=v_0\wedge\mathrbx>0\wedge\mathrbx\le n\wedge{} \\
        \mathrsp=\rspo-48*(n-\mathrbx+1)+8\wedge{} \\
        \rbxpops(24,\rspo,n,\mathrbx-1)\wedge{} \\
        \retsites(8,\rspo,n,\mathrbx-1)
      \end{array}" ->[
        align=left,
        "$\begin{array}{l}
          \text{if }\mathrbx = n \\
          \mathrsp\coloneqq\mathrsp+48
        \end{array}$"
      ] "\retaddr:\readmem{a}{1}=v_0\wedge\mathrsp=\rspo+8";
      a ->[out=-17, in=17, swap, align=left, looseness=2,
        "$\text{if }\mathdil\neq 0$ \\
        $\mathrsp\coloneqq\mathrsp-48$ \\
        push \lstinline|rbx| \\
        $\mathrbx\coloneqq\mathdil$ \\
        $\mathrdi\coloneqq\mathdil-1$"
      ] a;
      b ->[out=-17, in=17, swap, align=left, looseness=2,
        "$\text{if }\mathrbx\neq n$ \\
        $\mathrsp\coloneqq\mathrsp+48$ \\
        pop \lstinline|rbx|"
      ] b;
    };
  \end{tikzpicture}
  \caption{Floyd invariant for factorial}
  \label{fig:factcfg}
\end{figure*}
The set~$R$ of separated parent regions is characterized by the following assumptions:
\begin{subequations}
  \begin{align}
    \forall m\le n\cdot(i_m,\region{\rspo+48*m-(n*48)-40}{40}) &\in R \\
    \forall m\le n\cdot(i'_m,\region{\rspo+48*m-(n*48)}{8}) &\in R \\
    (i_a,\region{a}{1}) &\in R
  \end{align}
\end{subequations}
The first assumption models all stack frames of size~40.
The second models the parts of the stack where return addresses are pushed.
The parent relations are defined in a similar way.

The following functions assist in characterizing the stack frame
for any particular call in the recursive chain.
To start with, $\rbxpops$ characterizes the multiplicands currently stored on the stack
for any particular recursive call,
both in the initial recursing loop as well as in the second loop
as the recursive calls return.
The following functions, $\operatorname{ret\_address}$ and $\retsites$,
establish that return address~15 is pushed
to the correct memory location for every call except the first.
For the first call, the topmost stack frame,
the initial return address must also be properly stored.
\begin{multline}
  \rbxpops(\var{offset},\rspo,n,x)\equiv \\
  \forall i\cdot n>x\wedge
    i<n-x-1\longrightarrow\readmem{\mathrsp+i*48+\var{offset}}{8}=i+1+\mathrbx
  \label{eq:rbxpops}
\end{multline}
\begin{multline}
  \operatorname{ret\_address}(\var{rsp},\rspo,\var{offset},n,x,i)\equiv \\
  \text{if }\var{rsp}+(n-x-i)*48-\var{offset}=
  \rspo\text{ then }\retaddr \text{ else } 15
  \label{eq:ret_address}
\end{multline}
\begin{multline}
  \retsites(\var{offset},\rspo,n,x)\equiv \\
  \forall i\le n-x\cdot
    \readmem{\mathrsp+(n-x-i)*48-\var{offset}}{8}=
    \operatorname{ret\_address}(\mathrsp,\rspo,\var{offset},n,x,i)
  \label{eq:retsites}
\end{multline}

The Floyd invariant for the factorial function is shown in
\cref{fig:factcfg,eq:rbxpops,eq:ret_address,eq:retsites}.
The first loop, from location~1 back to~1, goes deeper into recursion,
pushing values onto the stack until \inlineasm{dil} becomes~0.
As stated, $\rbxpops$ and $\retsites$ characterize the stack frame for every call,
ensuring that all necessary information is properly stored.
Once $\mathdil=0$, the function has executed its final recursion
and control reaches location~15.
The second loop then pops values of the stack until $n=\mathrbx$,
at which point there are no more recursive stack frames to pop
and the final result of the factorial operation can be returned.

A Hoare triple can now be derived from the the Floyd invariant.
This is done by instantiating variable~$n$ with \lstinline|dil|.
Doing so simplifies the precondition,%
\index{precondition}
as initially, no values or return addresses are pushed
other than the upmost return address. The resulting Hoare triple becomes:
\begin{equation}
  \begin{array}{l}
    \{\readmem{a}{1}=v_0\wedge\mathrsp=\rspo\wedge\readmem{\rspo}{8}=\retaddr\} \\
    H \\
    \{\readmem{a}{1}=v_0\wedge\mathrsp=\rspo+8\wedge\loc=\retaddr\}.
  \end{array}
\end{equation}
This, combined with the regions presented above as assumptions,
provide us with our theorem of memory preservation.

\section{Application: HermitCore}\label{se:cfg_application}
The concept of \emph{unikernels} has existed in the world of virtualization
for over five years now.%
\index{unikernel}
The term ``unikernel'' can refer to any single-address-space program.
All that is required is that it be compiled with a library
that provides all kernel code necessary to run the program.
This bypasses the need for a separate \ac{os}~\citep{madhavapeddy2014unikernels},
allowing the program to be used directly with a hypervisor%
\index{hypervisor}
or even run on a bare metal system with no additional support.
This allows for reduced overall size and a reduction in attack surface
by leaving out those kernel components that are not necessary.

Slightly implied by the mention of hypervisors,
unikernels are intended for use in the same situations as traditional \acp{vm}
or Docker containers.
They are meant for simultaneous juxtaposed execution in a virtualized setting,
with many single-purpose unikernels all performing their own tasks in isolation.
This makes unikernels an interesting target for verification,
as they aim to provide a high speed and real-time environment for cloud software.

The unikernel library HermitCore~\citep{lankes2016hermitcore} was chosen%
\index{HermitCore}
to demonstrate the applicability of this methodology
due to its established functionality and decent size.
Designed for the x86-64 \ac{isa}, HermitCore is mostly written in C.
While it does use some inline assembly, not uncommon in kernel code,
that is no issue for the assembly-level methodology presented here.
The subset of HermitCore functions that were verified feature features
such as loops, pointers, complex data structures, function calls, and recursion.
The 71 functions analyzed were generally compiled unoptimized,
but twelve of those functions were also analyzed in their optimized forms.
This was done to show that the more complex code produced by optimizing compilers
can also potentially be handled.
The proofs and all associated code
are available at \url{https://doi.org/10.6084/m9.figshare.7356110.v4}.

\subsection{Functions Analyzed}
The functions from Hermitcore that were selected for analysis
are summarized in \cref{tbl:functions}.
The \lstinline|dequeue_*| functions
involve operations on a generic circular queue or ring buffer.
The \lstinline|buddy_*| functions, meanwhile,
are internal to HermitCore's implementation of \lstinline|kmalloc|.
HermitCore's task scheduler
is assisted by the linked list manipulation \lstinline|task_list_*| functions
as well as various functions from \lstinline|tasks.c|.
Next, the \lstinline|vring_*| functions are involved with virtual I/O operations.
Various system call wrappers from \lstinline|syscall.c| were also handled,
as well as eight functions from \lstinline|spinlock.h|.
In addition to those sets of functions,
the following \lstinline|string.h| functions were verified:
\lstinline|memcpy|, \lstinline|memcmp|, \lstinline|memset|, \lstinline|strlen|,
\lstinline|strcpy|, \lstinline|strncpy|, \lstinline|strcmp|, and \lstinline|strncmp|.

The string functions were of particular interest
due to the implicit assumption of null termination%
\index{null termination}
for those functions that do not have an explicit ending count.
Those functions, the ones whose names do not contain~\lstinline|n|,
require an explicit assumption of null termination in their verification process.
Otherwise they would continue to execute past the desired end of the supplied arrays,
reading/writing memory until a memory error occurs.
As the memory model used in this dissertation%
\index{memory!model}
does not support detection of access violations for unallocated areas of memory,
that would effectively mean an infinite loop.
Those functions with an explicit iteration limit
do not need to assume null termination,
as they will eventually terminate even if a null character is not encountered.
Due to the lack of access violation support,
we assume the arrays are of sufficient length
even if they do not possess a null terminator within the specified range.

All of these functions were isolated and then compiled into binaries.
Because of this, functions marked as \lstinline|static inline|
had those qualifiers removed.
This prevented them from being eliminated when compiled with optimizations,
as most of the functions would otherwise have their bodies inlined.

\begin{table*}
  \sisetup{table-format=4.0, table-number-alignment=right}
  \renewcommand\theadalign{tc}
  \centering
  \begin{threeparttable}
    \caption{Summary of functions analyzed}
    \label{tbl:functions}
    \begin{tabular}{lSSSSSSSS S[table-figures-integer=2]}
      \toprule
      \thead{Functions} & {\thead{Count}} & {\thead{\acs*{sloc}}} &
      {\thead{Insts\tnote{\dag}}} & {\thead{Loops}} & {\thead{Recursion}} &
      {\thead{Pointer\\args}} & {\thead{Globals}} & {\thead{Subcalls}} &
      {\thead{\texttt{-O3}}} \\
      \midrule
      \lstinline|dequeue_*| & 3 & 46 & 159 &&& 3 && 3 & 3 \\
      \lstinline|buddy_*| & 5 & 67 & 225 & 1 & 1 & 1 & 3 & 3 & 3 \\
      \lstinline|task_list_*| & 3 & 43 & 128 &&& 3 &&& 3 \\
      \lstinline|vring_*| & 3 & 19 & 80 &&& 1 &&& 3 \\
      \lstinline|string.h| & 8 & 81 & 280 & 8 && 8 &&& \\
      \lstinline|syscall.c| & 23 & 293 & 857 & 5 && 19 & 7 & 17 & \\
      \lstinline|tasks.c| & 10 & 122 & 396 & 2 && 3 & 9 & 4 & \\
      \lstinline|spinlock.h| & 8 & 89 & 254 & 2 && 8 & 2 & 6 & \\
      Total & 71 & 760 & 2379 & 18 & 1 & 46 & 21 & 33 & 12 \\
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \item[\dag] Non-optimized count
    \end{tablenotes}
  \end{threeparttable}
\end{table*}

\Cref{fig:dequeue_push,fig:buddy_large_avail} show the \acp{cfg} for two of
the HermitCore functions verified here,
\lstinline|dequeue_push| and \lstinline|buddy_large_avail|.
The former pushes a value onto a generic array-based queue
while the latter checks for the smallest available reused memory block
for a given allocation size.
The former, lacking any loops, requires only pre- and postconditions
(though additional invariants may be added).
In contrast, the latter function
requires a loop invariant in addition to the pre- and postconditions.

\begin{figure*}
  \centering
  \begin{subfigure}{.48\linewidth}
    \begin{tikzpicture}[>=stealth]
      \graph[math nodes, grow down=2.5cm]{
        "129:\begin{array}{l}
          \readmem{a}{1}=v_0\wedge\mathrsp=\rspo\wedge{} \\
          \mathrbp=\rbpo\wedge\mathrdi=\deqptr\wedge{} \\
          \readmem{\rspo}{8}=\retaddr
        \end{array}" ->[
          "\dots"
        ] "\retaddr:\begin{array}{l}
          \readmem{a}{1}=v_0\wedge{} \\
          \mathrsp=\rspo+8\wedge{} \\
          \mathrbp=\rbpo
        \end{array}"
      };
    \end{tikzpicture}
    \caption{\lstinline|dequeue_push|}\label{fig:dequeue_push}
  \end{subfigure}
  \begin{subfigure}{.50\linewidth}
    \begin{tikzpicture}[>=stealth]
      \graph[math nodes, grow down=3cm]{
        "0:\begin{array}{l}
          \readmem{a}{1}=v_0\wedge\mathrsp=\rspo\wedge{} \\
          \mathrbp=\rbpo\wedge
          \readmem{\rspo}{8}=\retaddr
        \end{array}" ->[
          align=left,
          "$\mathrsp\coloneqq\mathrsp-8$\\
          $\mathrbp\coloneqq\mathrsp$"
        ] b/"21:\begin{array}{l}
          \readmem{a}{1}=v_0\wedge\mathrsp=\rspo-8\wedge{} \\
          \mathrbp =\rspo-8\wedge{} \\
          \readmem{\rspo-8}{8}=\rbpo\wedge{} \\
          \readmem{\rspo}{8}=\retaddr
        \end{array}" ->[
          align=left,
          "$\mathrbp\coloneqq\readmem{\mathrsp}{8}$\\
          $\mathrsp\coloneqq\mathrsp+16$"
        ] "\retaddr:\begin{array}{l}
          \readmem{a}{1}=v_0\wedge{} \\
          \mathrsp=\rspo+8\wedge{} \\
          \mathrbp=\rbpo
        \end{array}";
        b ->[out=-15, in=15, looseness=1] b;
      };
    \end{tikzpicture}
    \caption{\lstinline|buddy_large_avail|}\label{fig:buddy_large_avail}
  \end{subfigure}
  \caption{Example Floyd invariants}
\end{figure*}

\section{On Usability}
The three main aspects of per-function user interaction for this methodology are
\begin{enumerate*}
  \item defining a Floyd invariant,
  \item defining memory region set~$R$, and
  \item proving memory preservation.
\end{enumerate*}
While restricting the verification effort to memory preservation
does reduce the effort required to provide Floyd invariants, it does not eliminate it.
This is more of a problem for loops with complex behavior
and is a significant problem for recursive functions.
With non-looping control flow,
the primary effort required for invariant predicates is showing how input arguments
are carried through the program (stored on the stack, in registers, etc.).%
\index{stack}
With loops, the exact formulation relies on development of a symbolic representation
of the behavior of the loop as it relates to memory accesses.
Recursive functions are the most complex
due to the flat, non-abstract memory model used in this work
requiring manual reasoning over stack frames.
Both the stack and frame pointers must be preserved throughout the recursion,%
\index{stack!pointer}%
\index{frame!pointer}
and all return addresses must be properly pushed on and popped off the stack as needed.

Another aspect of Floyd invariant development
that is not easily determined ahead of time is strengthening of the precondition.%
\index{precondition}
Making reasoned guesses about the necessary precondition clauses is one way to proceed,
and source code annotations as well as reference documentation
may provide additional help,
but sometimes it is necessary to just symbolically execute
until non-determinism is encountered.%
\index{non-determinism}
At that point, the cause of the non-determinism can be identified
and the precondition can be strengthened in such a way so as to eliminate%
\index{precondition}
that non-determinism.
Because this proof methodology works on the assembly level,
it may well expose implicit or undocumented preconditions.

Formulation of the memory region set~$R$ as well as parent relationships,%
\index{memory!region}
if necessary, are also generally manual.
If a necessary region is not present,
symbolic execution will again result in non-determinism,
requiring another round of user input.
After symbolic execution for a basic block has completed,
a proof that the resulting symbolic state satisfies the Floyd invariant
is generally required.
In most cases, that proof can be handled by Isabelle/HOL%
\index{Isabelle/HOL}
using standard off-the-shelf libraries,
either ones included with Isabelle or ones from the \ac{afp}~\citep{afp}
(though not necessarily efficiently).
Recursion is the primary exception,
with the proofs of stack and frame pointer preservation
requiring significant \ac{itp} over word arithmetic.

\section{Summary}
This chapter covered one method for formal verification%
\index{formal!verification}
of memory preservation in x86-64 binaries,%
\index{memory!preservation}
showing that functions in a binary restrict themselves to certain regions of memory.
The approach here aimed to automate verification
while still allowing user interaction wherever necessary.
As a semi-automated approach,
it requires setting up an invariant, which traditionally is a hard problem in itself.
Requirements for memory preservation invariants are provided for several examples.
For recursive functions, more involved invariants are required,
along with \ac{itp} to show preservation of the stack and frame pointers.
Invariants may include preconditions necessary for excluding exceptional behavior.
Such preconditions are exposed by applying the methodology to a disassembled binary,
instead of deriving them from documents or source code annotations.

The approach was applied to functions of HermitCore, a unikernel \ac{os}.%
\index{HermitCore}%
\index{unikernel}
Memory preservation was formally proven for functions with loops, recursion,
C structs and unions, and dynamic memory operations.
Both optimized and non-optimized versions were verified.

% TODO: move to dissertation conclusions?
Some major additions to the framework in this chapter
would be handling of concurrency and related instruction variants.
Additionally, proper modeling of \ac{vm} and hypervisor calls in logic%
\index{hypervisor}
would allow verification of a wider range of functions from the HermitCore library.%
\index{HermitCore}
