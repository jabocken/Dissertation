\title{Static Binary Analysis for Memory Properties and Lifting of Control Flow}
\keywords{%
  Formal Verification,
  \glsentryname{arch} Assembly,
  Interactive Theorem Proving,
  Static Analysis,
  %  Proof Generation,
  %  Memory Preservation%
  Exception Handling%
}
\author{Joshua Alexander Bockenek}
\program{Computer Engineering}
\degree{Doctor of Philosophy}

\submitdate{\DTMdate{2022-11-30}}

\principaladvisor{Binoy Ravindran}
\firstreader{Freek Verbeek}
\secondreader{Patrick R.\ Schaumont}
\thirdreader{Michael S.\ Hsiao}
\fourthreader{\todo{Changhee Jung}}

\dedication{%
  This work is dedicated to \todo{family, friends/coworkers?}
  my dearly departed cat, Abby, who lasted through my Master's but was not able to make it to the end of my PhD.
  \todo\dots%
}

\acknowledge{%
  This work was supported in part by \ac{onr} under grant N00014-17-1-2297
  and \ac{navsea}/\ac{neec} under grant N00174-16-C-0018.
  Any opinions, findings, and conclusions or recommendations expressed
  in this dissertation are those of the author
  and do not necessarily reflect the views of \ac{onr} or \ac{navsea}/\ac{neec}.%

  \todo{Any more?}
}

% The abstract is required and should be <=250 words for thesis, <=350 words for dissertation.
\abstract{%
  % Motivation
  Formal characterization of the memory used by a program is an important basis for security analyses and compositional verification.
  Proving that that program only modifies memory within specified regions, the property of \emph{memory preservation}, is an important aspect of that.
  However, accurately proving memory preservation requires operating on the assembly level due to the semantic gap between high-level languages and the code that processors actually execute.
  Automated methods, such as model checking, would not be able to handle many interesting functions due to the undecidability of memory preservation.
  Fully-interactive methods do not scale well either.
  The solution is to combine proof generation with interactive theorem proving in a \emph{semi-automated manner}: let some untrusted tool extract as much information as it can from the functions under test and then generate all the necessary proofs to be completed in a theorem prover.
  Pivoting to a nondeterministic, structured approach to memory representation and pen-and-paper proofs enables full automation and efficiency.
  Adding modeling of exception handling then allows following previously-unknown paths.

  % First contribution
  The first contribution of this dissertation is a control-flow-driven verification approach with mostly manual invariant specification at automatically-selected cutpoints.
  The memory regions and any additional preconditions must also be determined manually.
  This methodology was applied to \num{63} functions from the HermitCore unikernel library, including one recursive one, covering \num{2379} assembly instructions.

  % Second contribution
  The second contribution of this dissertation is a syntax-driven verification approach with fully-automated invariant and memory region generation.
  It produces \acp{fmuc} that can be verified in Isabelle/HOL
  with minimal effort, the main manual work being weakening any loop invariants and composing functions.
  This was successfully applied to \num{251} functions from the Xen hypervisor project, covering a total of \num{12252} assembly instructions.

  % Third contribution
  The third contribution of this dissertation is a fully-automated formal method for lifting \acp{cfg} from binaries even in the presence of indirection.
  It operates directly on the machine code level, producing \acp{hg} that can also be verified in Isabelle/HOL.
  The tool was successfully applied to \glssymbol{bin-success} programs and \glssymbol{lib-func-success} library functions from Xen, lifting a total of \glssymbol{inst-total-lifted} instructions.

  % Fourth contribution
  The fourth and final contribution of this dissertation is a fully-automated tool for the extraction of \acp{eicfg} from binaries.
  It models the \Cpp\ exception handling \ac{abi} and validated that modeling against some of the real-world implementation via fuzzing.
  It also provides an improved overapproximative approach to indirect branches.
  The tool was successfully applied to \glssymbol{eicfg-bin-success} binaries, covering a total of \glssymbol{eicfg-inst-total} instructions.%
}

% The general audience abstract is required. There are currently no word limits.
\abstractgenaud{%
  Modern computer programs are so complicated that individual humans cannot manually check them to make sure they are correct and secure.
  This is even worse if you want to reduce the \ac{tcb}, the stuff that you have to assume is correct in order to say a program is correct.
  The \ac{tcb} includes your computer itself, but also whatever tools were used to take the programs written by programmers and transform them into a form suitable for running on a computer. Such tools are often called \emph{compilers}.

  To minimize the \ac{tcb}, you have to examine the lowest-level representation of that program, the assembly or even machine code that is actually run by your computer.
  This poses unique challenges, because operating on such a low level means you do not have a lot of the structure that a more abstract, higher-level representation provides.
  Also, sometimes you want to \emph{formally} state things about program's behavior; that is, say things about what it does with a high degree of confidence based on mathematical principles.
  If you want to be detailed about that behavior, you may need to know all of the chunks, or \emph{regions}, in \ac{ram} that are used by that program.
  \Ac{ram}, henceforth referred to as just ``memory'', is your computer's first place of storage for the information used by running programs.
  This is distinct from long-term storage devices like \acp{hdd} or \acp{ssd}, which programs do not normally have direct access to.

  Unfortunately, determining with absolute certainty the exact regions of memory that are read or written is impossible in many cases.
  This is called \emph{undecidability}, and means that you need to \emph{approximate} those memory regions a lot of the time.
  An \emph{underapproximation}, an approach that only gives you some of the regions, is not useful for formal statements as it might miss out on some behavior.
  This means that you need an \emph{overapproximation}, an approach that is guaranteed to give you \emph{at least} the regions read or written.

  The first contribution of this dissertation is a preliminary approach to such an overapproximation.
  It requires a lot of user effort, including having to manually specify the regions in memory that were possibly used and do a lot of work to prove that those regions are (overapproximatively) correct, so our tests were limited in scope.

  The second contribution automated a lot of the manual work done for the first approach.
  This one produces what we call \emph{\acfp{fmuc}}, which are formal statements that the regions of memory they describe are the only ones possibly affected by the \emph{functions} (structured program components) under test.
  These statements also come with \emph{proofs}, which for our work are like scripts used to verify that the things the \acp{fmuc} assert about the corresponding functions can be shown to be true given the assumptions our \acp{fmuc} have.
  Sometimes those proofs are incomplete, though, such as when there is a \emph{loop} (repeated bit of code) in a function under test or one function calls another.
  In those cases, a user has to finish the proof, in the first case by \emph{weakening} (removing information from) the \ac{fmuc}'s statements about the loop and in the second by \emph{composing}, or combining, the \acp{fmuc} of the two functions.
  This tool also cannot handle \emph{indirect branching}.
  Such branching happens when the low-level instructions a program uses to move to another place in that program are supplied a place to go to as the program is running.
  This is opposed to \emph{direct branching}, where the place to go to is hardcoded into the program when it is compiled.
  It also cannot not deal with \emph{aliasing}, which is when different \emph{state parts} (value-holding components) of a program contain the same value and that value is a location in memory.
  Specifically, it cannot deal with \emph{potential} aliasing, when there is not enough information available to determine if the state parts alias or not.
  Because of that, we had to add extra assumptions to the \acp{fmuc} that limited them to those cases where ambiguous memory-referencing state parts referred to \emph{separate} memory locations.
  Finally, it requires specifically assembly as input; you cannot directly supply a binary to it.
  This is also true of the first contribution.
  Because of this, we were able to test on more functions than before, but not a lot more.

  To deal with the main drawbacks of \acp{fmuc} mentioned above, we developed \emph{\acfp{hg}}, the third contribution of this dissertation.
  These \acp{hg} focus specifically on \emph{control flow reconstruction} (figuring out where in a program execution will go), meaning we do not need to prove specific properties that are not related to control flow.
  Additionally, \acp{hg} use \emph{memory models} that take the form of \emph{forests}, or collections of tree data structures.
  A single tree represents a region in memory that may have multiple \emph{symbolic} references, or abstract representations of a value.
  The children of the tree represent regions used in the program that are \emph{enclosed} within their parent tree elements.
  Now, instead of assuming that all ambiguous memory regions are separate, we can use them under various aliasing conditions.
  We also have implemented support for some forms of indirect branching.
  Additionally, no extra user interaction is needed even when loops are present thanks to a methodology that automatically reduces the amount of information present at a re-executed instruction until the information stabilizes.
  Function composition is also automatic now thanks to a method that treats each function as its own \emph{context} in a safe and automated way, reducing memory consumption of our tool and allowing larger programs to be examined.
  In the process we did lose the ability to deal with \emph{recursion}, or functions that call themselves or call other functions that call back to them, though.
  Lastly, we provided the ability to directly load binaries into the tool, no external \emph{disassembly} (converting machine code into human-readable instructions) needed.
  This all allowed much greater testing than before, with applications to multiple programs and program libraries.

  The fourth and final contribution of my dissertation iterates on the \ac{hg} work by moving focus to supporting \emph{exceptional control flow}.
  Specifically, it models the kind of exception handling used by \Cpp\ programs.
  This is important as, if you want to explore a program's behavior, you need to know all the places it goes to.
  If you use a tool that does not model exception handling, you may end up missing paths of execution caused by \emph{unwinding}.
  This is when an exception is thrown and propagates up through the program's current \emph{stack} of function calls, potentially reaching programmer-supplied handling for that exception.
  Despite this, commonplace tools for static, low-level program analysis do not model such unwinding.
  The \acp{cfg} produced by our exception-aware tool are called \emph{\acfp{eicfg}}.
  These provide information about the exceptions being thrown and what paths they take in the program when they are thrown.
  Additional improvements are a better methodology for handling indirect branches as well adding back in support for recursion.
  All told, this allowed us to explore even more programs than ever before.
}
