\chapter{Background}\label{ch:background}
This part of my dissertation provides domain-specific information
necessary to understand the work presented in it.
It is grouped into the three main categories of
formal methods (\cref{se:formal_methods}),
assembly language (\cref{se:assembly_language}),
and structured exception handling (\cref{se:structured_exceptions}).

% Hiding while working on new content
\begin{comment}

%The section on formal methods explains formal semantics,
%symbolic execution, and the basics of Floyd and Hoare proving.
%It also talks a bit about \ac{smt} solvers, model checking, and theorem proving.
%
%The section on assembly language talks about
%
\section{Formal Methods}\label{se:formal_methods}
To quote \textcite{butler:fm},
\begin{quote}
  ``Formal Methods''\index{formal!methods}
  refers to mathematically rigorous techniques and tools
  for the specification, design and verification of software and hardware systems.
\end{quote}
This dissertation comes under the \emph{verification} component of that.%
\index{formal!verification}
%
%\subsection{Semantics}
%Very often when working with formal tools, you must formulate formal semantics
%for a language.
%
%\Textcite{schmidt2003pls}
%
%\subsubsection{Operational}
%This form of semantics is modeled as a set of state changes.
%\todo{not really right}
%
%\paragraph{Big-Step}
%Also called \emph{natural} semantics, these describe the overall results of execution.
%
%\paragraph{Small-Step}
%Also called \emph{structural operational} semantics, these describe the individual steps
%of execution.
%
%\subsubsection{Denotational}
%In this form of semantics, the language under consideration
%is modeled by some kind of mathematical object.
%\todo{not quite right either?}
%
%\subsubsection{Axiomatic}
%This form of semantics uses \emph{axioms} to describe behavior.
%\todo{more}
%
%The canonical example is \emph{Hoare logic}.%
%\index{Hoare!logic}
%
%\subsection{Fixed Point Representation}
%The \ac{lfp} and \ac{gfp} of a function are
%
%\todo\dots
%
%The \ac{lfp} of an infinite loop formulated in this way
%is thus the bottom element $\infloop$.%
%\nomenclature{$\infloop$}{Indicates non-terminating state}

\subsection{Symbolic Execution}
At its most basic, \emph{symbolic execution} refers to
executing a program with a set of symbolic inputs
rather than concrete values \autocite{king1976symbolic}.
Based on the semantics of the program, the execution may end up taking multiple paths;
it could potentially be an infinite number if there are loops involved.

In this work, the individual steps of symbolic execution
are implemented as \emph{rewrite rules} over the state%
\index{symbolic!execution!rewrite rule}
that derive their representation from
Applying those rules in sequence to each step or instruction of a program
allows aggregation of the individual state changes involved in the execution.

%\subsection{Floyd Flowcharts}\label{se:floyd}
%\todo\dots

\subsection{Hoare Logic}\label{se:hoare}
A form of \emph{axiomatic semantics},
\index{semantics!axiomatic}
Hoare logic \autocite{hoare1969axiomatic,myreen2007hoare}%
\index{Hoare!logic}
describes the behavior of a program with a \emph{Hoare triple}.\index{Hoare!triple}
\begin{definition}[Basic Hoare triple]
  Given predicates on state~$P$ and~$Q$ and a program~$C$,
  $\htriple{P}C{Q}$ asserts that executing~$C$ on a state where~$P$ holds
  will result in a state where~$Q$ holds (as long as~$C$ terminates, that is).

  In a more formal form and using $C(\sigma)$ to represent the result
  of executing program~$C$ from initial state~$\sigma$, we have:
  \begin{equation*}
    \htriple{P}C{Q}\equiv P(\sigma)\wedge C(\sigma)\neq\infloop\longrightarrow
    Q(C(\sigma)).
  \end{equation*}
\end{definition}
To prove that triple, deductive reasoning with \emph{Hoare rules}%
\index{Hoare!rule}
formally derived from the structure of whatever programming language is in use
would be used to syntactically decompose the program into its constituent behaviors.
If any loops were to be present, the loop rule would require an additional condition
that would hold on every iteration, a \emph{loop invariant},\index{loop!invariant} for each loop.

As Hoare logic was heavily inspired by Floyd's flowcharts, it is sometimes referred to
as \emph{Floyd-Hoare logic}.

\subsubsection{Verification Condition Generation}
In the context of this dissertation,
\iac{vcg} is the tool that applies Hoare rules (for \cref{ch:syntax})
or performs symbolic execution (for \cref{ch:cfg})
in order to obtain a set of proof subgoals
that must be proven true for the full proof to succeed.
These goals are the \acp{vc}.

%There are two ways of performing verification condition generation%
%\index{verification condition generation}:
%either start at the end and go backwards, deriving the \emph{weakest precondition},%
%\index{precondition!weakest}
%or start at the front and go forwards, deriving the \emph{strongest postcondition}.%
%\index{postcondition!strongest}
%The weakest-precondition approach appears most common,
%being the canonical methodology.\todo{need citation}

%\subsection{Model Checking}
%Model checkers work by taking a formal description of \iac{fsm}
%and a formal specification of some property
%
%\todo\dots
%
%The ``state explosion'' problem has been a well-known issue
%in the model-checking community \autocite{clarke2012modelchecking}
%
%\todo\dots

\subsection{Theorem Proving}
%%
%\index{theorem prover}

%\subsubsection{Automated versus Interactive}
%\index{theorem prover!interactive}

\subsubsection{Isabelle and \acs*{hol}}
The theorem prover utilized in this work
was Isabelle 2018 \autocite{nipkow2002isabelle}.%
\index{Isabelle/HOL!2018}
It is a generic proof assistant with a flexible, extensible syntactic framework.
Isabelle also utilizes a powerful proof language
known as \ac{isar} \autocite{wenzel2007isabelle}
Its most-commonly-used logic is \ac{hol}; when used with that logic,
it is referred to as Isabelle/HOL.

\paragraph{HOL-Word}
We made heavy use of Isabelle/HOL's Word library \autocite{isabelle-word-session}
for the work presented in this dissertation.
That library provides a limited-precision integer type, \lstinline|'a word|,
where \lstinline|'a| is the number of bits in the integer.
Various operations are provided for manipulation of and arithmetic
involving formal words, including bit indexing, bit shifting, setting specific bits,
and signed and unsigned arithmetic.
Operators for inequality are also included,
as well as operations for converting between word sizes.

\paragraph{Eisbach}
This simple but powerful language for declaring custom proof methods
\autocite{matichuk2018eisbach} is used in the verification methodologies
for both works presented in this dissertation, though \cref{ch:syntax}
used more of its features.
Internally, it relies heavily on the standard method expression syntax
\autocite{wenzel2018isar}, described in \cref{tbl:methods}.
The precedence of the syntactic elements involved, from low to high,
is \lstinline!| ; , [] + ?!. Parentheses can be used to modify the precedence,
as with regular mathematical expressions.
\begin{table*}
  \centering
  \caption{Method expressions}\label{tbl:methods}
  \begin{tabular}{lll}
    \toprule
    \thead{Syntax} & \thead{Name}  & \thead{Behavior} \\
    \midrule
    \lstinline|a, b| & Sequential composition & Apply \lstinline|a|,
      then \lstinline|b| \\
    \lstinline|a; b| & Structural composition & \makecell[l]{Apply \lstinline|a|
      to the first subgoal,\\
      then apply \lstinline|b| to just the goals produced by \lstinline|a|} \\
    \lstinline!a | b! & Fallthrough & Try applying \lstinline|a|
      and then apply \lstinline|b| if \lstinline|a| fails \\
    \lstinline!a?! & Attempt & Try to apply \lstinline|a|,
      leaving the goal alone if it fails \\
    \lstinline|a+| & Repeat at least once & Will repeat
      until \lstinline|a| fails or no subgoals remain \\
    \lstinline|a[n]| & Subgoal restriction & Apply \lstinline|a| to the first~$n$
      subgoals, defaulting to~1 \\
    \bottomrule
  \end{tabular}
\end{table*}

Creating a standalone method is as simple as:
\begin{lstlisting}[gobble=2]
  method a =
    b, c
\end{lstlisting}
Then \lstinline|a| can be used in a proof the same as any other method.
To make a method that can have theorems supplied to it,
you use the \lstinline|uses| keyword:
\begin{lstlisting}[gobble=2]
  method a uses t =
    (simp add: t), c
\end{lstlisting}
The method could now be called like \lstinline|apply (a t: thm1 thm2)|
and it would supply the two theorems to \lstinline|simp|.
You can also create methods that take terms as input:
\begin{lstlisting}[gobble=2]
  method a for L :: 'a list =
    b[where l=L]
\end{lstlisting}
And then call it like \lstinline|apply (a \<open>[1, 2]\<close>)|.

\section{Assembly Language}\label{se:assembly_language}
No modern high-level programming language is ever executed directly on hardware.
Instead, they are \emph{compiled},%
\index{compilation}
either before execution (\ac{aot} compilation) or at runtime (\ac{jit} compilation),
to low-level \emph{machine code}. That machine code is specified
by the \ac{isa} of the \ac{cpu} in use.
One step above machine code is assembly language, which is a textual representation
of the binary code to execute, with each instruction and its operands
being represented by human-readable mnemonics.
There are usually some additional abstractions,

\subsection{The \arch\ Instruction Set Architecture}
\Iac{isa} is the specification of the visible behavior of a processor.
They have long been published as human-readable documents
\autocite{bowen1985cards,intel2019manual},
though Arm recently released several of their \acp{isa} in a machine-parsable,
executable format \autocite{reid2016arm}.

\Iac{isa} can specify many things about the processor they describe.
The data types supported by the processor are included in that,
such as integers and floating-point values of specific sizes.
The processor state is another major one, including how much main memory and registers
are available; maybe even details of the cache(s) if it's an architecture
where caching (on certain levels) is a manual process, such as is traditional
for \acp{gpu}.
The semantics of the architecture are another, such as the memory consistency model
(complicated enough for \arch\ that formalizing it was a challenge
\autocite{sewell2010tso,owens2009tso,owens2009tsoextended})
and the addressing modes.
The actual set of instructions supported by the \ac{isa}
and how it communicates with external devices are also important.

Historically, there have been two main types of \acp{isa}:
\acp{cisc} and \acp{risc} \autocite{jamil1995rc}.
\Acp{cisc} came first, featuring complex \acp{isa} with multiple addressing modes
and many variable-length instructions with in-depth behavior.
\Acp{risc} were a response to the growing complexity of \ac{cisc} designs,
providing a set of simple instructions that,
other than loads and stores for memory access, only operate on a large register file.
The main push for \acp{risc} was actually a desire
to reduce the complexity of implementations. \Ac{risc} instructions being
generally simpler than their \ac{cisc} equivalents
means less circuitry is required to implement them,
which reduces die size/chip surface area and allows for increases in clock speed.
This also allows the instructions themselves to complete faster,
with most \ac{risc} instructions finishing in one clock cycle
compared to the many many-cycle instructions of \acp{cisc}.

Currently, \acp{risc} are used more often for systems that require
low-power operation while \acp{cisc} are used more for high-performance applications.
In modern times, however, differentiating between the two types of architectures
due to power or performance concerns is no longer as relevant as it used to be.
For current designs, the differences in performance and power
have more to do with implementation than \ac{isa} \autocite{blem2013struggles}.

In the end,
this dissertation used the \arch\ \ac{isa} as it is a widely-used architecture
that has had formal semantics derived for most of its instructions
in previous works \autocite{heule2016stratified,roessle2019}.
It is the 64-bit, mostly-backwards-compatible successor to the x86 \ac{isa},
also known as x64, x86\_64, AMD64 (for AMD chips), and Intel 64 (for Intel chips).
The x86 family of processors originated in 1978 with the 16-bit Intel 8086,
whose design is still reflected in that of modern x86 \acp{isa}.
Many of the instructions are the same,
though extended over the years to work with larger operands.
There are also many new instructions that have been added over the years.

The rest of this section describes features and properties of \arch\ that are relevant
to this dissertation.

\subsubsection{State}
For the purposes of this dissertation,
the important components of \arch\ state are the memory model
and the registers available for use.

\paragraph{Memory}
Addressing in \arch\ takes the form of \cref{eq:addressing}.
\begin{equation}\label{eq:addressing}
a = \left\{\frac{\begin{matrix}
  \mathfs: \\ \mathgs:
  \end{matrix}\begin{bmatrix}
  \vdots \\ \mathGPR \\ \vdots
  \end{bmatrix}+\left(
  \begin{bmatrix}
  \vdots \\ \mathGPR \\ \vdots
  \end{bmatrix}*\begin{bmatrix}
  1 \\ 2 \\ 4 \\ 8
  \end{bmatrix}
  \right)}{\mathrip}\right\}+\text{ displacement}
\end{equation}
In this \ac{isa}, while addresses are all 64 bits long,
only 52 of those bits are actually usable for physical addressing.
Additionally, while it provides \emph{virtual memory}
so that every process gets its own  address space,
hiding the details of its memory management and paging
from user-mode programs, that virtual memory only gets 48 bits for addressing;
the rest must be sign-extended from the \ac{msb} of that 48-bit value
or else the address will be rejected.

While the actual instructions have no problems accepting 64-bit address operands,
the issue lies in how paging is implemented and the usage of high address bits
as storage for metadata. Allowing a full 64-bit address space would require additional
storage space for the information that is currently stored in those bits.
It is not impossible that it will happen eventually, however.

For now, the works in this dissertation make a simplifying assumption that all 64 bits
are available for addressing. Properly modeling the 48-bit restriction would
require additional word arithmetic, though would not be infeasible.
We also do not deal with memory consistency, as the symbolic execution engine
in \cref{ch:symbolic_execution} does not implement out-of-order execution
or any similar optimizations.

\paragraph{Registers}\label{par:reg}
The standard registers\index{register} in \arch\ (\cref{tbl:regs}) are 64 bits in size,
but their lower bits can be referenced alone if need be.
If you write to one of the available 32-bit registers,
it will zero out the upper~32 bits, but if you write to a 16- or 8-bit register,
the rest of the contents will be preserved.
\begin{table*}
  \centering
  \appto\theadfont{\normalcolor\normalfont}
  \caption{The \arch\ registers (excluding \acs*{simd})}\label{tbl:regs}
  \begin{threeparttable}
    \begin{tabular}{lAAAAA}
      \toprule
      \thead{Purpose} & \thead{\SI{64}\bit} & \thead{\SI{32}\bit} &
      \thead{\SI{16}\bit} & \thead{\SI{8}{\bit} high} & \thead{\SI{8}{\bit} low} \\
      \midrule
      General Purpose\tnote{a} & rax & eax & ax & ah & al \\
      General Purpose\tnote{b} & rbx & ebx & bx & bh & bl \\
      General Purpose\tnote{c} & rcx & ecx & cx & ch & cl \\
      General Purpose\tnote{d} & rdx & edx & dx & dh & dl \\
      General Purpose\tnote{e} & rdi & edi & di && dil \\
      General Purpose\tnote{f} & rsi & esi & si && sil \\
      General Purpose & r8 & r8d & r8w && r8b \\
      General Purpose & r9 & r9d & r9w && r9b \\
      General Purpose & r10 & r10d & r10w && r10b \\
      General Purpose & r11 & r11d & r11w && r11b \\
      General Purpose & r12 & r12d & r12w && r12b \\
      General Purpose & r13 & r13d & r13w && r13b \\
      General Purpose & r14 & r14d & r14w && r14b \\
      General Purpose & r15 & r15d & r15w && r15b \\
      Base Pointer & rbp & ebp & bp && spl \\
      Stack Pointer & rsp & esp & sp && bpl \\
      Program Counter/Inst.\ Pointer & rip & eip & ip && \\
      Code Segment ($=0$) &&& cs && \\
      Data Segment ($=0$) &&& ds && \\
      Extra Segment (str ops, $=0$) &&& es && \\
      Variable-Purpose Segment &&& fs && \\
      Variable-Purpose Segment &&& gs && \\
      Stack Segment ($=0$) &&& ss && \\
      Processor Status & rflags & eflags & flags && \\
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \item[a] Formerly the accumulator (temporary variable for math);%
        \index{register!accumulator}
        was also used for I/O port access and interrupt calls
      \item[b] Formerly base pointer for memory access;%
        \index{register!base}
        also got some interrupt return values
      \item[c] Formerly loop counter;%
        \index{register!counter}
        was also used for shifts and got some interrupt return values
      \item[d] Formerly data;%
        \index{register!data}
        was used for I/O port access, arithmetic, some interrupt calls
      \item[e] Formerly destination index for string and memory array operations,
        was also used for far pointer addressing with \inlineasm{es}
      \item[f] Formerly source index for string and memory array operations
    \end{tablenotes}
  \end{threeparttable}
\end{table*}

\subsubsection{Flags}
Stored in the \inlineasm{rflags} register on \arch, these bits (\cref{tbl:flags})
indicate, and in some cases control, processor state.
\keyword{CF}, \keyword{OF}, \keyword{PF}, \keyword{SF}, and \keyword{ZF} are all used
for specific conditional branches.
\begin{table*}
  \centering
  \appto\theadfont{\normalcolor\normalfont}
  \caption{Flags for \arch}\label{tbl:flags}
  \begin{threeparttable}
    \begin{tabular}{rAl}
      \toprule
      \thead{Bit} & \thead{Label} & \thead{Description} \\
      \midrule
      0 & CF & Carry flag\tnote{*} \\
      1 &    & Reserved, always 1 in \inlineasm{eflags} \\
      2 & PF & Parity flag\tnote{\dag} \\
      3 &    & Reserved \\
      4 & AF & Adjust flag\tnote{\ddag} \\
      5 &    & Reserved \\
      6 & ZF & Zero flag; indicates result was 0 \\
      7 & SF & Sign flag; indicates \ac{msb} of result was 1 \\
      8 & TF & Trap flag; permits operation of a processor in single-step mode \\
      9 & IF & Interrupt enable flag\tnote{\S} \\
      10 & DF & Direction flag; controls the direction of string processing \\
      11 & OF & Overflow flag; indicates result overflowed \\
      12-13 & IOPL & I/O privilege level; shows I/O privilege level of current program or task \\
      14 & NT & Nested task flag\tnote{\textbardbl} \\
      15 &    & Reserved, always 1 on 8086 and 186, always 0 on later models \\
      16 & RF & Resume flag; enables turning off certain exceptions while debugging code \\
      17 & VM & Virtual 8086 mode flag\tnote{\textpilcrow} \\
      18 & AC & Alignment check (486SX+ only) \\
      19 & VIF & Virtual interrupt flag (Pentium+) \\
      20 & VIP & Virtual interrupt pending (Pentium+) \\
      21 & ID & Able to use \inlineasm{cpuid} instruction (Pentium+) \\
      \bottomrule
    \end{tabular}
      \begin{tablenotes}
        \item[*] Indicates arithmetic carry/borrow has been generated out of \ac{alu}'s
          \ac{msb}
        \item[\dag] Set if the number of set bits in the least significant byte is even
        \item[\ddag] Indicates when an arithmetic carry or borrow has been generated
          out of the four least significant bits (lower nibble).
          Primarily used to support \ac{bcd} arithmetic.
        \item[\S] Determines if the \ac{cpu} will handle maskable hardware interrupts
        \item[\textbardbl] In protected mode, indicates one system task
          has invoked another via \inlineasm{call} rather than \inlineasm{jmp}.
        \item[\textpilcrow] In protected mode, allows the execution of real mode
          applications that are incapable of running directly in protected mode
          (a form of hardware virtualization).
      \end{tablenotes}
  \end{threeparttable}
\end{table*}

\subsubsection{Instructions}
This section covers some of the important \arch\ instructions.

\paragraph{Function and Stack}
\begin{description}[align=right, font=\keywordstyle]
  \item[call] Pushes the address of the next instruction onto the stack,
    decrementing \inlineasm{rsp}, and then jumps to its operand.
  \item[enter] Modifies stack for entry to procedure
    for high level language. Takes two operands: the amount of storage to be allocated on the stack and the nesting level of the procedure.
  \item[push] Pushes its operand onto the stack,
    decrementing \inlineasm{rsp}.
  \item[pop] Pops the current value off the stack,
    incrementing \inlineasm{rsp}, and stores the popped value in its operand.
  \item[leave] Releases local stack storage created by the previous
    \inlineasm{enter} instruction.
  \item[ret] Pops the current value off the stack,
    incrementing \inlineasm{rsp}, and jumps to it.
\end{description}

\paragraph{Jumps}
\begin{description}[align=right, font=\keywordstyle]
  \item[jo] Jump if overflow ($\mathtt{OF}$)
  \item[jno] Jump if not overflow ($\neg\mathtt{OF}$)
  \item[js] Jump if sign ($\mathtt{SF}$)
  \item[jns] Jump if not sign ($\neg\mathtt{SF}$)
  \item[je] Jump if equal ($\mathtt{ZF}$)
  \item[jz] Jump if zero
  \item[jne] Jump if not equal ($\neg\mathtt{ZF}$)
  \item[jnz] Jump if not zero
  \item[jb] Jump if below ($\mathtt{CF}$)
  \item[jnae] Jump if not above or equal
  \item[jc] Jump if carry
  \item[jnb] Jump if not below ($\neg\mathtt{CF}$)
  \item[jae] Jump if above or equal
  \item[jnc] Jump if not carry
  \item[jbe] Jump if below or equal ($\mathtt{CF}\vee\mathtt{ZF}$)
  \item[jna] Jump if not above
  \item[ja] Jump if above ($\neg\mathtt{CF}\wedge\neg\mathtt{ZF}$)
  \item[jnbe] Jump if not below or equal
  \item[jl] Jump if less ($\mathtt{SF}\neq\mathtt{OF}$)
  \item[jnge] Jump if not greater or equal
  \item[jge] Jump if greater or equal ($\mathtt{SF}=\mathtt{OF}$)
  \item[jnl] Jump if not less
  \item[jle] Jump if less or equal ($\mathtt{ZF}\vee\mathtt{SF}\neq\mathtt{OF}$)
  \item[jng] Jump if not greater
  \item[jg] Jump if greater ($\neg\mathtt{ZF}\wedge\mathtt{SF}=\mathtt{OF}$)
  \item[jnle] Jump if not less or equal
  \item[jp] Jump if parity ($\mathtt{PF}$)
  \item[jpe] Jump if parity even
  \item[jnp] Jump if not parity ($\neg\mathtt{PF}$)
  \item[jpo] Jump if parity odd
  \item[jcxz] Jump if \inlineasm{cx} register is 0
  \item[jecxz] Jump if \inlineasm{ecx} register is 0
\end{description}

\paragraph{\Acf{simd}}
These instructions, which operate on multiple 32- or 64-bit chunks,
allow for a degree of vectorization (up to eight 64-bit values
or 16~32-bit values at once on modern versions of \arch\ with AVX-512).
When AVX-512 is supported, there are~32 512-bit \keyword{zmmN} registers available,
each of which can also be accessed as a 256-bit \keyword{ymmN}
or a 128-bit \keyword{xmmN} register.
Those functions from the case studies that use \ac{simd} instructions
only go up to \keyword{xmm}. In some cases, they are just used for internal moving
of several values at once, but several of the functions rely on \ac{simd} instructions
to support variable-length argument lists.

\subsection{The System~V AMD64 Application Binary Interface}
While \iac{isa} specifies the features of a processor and what it can do,
there is plenty more that must be standardized for easy interoperability of
software. \Iac{abi} specifies the interfaces for different components
in a software system, though not always formally. Those interfaces include
the binary format of \emph{object files}, the format of program libraries,
how system calls are made, and the standard \emph{calling convention},
how function calls are structured on the machine code level.

As the programs used in \cref{ch:cfg,ch:syntax} were compiled and run on Linux,
this dissertation focuses on the System~V AMD64 \ac{abi} \autocite{systemv-ABI-git},
which is the standard for most Unix-based systems.
The relevant aspects are the fact that the System~V \ac{abi} uses the \ac{elf}
format for binaries, though we do not care about that
once we are dealing with assembly, and the calling convention.
The important aspects of the calling convention for this \ac{abi}
are the following:

The first six integer or pointer arguments are stored in \inlineasm{rdi},
\inlineasm{rsi}, \inlineasm{rdx}, \inlineasm{rcx}, \inlineasm{r8},
and \inlineasm{r9}, in that order.
\inlineasm{r10} is used as a static chain pointer when there are nested functions.
Certain floating-point arguments will be stored in
\keyword{xmm0-7}. Any additional arguments will be passed along on the stack.

Integral values up to 64 bits are stored in \inlineasm{rax} while those up to 128 bits
are split between \inlineasm{rax} and \inlineasm{rdx}.
Floating-point return values are stored in \inlineasm{xmm0} and \inlineasm{xmm1}.

Registers \inlineasm{rbx}, \inlineasm{rbp}, and \keyword{r12-r15} are
\emph{callee-saved}, meaning they must be pushed and popped locally to preserve their
values between function calls. All other registers must be saved by the caller.

\subsection{\Ac{gcc}}
The compiler used to build the case studies in \cref{ch:cfg,ch:syntax} is important,
as it uses the segment registers \inlineasm{fs} and \inlineasm{gs} for its own purposes.
\inlineasm{fs} keeps track of the original value of the \emph{stack canary}
used to detect stack smashing \autocite{cowan1998stackguard}
while \inlineasm{gs} is used to access thread-local storage.
For this dissertation, only \inlineasm{fs} is relevant, as none of the case studies
or examples in \cref{ch:cfg,ch:syntax} used thread-local storage.

\subsection{Basic Blocks}
Much of the memory work in this document relates to the concept of \emph{basic blocks}.%
\index{basic block}
A basic block is defined here as a sequence of assembly instructions
whose behavior can be described using only state transitions and branches.
An additional restriction is that they have no internal loops;
they will always terminate, either due to successful completion or early failure.
This definition differs slightly from the definition used by compilers such as LLVM.
The compiler definition is stricter, as it requires that
each block must terminate with a control flow instruction
(or fall through, on the assembly level)
and must not contain any other control flow instructions
\autocite{llvm:functions,llvm:terminators}.

\subsection{Tail Call Optimization}
Tail call%
\index{tail call optimization}
optimization, implemented in most major C and \Cpp\ compilers, is a technique
for converting recursive functions into loop-based ones that only use
a set number of stack frames.
Unfortunately, that can only be applied to functions that fit the requirements
for proper tail calls \autocite{probst2001proper},
as imperative languages like C were not designed with such features in mind.
The technique also supports optimizing non-recursive tail calls, however,
and that has a lower bar.

\end{comment}

\section{Lattices and Their Usage in Programming}
\todo{talk about how lattices are an ordering, their properties, and how we use a join-semilattice}

\subsection{Meet and Join}
\todo{talk about the differences and how in this work we focus on join-semilattices}

\subsection{Least Fixed Point}
\todo{Should this be Greatest Fixed Point instead?}

\section{Abstract Interpretation}
Several contributions of this work were inspired by
\emph{abstract interpretation}\index{abstract!interpretation}.
\todo\dots
% Removing abstract interpretation talk for VSTTE as we're going for a more informal approach; can discuss it more later in dissertation though maybe.
% \subsection{Abstract Interpretation}\label{sec:abstract-interpretation}
% The key property we aim to prove is correctness of the produced \ac{EICFG}.
% Intuitively, correctness means that the \ac{EICFG} is an \emph{abstraction} of the actual binary.
% Specifically, one that preserves its properties pertaining to control flow.
% We therefore restate the definition of an \emph{$\alpha$-simulation} \autocite{clarke94,loiseaux1995property}.
% This definition coincides exactly with the standard notion of a behavioural simulation \autocite{milner1971algebraic}, but is formulated in terms of the concepts used in abstract interpretation.

% We use $s,s',\dotsc$ to denote \emph{concrete states} and $S,S',\dotsc$ to denote sets of concrete states.
% The concrete transition relation over concrete states is denoted $\concTransition$. Similarly, \emph{abstract states} are denoted by $\sigma,\sigma',\dotsc$ and sets of abstract states by $\absState,\absState',\dotsc$, while the abstract transition relation is $\absTransition$.

% Abstract interpretation requires the definition of a Galois connection $\alphagamma$ that respectively does abstraction and concretization \autocite{cousot1977abstract,cousot1996abstract}.
% A Galois connection can be used to construct an overapproximative abstraction of a transition system, by using~$\alpha$ as a simulation-relation and $\gamma$ to map properties on abstract states back to properties over the concrete system \autocite{yavuz16}.

% \begin{definition}\label{def:simulation} %[$\alpha$-similarity]
  %     Transition system $T_\mathsf{C} = (S,s_0,\concTransition)$ is \emph{$\alpha$-simulated by} transition system $T_\mathsf{A} = (\Sigma,\sigma_0,\absTransition)$, notation $T_\mathsf{C} \sqsubseteq T_\mathsf{A}$, if and only if $\sigma_0 = \alpha(s_0)$ and, for all concrete states $s$ and $s'$,
  %     % \begin{equation*}
    %     %     s \concTransition s'\implies\alpha(s)\absTransition\alpha(s').
    %     % \end{equation*}
  %     \begin{prooftree}
    %         \AxiomC{$s \concTransition s'$}
    %         \UnaryInfC{$\alpha(s)\absTransition\alpha(s')$}
    %     \end{prooftree}
  % \end{definition}

% The next sections define abstract states and the abstract step rules.
% Based on these, functions $\alpha$ and $\gamma$ are straightforward (more formal details can be found in \autocite{yavuz16}).
% It is crucial that not too many details are abstracted away: information pertaining to exceptional control flow should be part of the abstract states.
% Under that assumption, $T_\mathsf{C} \sqsubseteq T_\mathsf{A}$ coincides with a behavioral simulation sufficiently strong to preserve properties of the universal fragment of computation tree logic ($\forall$CTL) \autocite{baier2008principles} pertaining to exceptional control flow.
% This includes safety and liveness.




%Traditionally, a key challenge in abstract interpretation is finding the proper abstract domain and formulating the Galois connection.
%A technique addressing this challenge is \emph{predicate abstraction} \autocite{clarke94,graf97}.
%Consider a list $\Pi$ of predicates over concrete states.
%Abstraction occurs by valuating each of these predicates, resulting in abstract states that are lists of Booleans.
%Concretization occurs by replacing Boolean values with the set of concrete states satisfying the corresponding %predicate.
%Yavuz et al. provide formal definitions for $\alphagamma$ for abstract interpretation based on predicate abstraction \autocite{yavuz16}.

%Correctness, then, is provided by Lemma~3 of \autocite{yavuz16}.
%Having the list of concrete state predicates~$\Pi$, let $\alphagamma$ be the Galois connection as defined in \autocite{yavuz16}.
%Then $T_\mathsf{C} \sqsubseteq T_\mathsf{A}$ coincides with a behavioral simulation sufficiently strong to preserve \ac{ACTL} properties pertaining to $\Pi$.
%In other words, $T_\mathsf{A}$ is a proper abstraction of $T_\mathsf{C}$ for all \ac{ACTL} properties formulated over $\Pi$-predicates.

\section{Structured Exception Handling}\label{se:structured_exceptions}

\subsection{Motivation: Structured Programming}
\todo{functions, stacks, etc.; maybe just explain as intro?}

\subsection{Stack Unwinding}

% While the \acs* looks nice in place it actually produces the acronym link in the table of contents as part of the main section link, which is not ideal. That wouldn't even be an issue if the links were colored differently though, can I adjust those to be purple somehow?
\subsection{The \texorpdfstring\Cpp{C++} \acs*{abi}}

\subsubsection{Concrete Structures}

\paragraph{Per-exception info}
\todo\dots
\paragraph{Global info}
\todo\dots

\subsubsection{Library Functions}

\section{Fuzzing and Validation?}
\todo{Might be a useful thing to talk about a little here to expand on what we put in the \ac{eicfg} section}

\section{Summary}
This chapter gave an introduction to formal methods, including formal semantics,
symbolic execution, Floyd verification, Hoare logic, model checking, and theorem provers.
It also described some basic aspects of the \arch\ \ac{isa} and System~V\index{ABI!System V}
AMD64\index{ABI!AMD64} \ac{abi}
as well as some general details on assembly.
Finally, it covered the process of structured exception handling%
\index{structured!exception handling}.
Specifically, structured exception handling as utilized by the \Cpp\ \ac{abi}.
