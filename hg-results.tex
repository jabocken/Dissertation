\chapter{Experimental Results}\label{ch:hg-results}
This chapter covers the \todo\dots

\section{Hoare Graph Extraction}\label{hg-extraction}
We applied \ac{hg} extraction to:
\begin{enumerate}
  \item several stripped binaries of CoreUtils as found in a standard Ubuntu distribution;
  \item a binary with a manually induced buffer overflow, confirming that no \ac{hg} is extracted; and
  \item all 63 \todo{65?} \gls{arch} binaries and all 2151 functions from the 25 shared objects we identified in the Xen hypervisor.
  \todo{make nums gls entries with number type?}
  \todo{gls entry for hypervisor?}
\end{enumerate}
This \namecref{hg-extraction} % make lnamecref if switching to capitalize
focuses on the Xen case study specifically.
The Xen Project is a mature, industrial-strength hypervisor used in many production systems, including Amazon's cloud platforms \autocite{chisnall2008definitive}.
Hypervisors provide a method for managing multiple virtual instances of operating systems (guests) on a physical host.
\todo{glossary entry for hypervisor?}
Xen is a suitable case study because of two things:
\begin{itemize}
  \item its complexity and
  \item the wide range of programs and shared libraries produced by its build process.
\end{itemize}

\begin{table}
  \centering
  \newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
  \caption{Xen Case Study Statistics Summary}
  \label{tab:xen}
  \begin{tabular}{lC{4.8ex}@{$=$}C{4.8ex}@{$+$}C{2.4ex}@{$+$}C{2.4ex}@{$+$}rrrrrrr}
    \toprule
    \thead{Directory} & \multicolumn{5}{c}{} & {\thead{~~Instrs.~~}} & {\thead{Symbolic\\States}} & {\thead{~A~}} & {\thead{~~B~~}} & {\thead{~~C~~}} & \thead{Time/\\h:m:s} \\
    \midrule
    & \multicolumn{5}{c}{\thead{Binaries}} &&&&&&\\
    \texttt{\dots/bin} & 15 & 12 & 2 & 1 & 0 & 6751 & 6829 & 21 & 19 & 0 & 0:15:54 \\
    \texttt{\dots/xen/bin} & 17 & 7 & 1 & 8 & 1 & 2433 & 2468 & 8 & 3 & 3 & 0:01:17 \\
    \texttt{\dots/libexec} & 1 & 1 & 0 & 0 & 0 & 82 & 87 & 1 & 0 & 0 & 0:00:10 \\
    \texttt{\dots/sbin} & 30 & 25 & 1 & 4 & 0 & 8858 & 9178 & 26 & 4 & 8 & 0:18:39 \\
    %    \texttt{local/\ldots/boot} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    %    \texttt{lib/debug} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    \midrule
    Total & 63 & 45 & 3 & 13 & 1 & 18\,124 & 18\,562 & 56 & 26 & 11 & 0:35:59 \\
    \midrule
    & \multicolumn{5}{c}{\thead{Library functions}} &&&&&&\\
    \texttt{\dots/lib} & 1907 & 1874 & 29 & 0 & 4 & 353\,433 & 362\,635 & 1 & 244 & 600 & 15:28:17 \\
    \texttt{\dots/xenfsimage} & 109 & 106 & 3 & 0 & 0 & 17\,184 & 17\,683 & 0 & 0 & 27 & 1:58:36 \\
    \texttt{\dots/dist-packages} & 16 & 16 & 0 & 0 & 0 & 379 & 407 & 0 & 0 & 3 & 0:00:06 \\
    \texttt{\dots/lowlevel} & 119 & 119 & 0 & 0 & 0 & 10\,651 & 10\,799 & 0 & 0 & 90 & 0:08:43 \\
    \midrule
    Total & 2151 & 2115 & 32 & 0 & 4 & 381\,647 & 391\,524 & 1 & 244 & 720 & 17:35:42 \\
    \bottomrule
  \end{tabular}\\
  \begin{tabular}{rcl rcl rcl}
    \multicolumn{9}{c}{$w+x+y+z$: $w$ lifted, $x$ unprovable return address, $y$ concurrency, $z$ timeout} \\
    A &=& Resolved indirection & B &=& Unresolved jump(s) & C &=& Unresolved call(s) \\
  \end{tabular}
\end{table}

The analysis was performed on a machine with a six-core (twelve logical cores), \SI{2.9}{\giga\hertz} Intel Core i9-8950HK \ac{cpu}.
That machine had \SI{31}{\gibi\byte} of \ac{ram}
and \SI{32}{\gibi\byte} of swap space on a KXG50PNV1T02 NVMe \ac{ssd}.
\todo{does NVMe have a proper long-form or is it fine as is?}
Its \ac{os} was Linux Mint 20.1 Cinnamon.
The version of Xen under test was 4.12.
\begin{remark}[Parallelization]
  Our tool for \ac{hg} extraction is not in itself parallelized.
  That means the core count listed above did not directly affect the execution times shown below.
  However, the only restriction on running multiple instances of the tool simultaneously is the availability of system resources.
  Thus, in the published artifact \autocite{bockenek2022artifact} we provided examples of using GNU parallel \autocite{Tange2011a} to perform analyses efficiently.
\end{remark}

\Cref{tab:xen} shows an overview of the results.
The upper part of the table shows binaries.
Lifting of the binaries was done by starting the extraction algorithm at each binary's \ac{elf} entry point and exploring all reachable assembly instructions.
This includes all resolvable internal function calls.
The lower part shows library functions in \acp{so}.
For every \ac{so} file, all externally exposed functions as reported by the \lstinline|nm| utility were considered.
Lifting individual functions required starting the extraction algorithm at the function's address and exploring all reachable assembly instructions from that point.
As with the binaries, that included resolvable calls to other internal functions.

\subsection{Failure Cases}
Three issues may prevent lifting a binary to an HG, shown in the second column of \cref{tab:xen}. These issues are explained below.

\subsubsection{Unprovable Return Addresses}
This case calls back to \cref{sec:ssms}.
When a \inlineasm{ret} instruction is encountered,
the current precondition\footnote{symbolic state predicate}
must be strong enough to prove that the return address at the top of the current stack frame has not been modified.
Furthermore, that precondition must also be strong enough to show that the value of the stack pointer has been restored to the initial value it held on function entry.
If the current precondition is not strong enough to satisfy those conditions, the algorithm does not produce \iac{hg}.
This is because it cannot prove return address integrity.
\todo{This should have been mentioned earlier, double check. also add glossary entry?}

\subsubsection{Concurrency}
Binaries that contain multithreading-related function calls are out of scope for this analysis.
This was determined primarily by the presence of \inlineasm{pthread} calls, mutexes, and semaphores.
\todo{glossary entries for mutex/semaphore?}
However, those binaries were still included in \cref{tab:xen} in order to account for all \gls{arch} Xen binaries.

\subsubsection{Timeout}
While our algorithm has a proof of termination \todo{double-check, it looks like we do but I want to make sure},
state space explosion or other resource limitations sometimes make full execution infeasible.
In order to ensure a full analysis, we set a timeout on analysis to \SI4\hour\ per binary/function.
This resulted in failure for only one binary and four library functions.
\todo{This is actually discussed in two places, either consolidate the discussion of timing here or down below!}

\subsection{Successful Cases}
\todo{here right now}
In total, for 45 out of 63 binaries and 2115 out of 2151 library functions, the basic sanity properties (return address integrity, bounded control flow and calling convention adherence) could be proven and \iac{hg} could be generated.

The third and fourth columns of \cref{tab:xen} show the number of instructions lifted out of the binary and the number of states of the \ac{hg}.
Taking both the binaries and shared objects into account, \numberinstructions\ instructions were lifted.
Since states belonging to the same address are joined whenever compatible, the number of states is close to the number of instructions.
%Overhead occurs when states are not compatible due to the second extension of the algorithm (see Section~\ref{sec:algorithm}).
%Overall, the number of states is a factor 1.06 larger than the number of instructions.


Column~A shows the number of resolved indirections, i.e., the indirections where the effect of the instruction on the instruction pointer could be overapproximatively established.
Columns~B and~C show the \emph{annotations}, i.e., the numbers of unresolved indirect jumps and calls, respectively.
% Unresolved indirect jumps typically where caused by \todo{why?}.
Unresolved indirect calls are often caused by function callbacks: a function pointer is passed as a parameter (or through a global variable) from function to function.
Programmer-supplied function arrays are another source of non-resolution.
Since function calls are handled without context, the function pointer is unknown at the time it is actually called.

\begin{figure}
  \centering
  \todo{figure out how to make this wide}
  \begin{tikzpicture}
    \begin{axis}[
      date coordinates in=y,
      date ZERO=0-0-0,
      yticklabel=\hour:\minute,
      xlabel=Instruction Count,
      ylabel=Time/h:m,
      title=Distribution of time versus instructions
      ]
      \addplot[scatter, only marks] table [col sep=comma]{data/timing-hg.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{Case study timing analysis}\label{fig:distr}
\end{figure}
\Cref{fig:distr} relates the sizes of functions (in numbers of instructions) to the verification time.
The largest function successfully verified was \lstinline|libxl_domain_suspend| from \lstinline|libxenlight.so.4.12.0|, with 3925 instructions and 4207 symbolic states. The analysis took 49 minutes and 10 seconds to complete. The second-largest function verified, \lstinline|libxl_domain_suspend_only|, had 3713 instructions with 4100 symbolic states and took 16 minutes 34 seconds to complete. The longest verification time was around 2 hours for function \lstinline|libxl_domain_build_info_gen_json| with 1584 instructions.
For the 1907 functions, we had 4 timeouts (not included in the 15:28 hrs of verification time). These functions generally had a large number of states that could not be joined (causing explosion in the number of states to be explored).
Figure~\ref{fig:distr}  shows that there is very little correlation between verification times and instruction count.

In total, we lifted an HG for 2115 out 2151 functions (98\%).
We can account for why this number is relatively high:
\begin{itemize}[wide, labelwidth=!, labelindent=0pt,nosep]
  \item For many functions, any pair of pointers \emph{to the local stack frame} abided by any of the four relations for which we accurately model memory relations (aliasing, separations, enclosed within, encloses).
  As a result, even if the heap and the global memory space were grossly overapproximated, the local stack frame was modelled accurately and return address integrity could be proven.
  \item In case of an unresolved function call, we treated the function overapproximatively as an unknown external function.
  Typical reasons for unresolved indirections include callbacks: a function pointer is set by some function $f$ and is retrieved and called back in function $g$. A context-sensitive approach would be able to increase the number of supported indirect calls, but this would need to be done sufficiently scalable.
  \item Some of the rejections constitute functions that do not adhere to the calling convention. Manual analysis of these cases shows that these are all compiler-generated functions that are not required to follow a calling conventions.
  \item Other rejections were caused by a precondition insufficient to derive an overapproximative bounded set of concrete values for the next instruction pointer.
  This may occur when an array or struct is stored on the stack and accessed via variable offset.
  Such constructs may lead to complicated pointer arithmetic \emph{within} the stack frame.
  The result is that the algorithm cannot prove that a memory region was separate from the top of the stack frame, storing the return address.
  \item Even though not all instructions of the \gls{arch} \ac{isa} are supported, all instructions occurring in the case study are, so this is not a reason why functions were rejected.
\end{itemize}

\section{Formal Proofs in Isabelle/HOL}
\todo{This was \emph{not} my work, need to figure out how to include it.}

\section{Examples of Failures}
\todo{This was not my work either, maybe come up with some of my own examples?}
