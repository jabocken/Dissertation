\chapter{Conclusions}\label{ch:conclusions}
Formal verification of assembly code can produce highly reliable claims over software.
By eliminating the need to trust the compiler
and the semantics of whatever source language the program was written in,
you can drastically decrease the \ac{tcb} in use.
However, assembly-level verification is a fundamentally harder problem
than source code verification.

\section{Contributions Revisited}

\section{Post-Preliminary Exam Work}

\subsection{Reducing Overapproximation}\label{mem_use_over}
As a formal property, memory usage has been proven to never miss any memory regions
written to, assuming the correctness of the semantics and model it is applied
to~\citep{bockenek2019preservation,popl2019underreview}.
Put another way, however,
this means that the methodology \emph{must} be conservative.
If it cannot make a determination about the usage status of some part of memory,
either due to an underdeveloped state or too large of one to easily reason about,
it must assume that that region is used. It must \emph{overapproximate}.%
\index{overapproximation}
This sort of false positive can be an issue in the field of formal verification,
as it can make the property under consideration weaker despite being correct.

One way to shrink such overapproximations is to increase the
\emph{context sensitivity}\index{context sensitivity}
of the approach, such as performing the analysis over the full program at once
rather than individual components, but that can involve
a significant increase in time and verification effort.

\todo{Want talk about the usage of SMT solvers/etc.,
  increase in automation in general
  as one way of mitigating increased verification effort somewhere}

\section{Summary?}
