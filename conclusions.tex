\chapter{Conclusions}\label{ch:conclusions}
Formal verification of assembly code can produce highly reliable claims over software.
By eliminating the need to trust the compiler
and the semantics of whatever source language the program was written in,
you can drastically decrease the \ac{tcb} in use.
However, assembly-level verification is a fundamentally harder problem
than source code verification.

\todo\dots

\section{Contributions Revisited}
\todo\dots

\section{Proposed Post-Preliminary Exam Work}
As a formal property, memory preservation
has been proven to never miss any memory regions written to,
assuming the correctness of the semantics and model it is applied
to \autocite{bockenek2019preservation,popl2019underreview}.
Put another way, however, this means that the methodology \emph{must} be conservative.
If it cannot make a determination about the usage status of some part of memory,
either due to an underdeveloped state or too large of one to easily reason about,
it must assume that that region is used. It must \emph{overapproximate}.%
\index{overapproximation}
In order to reduce that overapproximation, the conditions on blocks

\subsection{Strengthen Invariants}
In order to improve automation, we currently generate very weak invariants.
While this worked for \cref{ch:syntax},

\subsection{Model a More Realistic Memory Model}
Most applications do not run in isolation. Their behavior is limited by
the kernel of whatever \ac{os} is in use,
and that includes limits on the amount of memory they are allowed to use.

In particular, process and thread stacks are limited
by how they are laid out in (virtual) memory, and on top of that
most modern \ac{os} kernels put limits on stack size as sanity checks.
The kernel limits are generally configurable,
both at compile time as well as at runtime, but can require privileged access.
Properly modeling those restrictions
would potentially require formulating a more in-depth memory model.
This would be useful to 
