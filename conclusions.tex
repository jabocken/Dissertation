\chapter{Conclusions}\label{ch:conclusions}
Certain properties, such as memory preservation, can only be proven on the assembly level.
This is due to memory preservation requiring a concrete representation of memory.
Unfortunately, assembly-level verification is a fundamentally harder problem
than source-level verification due to the low level of abstraction.
However, it can also produce highly reliable claims over software.
By eliminating the need to trust the compiler
and the semantics of whatever source language the program was written in,
you can drastically decrease the \ac{tcb} in use.

Additionally, the property of memory preservation cannot be determined automatically
under all circumstances. It is an undecidable property.
Because of this and the overheads that \ac{itp} can have,
we designed \emph{semi-automated} methodologies for that purpose.
Those methodologies are briefly revisited below.

\section{Contributions Revisited}
This dissertation presented two methods for proving memory preservation,
control-flow-driven verification and syntax-driven verification.
Both approaches rely on the same symbolic execution model and memory-related rewrite rules
documented in \cref{ch:symbolic_execution}, but differ in several major aspects.

\todo{remark}
Technically they also both use Hoare triples, but only \cref{ch:syntax}
uses proper Hoare rules. \cref{ch:cfg} uses a modified style
that takes a halting condition~$H$ instead of a syntactic construct in the middle.

\subsection{Control-Flow-Driven Verification}
This methodology uses a Floyd-style approach \autocite{floyd1967assigning}
with automatically-selected cutpoints.
It is very similar to the work of \textcite{matthews2006verification},
but with a focus on memory preservation specifically.
The rewrite rules over memory accesses from \cref{memory_rewrite}
result in additional \acp{vc} that would not be present in their framework.
Those \acp{vc} require time-consuming word arithmetic
when the appropriate preconditions/assumptions are not present.
The preconditions/assumptions simply establish separation and enclosure relations
for the necessary memory regions.

This methodology was applied to \num{63} functions
extracted from the HermitCore unikernel library, plus \num{12} optimized versions,
resulting in more than \num{2379} assembly instructions verified.

\subsection{Syntax-Driven Verification}
Rather than using a more general \ac{cfg} to guide the verification,
a more structured \ac{scf} is extracted from the assembly under test.
This \ac{scf} is used as one of the generated \ac{fmuc}'s proof ingredients.
The other proof ingredients are the generated memory regions,
\acp{mrr}, and block conditions.
With the invariant generation as it currently is,
the only user interaction required under normal circumstances
is weakening the condition for a loop entry block
by merging it with the condition for all of the loop's exit blocks.

This methodology was applied to 251 functions from the Xen Project binaries examined,
\xenpercentage\ of the total functions from the examined binaries.
Ultimately, \num{12252} instructions were covered
with only \num{1047} manual lines of proof required.

\subsection{Hoare Graphs}
\Cref{ch:hg-lifting,ch:hg-formulation,ch:hg-results} present the first \emph{provably overapproximative} lifting mechanism for \gls{arch} binaries.
Any overapproximative representation of a binary must include both all its ``normal'' as well as all its ``weird'' behaviors.
%As examples, if the binary has a stack overflow overwriting a return address, or if control flow depends on whether two pointers alias, this must be represented in the lifted model.
We proposed a method that takes a potentially-stripped binary as input (no debugging information or address labeling is required).
It produces \iac{hg} as output that contains:
\begin{enumerate}
  \item the assembly instructions found in the binary;
  \item the control flow; and
  \item evidence in the form of inductive invariants that are sufficiently strong enough to prove soundness.
\end{enumerate}
Our approach can deal with overlapping instructions and aims at providing overapproximative bounds to indirect branches (such as when a \inlineasm{jmp} is based on a computation instead of on a constant).
In some cases, unsoundness annotations are used to indicate possible issues.
Also, assumptions are enumerated explicitly in the form of proof obligations asserting requirements over external functions.
If our technique succeeds and the proof obligations are proven true, then under these assumptions, the lifted representation is a provable overapproximation of the binary.
We have applied our approach to binaries and shared objects of the Xen Hypervisor, covering \glssymbol{inst-total-lifted} instructions in total.
This case study shows that our methodology is scalable and applicable to commercial off-the-shelf software written without verification in mind.
\Acp{hg} can then be exported to the Isabelle/HOL theorem prover, where they can be formally verified.
This second step essentially validates any inference made by the algorithms during Step~1.

In future work, we aim to provide support for concurrency.
Moreover, we find that the context-free nature of our approach limits the number of function callbacks that are properly dealt with.
We will study passing around stateful information between functions to find a midpoint between scalability and better support for function callbacks.

Handling for programmer-provided function arrays, such as those used to select behavior
based on program arguments, would also be useful.

Additionally, it is, in theory, possible to craft a program that would result in state explosion via sufficiently complex indirect control flow.
That would result in state explosion due to the incompatibility factor we introduced to deal with jump tables.
Detecting such situations may be useful, but do not seem necessary to deal with typical real-world programs.
We are also looking into integrating better pointer inference support (which registers/memory locations hold pointers and what types of memory they point to, similar to Jakstab's handling but ideally more in-depth).

Finally, we aim to combine the lifted \acp{hg} with existing approaches to binary analysis.
Provably sound binary lifting can be the base for \emph{any} trustworthy binary-level technique,
including decompilation, binary verification and binary patching.

\subsection{Exceptional Interprocedural Control Flow Graphs}
Many \Cpp\ programs exhibit exceptional control flow that standard \ac{cfg} extraction tools in disassemblers and decompilers do not identify.
To deal with that issue, we have provided \acp{eicfg} and a tool for generating them.
\acp{eicfg} extend standard \acp{cfg} extracted from binaries by including nodes and edges for exceptional control flow.
Our abstract transition relation for exceptional control flow has been informally shown to overapproximate the concrete versions of those edges.
Furthermore, we have applied our \ac{eicfg} generator to \totalbins\ real-world programs and libraries.
We identified \uniquethrows\ unique throws and were able to trace each one's exceptional control flow: \caughtthrows\ were potentially caught while \uncaughtthrows\ had no identified potential for being caught.
On average, dealing with exceptional control flow can increase coverage by \avgdiffinst\ per unique throw, with each throw averaging \avgunwinds\ unwind edges.
Those edges are ones tools such as Ghidra do not produce.
%\satisfactorybins\ of which were analyzed in a satisfactory manner (\cutoffpercent\ coverage).
%On average, we achieved \coveredpercent\ coverage.

\subsubsection{anything else?}

\subsubsection{Limitations}
One of the main drawbacks of our work is a propensity for state space explosion.
While we were able to target programs with over \num{400000} instructions,
our \acp{eicfg} generally do not scale far beyond that. Even for smaller programs, we experienced timeouts and out-of-memory cases when a significant number of control flow nodes and edges were generated.
Methods of reducing the state space while maintaining interprocedural exceptional analysis would provide for increased scalability and the ability to target even larger programs.
For example, modeling of exception type info and integrating it into the $\landingpadtable$ determinations would allow pruning of dead branches, reducing the tool's overapproximation.
% Wanted to use this to begin with but I couldn't get types table parsing fully working right

The other main drawback lies with stripped programs that do not perform indirect calls or jumps, but do pass non-immediate values to external or internal calls.
Dealing with this requires significant manual modeling in our current approach.
Additional heuristics for identifying possible function entry points \autocite{bao2014byteweight,pe2020probabilistic}
may help with this drawback.

Finally, unresolved indirections are also an issue, particularly in stripped binaries.
Better jump table heuristics \autocite{cifuentes2001recovery,flexeder2010reconstruction,gedich2015switch,an2022dsv}
may result in less false positives/negatives for jump table calculations without the need for manual tuning.
This issue may also benefit from additional function entry heuristics as well,
as they would allow per-function fallbacks in the presence of unresolved jumps even without function symbols.

While we deal with structured \Cpp\ exceptions, we do not provide support for interrupt-based exception handling.
Semantics are not provided for interrupt-triggering instructions; they are instead treated as no-ops.
We also do not deal with concurrency.

\section{Potential Improvements (old Post-Preliminary Exam Work)}
As a formal property, memory preservation
has been proven to never miss any memory regions written to,
assuming the correctness of the semantics and model it is applied
to \autocite{bockenek2019preservation,popl2019underreview}.
Put another way, however, this means that the methodology \emph{must} be conservative.
If it cannot make a determination about the usage status of some region of memory,
it must assume that that region is used. It must \emph{overapproximate}.%
\index{overapproximation}
It does not matter if the cause was an underdeveloped state
or too large of one to easily reason about.

There are two potential ways to reduce that overapproximation detailed below.

\subsection{Strengthen Invariants}
In order to enhance automation, we currently generate very weak invariants.
While this works reasonably well, being able to generate stronger invariants
would be advantageous. Stronger invariants mean stricter memory preservation proofs.

\todo{no more abstract intrpretation here}

\subsection{Model a More Realistic Memory Model}
Most applications do not run in isolation. Their behavior is limited by
the kernel of whatever \ac{os} is in use,
and that includes limits on the amount of memory they are allowed to use.

In particular, process and thread stacks are limited
by how they are laid out in (virtual) memory, and on top of that
most modern \ac{os} kernels put limits on stack size as sanity checks.
The kernel limits are generally configurable,
both at compile time as well as at runtime, but can require privileged access.
Properly modeling those restrictions
would potentially require formulating a more in-depth memory model
as the stack limits that are changed at runtime come in two forms.
There is a \emph{soft} limit on stack size that unprivileged users
can modify, but there is also a \emph{hard} limit
that requires root access to modify.

Additional features that would be desirable
would be the ability to treat memory as \emph{allocated} and \emph{deallocated}.
On modern systems, this is usually handled on the page level,
meaning via virtual memory management.
Modeling virtual memory would likely not be a good idea,
as we are currently focusing on userspace analysis.

The main restriction that would be interesting to model, however,
would be the restriction of addresses to their lower 48 bits for actual addressing,
with the remaining 16 bits being equal in value to the 48th bit.
