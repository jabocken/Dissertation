\chapter{Syntax-Driven Verification}\label{ch:syntax}
Again as described in the preface,
my primary contributions to the certificate generation and verification approach%
\index{certificate}
presented in this chapter include the Hoare rules we developed
for memory usage (\cref{scf_hoare})
as well as the \ac{vcg} used to apply those rules
to \ac{scf} in Isabelle (\cref{sse:vcg}).
I also wrote much of the code for full translation of the \acp{fmuc}
into the Isabelle/HOL language
and helped adapt the \ac{scf} into a form suitable for verification
in Isabelle (\cref{isabelle_scf}).
I also performed much of the verification work on our large case study,
presented in \cref{se:xen}.

The rest of the chapter provides some information
on why such certificates are desirable (\cref{se:fmuc_motivation}),
provides an overview of the certificates (\cref{se:fmuc_overview})
and explains the process of generating them in \cref{se:fmuc_gen}.
As previously stated,
the paper detailing the work as a whole~\citep{popl2019underreview}
is currently under review.

\section{Chapter Overview}\label{se:fmuc_motivation}
While the methodology presented in the previous chapter
for verifying memory preservation works well, it is not ideal.
The need to manually formulate regions
and the amount of work required for developing invariants
reduces potential scalability.

To build on the work from the previous chapter,
this chapter introduces the concept of \emph{\acfp{fmuc}}
generated by untrusted, informal tools.
\Acp{fmuc} consist of two main components:
theorems on \hyperref[ch:memory]{memory usage} and \emph{proof ingredients}.%
\index{proof!ingredient}
The proof ingredients are assumptions on memory layout,
control flow information, and invariants
generated to reduce the amount of work required from end users.
Information on how these certificates are generated
can be found in \cref{se:fmuc_gen}.
This includes the algorithm for control flow information extraction
as well as how symbolic execution is used
to produce the preconditions and proof ingredients.

Once generation is complete, the certificate and the original assembly%
\index{certificate}
can then be loaded into an interactive theorem prover.
In the theorem prover,
minimal user input is required for discharging \iac{fmuc}s lemmas and theorems
via the proof ingredients and customized proof methods.%
\index{proof!ingredient}%
\index{proof!method}
A full example demonstrating usage along with 
further information on the structure of \acp{fmuc}
can be found in \cref{se:syntax_example}.
That example could theoretically overwrite
its own return address due to its pointer arguments, causing \ac{cfi} issues.
The associated \ac{fmuc} provides preconditions to prevent such cases
along with a formal proof of return address preservation under those conditions.

Following that example is an in-depth case study
on the Xen Project hypervisor~\citep{chisnall2008definitive} in \cref{se:xen}.
Unlike the HermitCore work in \cref{se:cfg_application},
no modifications were made to the Xen build process
and the basic utility \texttt{objdump} was used for disassembly
rather than a tweaked version of a less-commonly-used utility.
In total, \acp{fmuc} were generated and proofs discharged in Isabelle
for 251 Xen functions.
Minimal user interaction was required;
on average, only \num{85} lines of additional proof were needed
for every \num{1000} assembly instructions verified.
In total, the \num{12252} assembly instructions
were verified with only \num{1047} manual proof lines added,
all of which were simple reuses of established proof methods.
The majority of added lines of proof involved guiding loop invariant application.

\section{Overview of Formal Memory Usage Certificates}\label{se:fmuc_overview}
\Cref{fig:fmuc} provides an example of \iac{fmuc}.
\Acp{fmuc} are produced from assembly code,
which may be generated by a disassembler such \texttt{objdump},
IDA\fturl{https://www.hex-rays.com/products/ida/index.shtml},
Ghidra's decompiler\fturl{https://ghidra-sre.org/}, or Capstone~\citep{capstone},
or generated directly by a compiler when source code is available.
Each function specified for verification receives \iac{fmuc};
those that are not included in the verification effort,%
\index{verification!effort}
including system calls and functions from dynamic libraries,
can be treated as black boxes,
the usage of which is described in \cref{sse:fmuc_comp}.

\begin{figure*}
  \centering
  \lstset{frame=none, numbers=none}
  \begin{subfigure}{.51\linewidth}
    \begin{lstlisting}[style=C, gobble=6]
      int main(int argc, char** argv) {
          return argv[argc - 1][0];
      }
    \end{lstlisting}
    \caption{Source code}\label{fig:example-src}
  \end{subfigure}
  \begin{subfigure}{.48\linewidth}
    \begin{lstlisting}[style=x64, basicstyle=\footnotesize\ttfamily, gobble=6]
      4f0: movsxd rdi, edi
      4f3: mov rax, qword ptr [rsi+rdi*8-8]
      4f8: movsx eax, byte ptr [rax]
      4fb: ret
    \end{lstlisting}
    \caption{Compiled assembly}\label{fig:example-asm}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \centering
    \begin{align*}
      \text{\bfseries Theorem: } & \var{MRR}\longrightarrow\htriple*{P}f{Q}{M} \\
      \text{\bfseries Proof: } & \ldots
    \end{align*}
    where
    \begin{subequations}
      \begin{align}
        f &\equiv\ABB~\Block{4f0}{4fb} \notag \\
        P &\equiv\mathrip=
        \mathtt{4f0}\wedge\mathrsp=
        \rspo\wedge\ldots\wedge\readmem{\rspo}{8}=\retaddr \notag \\
        Q &\equiv\begin{multlined}[t]
          \mathrip=\retaddr\wedge\mathrsp=\rspo+8\wedge\ldots\wedge \notag \\
          \takebits{31,0}\rangle\mathrax=
          \sextend(\readmem{\readmem{\rsio+
              \sextend(\takebits{31,0}\rangle\rdio)*8-8}{8}}{1})
          \end{multlined} \notag \\
        M &\equiv\{\region{\rspo}{8}, \label{m:a} \\
          &\equivsetfix\region{\rsio+\sextend(\takebits{31,0}\rdio)*8-8}{8},
              \label{m:b} \\
          &\equivsetfix\region{\readmem{\rsio+
              \sextend(\takebits{31,0}\rdio)*8-8}{8}}{1}\} \label{m:c} \\
        \var{MRR} &\equiv\begin{aligned}
          \not\separate &= \varnothing \\
          \enclosed &= \varnothing
        \end{aligned} \notag
      \end{align}%
      \nomenclature{$\varnothing$}{Represents an empty set}
    \end{subequations}
    \caption{\Acl*{fmuc}\footnote{%
      $\sextend$ indicates sign extension,
      $\takebits{h,l}w$ indicates taking bits~$l$ through~$h$ of word~$w$,
      and $\readmem{a}{n}$ indicates reading~$n$ bytes of memory
      starting at address~$a$.
    }}\label{fig:fmuc-thm}
  \end{subfigure}
  \caption{Example \acl*{fmuc}}\label{fig:fmuc}
\end{figure*}

The small program shown in \cref{fig:example-src} provides a simple example
for \ac{fmuc} explanation. This one-function program
returns the numeric value of the first character of its last command-line argument.
When compiled to assembly (x86-64 using the System~V \ac{abi}),
the program will resemble the code depicted in \cref{fig:example-asm}.
In this assembly code,
\lstinline|argc| maps to the register \inlineasm{edi},
which is the low 32 bits of the 64-bit register \inlineasm{rdi},
and \lstinline|argv| maps to \inlineasm{rsi}.
After sign-extending \inlineasm{edi} to fill \inlineasm{rdi},
that value and the other argument are used to calculate
an address, stored in the register \inlineasm{rax},
from which a one-byte value is read.
That value sign extended and stored in the lower~32 bits of \inlineasm{rax},
\inlineasm{eax}.

As stated in the introduction to this chapter, \iac{fmuc} consists of two main parts:
a theorem on memory usage and its associated proof ingredients,%
\index{memory!usage}%
\index{proof!ingredient}
the two of which are used in a theorem prover to prove the theorem.
The \ac{fmuc} for the associated example can be seen in \cref{fig:fmuc-thm}.

The theorem consists of a Hoare triple as defined in \cref{def:usage}.
One set of proof ingredients,
the \acp{mrr} required to ensure successful proof completion (detailed below),
are included as assumptions on the theorem.
The control flow ingredient,
represented as the \ac{scf} variable~$f$, is a single basic block for this function
starting at instruction address \inlineasm{4f0} and going to address \inlineasm{4fb}.
Following that,
the precondition~$P$ provides the starting conditions for the function,
which include symbolic variables for initial register values,
storage on the stack of the address to jump to after function execution,
and the address of the instruction to start from
(stored in the instruction pointer register, \inlineasm{rip}).
Last of the traditional Hoare triple components is the postcondition~$Q$,
which indicates completion of function execution
by showing that the instruction pointer is now set to the address to return to,
the stack pointer \inlineasm{rsp} has been updated with an increment of eight,
and the return value register, \inlineasm{rax},
contains the value \lstinline|argv[argc - 1][0]|.
All additional callee-saved registers are shown to retain their initial values%
\index{register!callee-saved}
as well.

Following the traditional Hoare triple elements is the memory region set,~$M$.
For the function under consideration, this set consists of three regions.
The eight-byte location of the return address
on the stack frame (\cref{m:a}),
the location of the pointer \lstinline|argv[argc - 1]| (\cref{m:b}),
and the first element of the array pointed to by that pointer (\cref{m:c}).
Without additional assumptions, those regions may \emph{alias};
they may overlap or even be the same.
While that is not a problem for this specific example,
as none of the regions are written to, in cases where writes do occur
aliasing can result in writes to regions
that are not considered by symbolic execution.
This can result in requirements for significant effort on the part of proof engineers,
greatly reducing automation.
To avoid such issues, the \ac{fmuc} generation methodology
produces the aforementioned \acp{mrr} proof ingredient.%
\index{proof!ingredient}
\Acp{mrr} express which pairs of regions are not separate ($\not\separate$)
as well as which regions are enclosed within one another ($\enclosed$).

The \ac{fmuc} for a function as a whole is broken down by Hoare rules,
as described in \cref{scf_hoare},
to the level of basic blocks, each of which gets its own \ac{fmuc} of sorts.
The generation of these per-block \acp{fmuc} involves generation of per-block
pre- and postconditions as well; these conditions are treated as invariants.
Stronger invariants can lead to a tighter approximation of memory usage
(recall from \cref{mem_use_over} that memory usage as a property
is an overapproximation of actual memory usage in a program).

With the \acp{fmuc} as generated, their theorems and proof ingredients all considered,
minimal effort is required in the vast majority of cases to complete the proofs.
The main exception is functions containing loops.
For functions that do have loops,
as well as for any other cases where proof completion is not automatic,
Isabelle/HOL proof strategies as documented in \cref{se:fmuc_ver}
provide assistance in completing the proofs efficiently.

\section{FMUC Generation}\label{se:fmuc_gen}
\begin{figure*}
  \centering
  \begin{tikzpicture}[>=stealth, gnode/.style={draw, rounded corners, text centered}]
    \graph[grow right=2.8cm]{
      Assembly[gnode] ->[dashed]
      "Control Flow Graph"[gnode, text width=1.4cm] ->["\ref{sse:cfg_extract}"]
      "Syntactic Control Flow"[gnode, text width=1.7cm] ->["\ref{sse:syntax_symb}"]
      "Memory Regions and \acsp*{mrr}"[gnode, text width=1.5cm]
      ->["\ref{sse:inv_gen}"]
      Invariants[gnode] ->[dashed] Certificate[gnode];
    };
  \end{tikzpicture}
  \caption{\acs*{fmuc} overview}\label{fig:overview}
\end{figure*}

The general procedure for generating \acp{fmuc}, laid out in \cref{fig:overview},
can be broken up into three main parts.
The first part involves control flow extraction from assembly using \iac{cfg} analysis
similar to angr's CFGFast~\citep{shoshitaishvili2016state},%
\index{angr!CFGFast}
ultimately producing \iac{scf} (details of which are presented
in \cref{sse:cfg_extract}).
Afterwards, per-basic block symbolic execution,
as detailed in \cref{ch:symbolic_execution},
is utilized to generate the set of memory regions
read and written by the function in question.
To eliminate duplicates and produce \acp{mrr}
showing which regions overlap or are enclosed or separate,
the region sets are fed to the \ac{smt} solver Z3~\citep{de2008z3}.
Symbolic execution is also used in the process of generating
the pre- and postconditions for each basic block,
elaborated in \cref{sse:inv_gen}.

With the exception of \ac{mrr} generation,
none of the steps in this procedure are included in the \ac{tcb}.
The process of verifying the generated \ac{fmuc} (see \cref{se:fmuc_ver})
will fail if there are issues in control flow extraction,
\ac{scf} generation, informal symbolic execution, or invariant generation.
\Ac{mrr} generation is an exception
because the \acp{mrr} are formulated as assumptions,
and thus inconsistent \acp{mrr} will result in vacuous proofs.
This is why the methodology relies on Z3 for \ac{mrr} generation;
using a known-reliable tool greatly reduces the possibility of issues.

\subsection{Control Flow Extraction}\label{sse:cfg_extract}
As described in \cref{se:hoare},
in order to apply \iac{vcg} that utilizes Hoare rules to verify a Hoare triple,
there must be some syntactic structure to apply those rules to.
This chapter uses a syntactic representation of control flow called \ac{scf}
in part for that purpose.
\Ac{scf} expresses assembly programs as a combination of basic blocks,
branches, loops, and function calls.
The following grammar provides a description of \ac{scf}
produced by the extraction code.
Each basic block is represented by the polymorphic type~$\beta$,%
\nomenclature{$\beta$}{Type of basic blocks}
while branching conditions are represented using the polymorphic type~$\Phi$.%
\nomenclature{$\Phi$}{Type of branching conditions}
\begin{bnf}
  \bnfprod*{scf}{
    \bnfpn{scf}\bnfsp\bnfts{;}\bnfsp\bnfpn{scf}
    \bnfor\bnfts{Block}\bnfsp\bnftd{$\beta$}
    \bnfor\bnfts{Skip}
    \bnfor\bnfts{Continue}
    \bnfor\bnfts{Break}\bnfsp\bnfpn{br}
  } \\
  \bnfmore{
    \bnfor\bnfts{If}\bnfsp\bnftd{$\Phi$}
      \bnfsp\bnfts{Then}\bnfsp\bnfpn{scf}
      \bnfsp\bnfts{Else}\bnfsp\bnfpn{scf}
      \bnfsp\bnfts{Fi}
    \bnfor\bnfts{Loop}\bnfsp\bnfpn{scf}\bnfsp\bnfts{Pool}\bnfsp\bnfpn{res}
  } \\
  \bnfprod{br}{
    \bnftd{ID}\bnfor\bnfes
  } \\
  \bnfprod{res}{
    \bnfts{Resume}\bnfsp\bnfts{\{(}\bnftd{ID}\bnfts{,}
      \bnfpn{scf}\bnfts{),}\bnfsk\bnfts{\}}
    \bnfor\bnfes
  }
\end{bnf}
Loops in this formulation have no exit condition;%
\index{loop}
instead, they rely on having one or more internal \texttt{Break} statements,%
\index{loop!break}
which may have an identifier to indicate how the loop was exited, for termination.
\texttt{Continue}s function the same as in C,%
\index{loop!continue}
causing loop execution to skip to the next iteration.
For loops that have multiple exit points,
\texttt{Resume} statements provide different code to execute
based on which exit was taken as indicated by the \texttt{Break} identifier.

Notably, the above data structure does not explicitly contain
control flow statements such as \texttt{goto} or \texttt{throw/catch}
as those statements are not necessary for assembly-level analysis.
A non-indirect \texttt{goto} is simply an unconditional jump,
which is just another singular edge in the \ac{cfg},
and structured exception handling as used in C++ is generally represented
by library-supplied and compiler-generated function calls.

There are a couple of important restrictions on the control flow extraction approach
presented in this section,
the more severe of which is the lack of support for indirect branching.
Handling indirect branching requires a more in-depth \ac{cfg} analysis
than that done here,
as the targets of indirect branches cannot necessarily be determined statically.
As all other forms of conditional branching have only two branches,
the above data structure uses the if-then-else statement to represent all forms of
conditional branching.
The lesser restriction on control flow extraction is that the algorithm is not optimal
due to the just-mentioned fact that if-then-else statements
are the only method of representing multiple branches
as well as the restrictions on loop structure.
This is elaborated on in \cref{sse:code_dup},
which provides an example where one basic block is repeated twice
in the generated \ac{scf}.

\subsection{Extraction Algorithm}
\begin{algorithm}
  \caption{Control flow extraction}\label{algo:cf}
  \begin{algorithmic}
    \Require{A subgraph $\var{sg}=(B,E,L)$ and its entry block}
    \Ensure{Result is syntactic control flow of type $\scf(B,E_F)$}
    \Function{cfgToScf}{$\var{sg},\var{entry}$}
      \State $\var{sccs}\gets\Call{sccs}{\var{sg}}$
        \Comment{Implicit argument to subcalls, along with $\var{entry}$}
        \label{line:scc1}
      \If{$\abs{\var{sccs}}=1$}\nomenclature{$\abs\cdot$}{Indicates size of~$\cdot$}
          \Comment{Only one \acs*{scc} present, may be a recursive loop}
        \State $\var{sccs}\gets\Call{sccs}{\var{sg}-\text{edges to }\var{entry}}$
        \label{line:scc2}
      \EndIf
      \State \letin{[\var{entry},b_1,\dotsc,b_k]}
      {\var{entry}\ll b_1\ll\dotsb\ll b_k\ll\bot}%
      \nomenclature{$\ll$}{Indicates post-dominance when used with blocks/nodes;
        in this dissertation, it is restricted to immediate post-dominance}
      \label{line:ipdoms}
      \State\Return $\Call{stmts}{\var{entry},b_1,\true}\ASeq
      \Call{stmts}{b_1,b_2,\false}\ASeq\dotsb\ASeq
      \Call{stmts}{b_k,\bot,\false}$
    \EndFunction
    \Function{stmts}{$b,b_j,\var{first}$}
      \State\Return $\begin{cases}
        \ASkip & \text{if }b=b_j \\
        \AContinue & \text{if }b=\var{entry}\wedge\neg\var{first} \\
        \ABreak~\ID(\Call{pre}{b}) & \text{if }\var{sccs}(b)=\bot \\
        \ABB~b & \text{if }\abs{\Call{post}{b}}=0 \\
        \AWhile~\ABB~b~\AOd & \text{if }\Call{post}{b}=\{b\}
      \end{cases}$\label{line:prepost}
      \State \letin{b'}{b\ll b'}\Comment{$b'$ may be~$b_j$}
      \State $a_0\gets\begin{cases}
        \Call{loop}{b,b'} & \text{if }\abs{\var{sccs}(b)}>1 \\
        \ABB b & \text{if }\abs{\Call{post}{b}}=1 \\
        \Call{singleLoop}{b,b_e} & \text{if }\Call{post}{b}=\{b,b_e\} \\
        \Call{ite}{b,b_0,b_1,b'} & \text{if }\Call{post}{b}=\{b_0,b_1\}
      \end{cases}$\label{line:post}
      \State $a_1\gets\begin{cases}
        \Call{stmts}{b',b_j,\false} & \text{if }b'\neq\bot \\
        \ASkip & \text{otherwise}
      \end{cases}$
      \State\Return $a_0\ASeq a_1$
    \EndFunction
    \Function{singleLoop}{$b,b_e$}
      \State\Return $\AWhile~\ABB~b\ASeq
      \AIf~L(b,b_e)~\AThen~\ABreak(\ID b)~\AElse~\ASkip~\AFi~\AOd$
    \EndFunction
    \Function{ite}{$b,b_0,b_1,b'$}
        \State\Return $\ABB~b\ASeq\AIf~L(b,b_0)~\AThen~
        \Call{stmts}{b_0,b',\false}~\AElse~\Call{stmts}{b_1,v',\false}~\AFi$
    \EndFunction
    \Function{loop}{$b,b'$}\label{line:loop}
      \State $\var{scc}\gets\var{sccs}(b)$
      \State $a\gets\Call{cfgToScf}{\var{scc},b}$
      \State $\var{resumes}\gets\{(i,a')\mid
      \exists b_0~b_e\cdot i=\ID b_0\wedge
      \Call{exit}{b_0,b_e,\var{scc}}\wedge a'=\Call{stmts}{b_e,b',\false}\}$
      \State\Return $\AWhile~a~\AOd~\AWhileResume~\var{resumes}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\Cref{algo:cf} presents the methodology for control flow extraction
used in this chapter.
The first input is a (sub-)\ac{cfg}, an edge-labeled graph $(B,E,L)$.
$B$ is the set of all vertices in the graph, the basic blocks.%
\nomenclature{$B$}{Concrete type of basic blocks}%
\index{basic block}
These blocks are represented by their start and end addresses.
Following on from~$B$, $E:B\rightarrow\powerset(B)$%
\nomenclature{$\powerset(\cdot)$}{Indicates power set of $\cdot$}
indicates the edges originating from each block
and $L:B\times B\rightarrow E_F$ is a labeling function.
The label type,~$E_F$, consists of logical expressions over flags.%
\nomenclature{$E_F$}{Label type, logical expressions over flags}
Thus, the control flow extraction algorithm
produces \iac{scf} with $\beta=B$ and $\Phi=E_F$.

The algorithm, \textsc{cfgToScf}, operates recursively,
starting with the full \ac{cfg} of the function under consideration.
It decomposes the \ac{cfg} into a sequence of syntactic statements
using the subfunction \textsc{stmts}, which also operates recursively.
If any potentially non-trivial loops are encountered
in the execution of \textsc{stmts},
\textsc{cfgToScf} is again called with the bodies of those loops as subgraphs.

The following paragraphs elaborate on the notation and functions
used in the text of the algorithm.
Functions \textsc{pre} and \textsc{post} (\cref{line:prepost,line:post})
respectively return the parent and child blocks of the block that is their argument.
% \bot explained in Nomenclature, no need to do so here
The helper function \textsc{sccs} (\cref{line:scc1,line:scc2})
returns the \acp{scc} of the subgraph supplied to it,
the subgraphs of that subgraph that are all \emph{strongly connected}.%
\index{strongly connected}
A strongly connected graph is one
where each of its vertices is reachable from every other vertex.
\Iac{scc} is trivial if it consists of only one element and is not a loop.
The \acp{scc} are stored as a partial surjection%
\index{surjection}
from blocks to their corresponding \acp{scc}.
If block~$b$ is not in subgraph~$\var{sg}$,
then $\textsc{sccs}(\var{sg})(b)$ returns~$\bot$.
Function $\textsc{exit}(b_0,b_1,\var{sg})$ checks if~$b_1$ is an \emph{exit child}%
\index{exit child}
of~$b_0$; that is, it checks if~$b_1$ is a child of~$b_0$,
that~$b_0$ is in $\var{sg}$, and that~$b_1$ is not.

Next, notation $b_0\ll b_1$ expresses that block~$b_1$
\emph{immediately post-dominates}~$b_0$.%
\index{dominance!post!immediate}
This means two things:
\begin{enumerate}
  \item $b_1$ strictly post-dominates~$b_0$:
  all paths to an exit block of the \ac{cfg} that start at~$b_0$
  must go through~$b_1$ and $b_0\neq b_1$.
  \item $b_1$ does not strictly post-dominate
  any other strict post-dominators of~$b_0$.
\end{enumerate}
The paths involved in post-dominance calculation include those in loops,
so if a \ac{cfg} contains a block~$b_0$
that loops back on itself and has one child~$b_1$,
then~$b_1$ is still considered the immediate post-dominator of~$b_0$.%
\index{dominance!post!immediate}
Only exit blocks lack immediate post-dominators, a situation denoted as $b\ll\bot$.
As every block has at most one immediate post-dominator
and no other block in its path will have the same one,
there exists a unique chain of blocks formed by the immediate post-dominance relation
for every starting block that leads to an exit.

Now that the terminology and notation not elsewhere defined is out of the way,
the behavior of \textsc{cfgToScf} itself can be discussed.
It starts by computing the \acp{scc} of the supplied subgraph,~$\var{sg}$.
If~$\var{sg}$ consists of a single non-trivial \ac{scc}, it is a loop,%
\index{loop}
a situation that only happens when the function is recursive.%
\index{recursion}
Otherwise, the \ac{cfg} either has multiple \acp{scc} or one trivial \ac{scc}.
As the recursive loop situation cannot be handled in that exact form,
it is broken by removing all back edges to the entry block
and recomputing the \acp{scc}.
The immediate post-dominance chain of blocks in the subgraph
that follow from the entry block is then produced (\cref{line:ipdoms})
and used to construct a sequence of \acl{scf} statements
extracted from every immediate post-dominance pair,
ending with a statement extracted from the exit block.

Statement extraction occurs in the \textsc{stmts} function.
It is also recursive, operating from block~$b$ to a \emph{joint} block,~$b_j$,%
\index{basic block!joint}
and has multiple base cases.
The first is when execution has reached the joint block,
in which case the function produces \texttt{Skip}.
Alternatively, if~$b$ is the entry block for the current subgraph
but this is not the first time \textsc{stmts} has been called
for the current invocation of \textsc{cfgToScf},
then the \ac{cfg} has jumped back to the start of a loop
and the result should be \texttt{Continue}.%
\index{loop!continue}
There is also the possibility of the current block
not being present in the current set of \ac{scc} and thus not in the current subgraph,
indicating that a loop has been exited.
In that case, a \texttt{Break} is generated%
\index{loop!break}
and annotated with the ID of the parent block, the one that caused the exit.
The next two cases to consider are based on the children of the current block~$b$.
If it has no children, then the result is a single \texttt{Block}
with~$b$ as its argument.
In contrast, if its only child is itself then it is a simple, one-block infinite loop
and is returned as such.

In the remaining cases,
the algorithm produces one or more sequences of \ac{scf} components;
the components from~$b$ to its immediate post-dominator~$b'$ (stored in~$a_0$)%
\index{dominance!post!immediate}
sequenced with those from~$b'$ to~$b_j$ ($a_1$, which may be \texttt{Skip} if,
for whatever reason,~$b$ has no immediate post-dominator).
If there are multiple \acp{scc} in the current mapping,
a non-trivial loop is present and requires loop analysis between~$b$ and~$b'$,
performed by \textsc{loop}.
Next, if~$b$ only has one child, it can be mapped to a simple \texttt{Block}.
The two remaining cases involve~$b$ having two children,
meaning there is a conditional jump.
If one of~$b$'s children is itself and the other is some block~$b_c$,
then it forms a single-block loop that exits to~$b_c$.
Otherwise,~$b$ and its two children produce an if-then-else statement.

Loop extraction in \textsc{loop} (\cref{line:loop}) requires two steps.%
\index{loop!extraction}
First, the body of the loop is extracted
using a (recursive) call to \textsc{cfgToScf}.
Second, the \ac{scf} components between every exit child of the current \ac{scc}
and the joint block~$b'$ are stored in a set, associated with the corresponding%
\index{basic block!joint}
exit child's ID, and supplied to the loop's \texttt{Resume} statement.
This ensures that the control flow is properly configured
for all exit points of the loop.

\begin{example}
  \Cref{fig:ex_cf} provides an example of \ac{scf} extracted from \iac{cfg}.
  At the first call of \textsc{cfgToScf},
  the post-dominator relation is $0\ll 6\ll\bot$
  and thus $\textsc{stmts}(0,6,1)$ and $\textsc{stmts}(6,\bot,0)$ are computed.
  For the first call to \textsc{stmts},
  term~$a_0$ becomes an if statement computed by $\textsc{ite}(0,1,7,6)$
  and term~$a_1$ is computed by $\textsc{stmts}(6,6,0)$.
  The computed if statement has two branches,
  with this example considering the branch where~$f_0$ holds.
  Extraction for that branch computes $\textsc{stmts}(1,6,0)$.
  As $1\ll 5$, that call results in $a_0=\textsc{loop}(1,5)$
  and $a_1=\textsc{stmts}(5,6,0)$.
  The loop body in~$a_0$ is extracted from subgraph $\{1,2\}$,
  while the paths from~$4$ to~$5$ and~$3$ to~$5$ continue after the loop.
  Term~$a_1$ is simply $\ABB~5$.
\end{example}

\begin{figure*}
  \hspace*\fill
  \subcaptionbox{Example \ac*{cfg}\label{fig:ex_cf_cfg}}{
    \begin{tikzpicture}[->, >=stealth, node distance=0.75cm]
      \node[draw=none] (0) {0};
      \node[draw=none] (1) [below=of 0] {1};
      \node[draw=none] (2) [below=of 1]{2};
      \node[draw=none] (3) [below=of 2]{3};
      \node[draw=none] (4) [left=of 3, xshift=0.5cm, yshift=0.5cm]{4};
      \node[draw=none] (5) [below=of 3]{5};
      \node[draw=none] (6) [below=of 5, xshift=0.5cm, yshift=0.5cm]{6};
      \node[draw=none] (7) [right=of 1]{7};
      \node[draw=none] (8) [right=of 3, xshift=0.5cm, yshift=0.5cm]{8};
      \node[draw=none] (9) [right=of 5]{9};
      
      \path (0) edge node[left]{$f_0$} (1);
      \draw[rounded corners=2mm] (0) -|
        node[above, xshift=0.4cm, yshift=-0.9cm]{$\neg f_0$} (7);
      \path (1) edge [bend left] node[right]{$f_1$} (2);
      \draw[rounded corners=2mm] (1) -| node[left, yshift=-1.3cm]{$\neg f_1$} (4);
      \path (2) edge [bend left] node[left, xshift=0.1cm]{$f_2$} (1);
      \path (2) edge node[right]{$\neg f_2$} (3);
      \path (3) edge node[right]{} (5);
      \draw[rounded corners=2mm] (4) |- (5);
      \path (5) edge node[right]{} (6);
      \draw[rounded corners=2mm] (7) -| node[right, yshift=-1.3cm]{$\neg f_7$} (8);
      \path (7) edge node[left, yshift=-1cm]{$f_7$} (9);
      \draw[rounded corners=2mm] (8) |- (9);
      \path (9) edge node[right]{} (6);
    \end{tikzpicture}
  }
  \hfill
  \subcaptionbox{Syntactic Control Flow}{
    \(\begin{array}{l}
      \ABB~0\ASeq* \\
      \AIf~f_0~\AThen \\
      \ind{4ex}\AWhile \\
      \ind{8ex}\ABB~1\ASeq* \\
      \ind{8ex}\AIf~f_1~\AThen \\
      \ind{12ex}\ABB~2\ASeq* \\
      \ind{12ex}\AIf~f_2~\AThen~\AContinue~\AElse~\ABreak~2~\AFi \\
      \ind{8ex}\AElse \\
      \ind{12ex}\ABreak~1 \\
      \ind{8ex}\AFi \\
      \ind{4ex}\AOd~\AWhileResume~\{(2,\ABB~3),(1,\ABB~4)\} \\
      \ind{4ex}\ABB~5 \\
      \AElse \\
      \ind{4ex}\ABB~7\ASeq* \\
      \ind{4ex}\AIf~f_7~\AThen~\ASkip~\AElse~\ABB~8~\AFi\ASeq* \\
      \ind{4ex}\ABB~9 \\
      \AFi\ASeq* \\
      \ABB~6
    \end{array}\)
  }
  \hspace*\fill
  \caption{Example of control flow extraction}\label{fig:ex_cf}
\end{figure*}

\subsubsection{Code Duplication}\label{sse:code_dup}
The algorithm is not optimal in terms of generated \ac{scf} size
as basic blocks may be duplicated.
There are two situations where basic block duplication occurs,
one less common and one more common.
The less common situation is when \iac{scc} has multiple entry blocks.
This corresponds to a loop having multiple entry points,
which can occur in situations that involve less-structured control flow,
such as a C program that jumps into a loop using \texttt{goto}.
Such situations are relatively uncommon, even in optimized code.
If it does happen, the entire loop must be duplicated.
The more common situation, by contrast, involves complex conditional branching
that can occur even without loops.
\begin{example}
  \Cref{fig:ex_nonopt} shows a small example of branching control flow
  that results in $\ABB~3$ being duplicated.
  That block could itself be an even more complicated subgraph,
  possibly leading to exponential code duplication.
\end{example}
\begin{figure*}
  \hspace*\fill
  \subcaptionbox{\acs*{cfg}}{
    \begin{tikzpicture}[>=stealth, ->, node distance=0.75cm]
      \node[draw=none] (0) {0};
      \node[draw=none] (1) [right=of 0] {1};
      \node[draw=none] (2) [right=of 1] {2};
      \node[draw=none] (3) [right=of 2] {3};
      \node[draw=none] (4) [right=of 3] {4};
      
      \path (0) edge node[below]{$f_0$} (1);
      \path (0) edge [bend left] node[above]{$\neg f_0$} (4);
      \path (1) edge node[above]{$f_1$} (2);
      \path (1) edge [bend right] node[below]{$\neg f_1$} (3);
      \path (2) edge node[above]{$f_2$} (3);
      \path (2) edge [bend right] node[below]{$\neg f_2$} (4);
      \path (3) edge (4);
    \end{tikzpicture}
  }
  \hfill
  \subcaptionbox{\acs*{scf}}{
    \(\begin{array}{l}
      \ABB~0; \\
      \AIf~f_0~\AThen~\ABB~1; \\
      \ind{4ex}\AIf~f_1~\AThen~\ABB~2; \\
      \ind{8ex}\AIf~f_2~\AThen~\ABB~3 \\
      \ind{8ex}\AElse~\ASkip~\AFi \\
      \ind{4ex}\AElse~\ABB~3~\AFi \\
      \AElse~\ASkip~\AFi; \\
      \ABB~4
    \end{array}\)
  }
  \hspace*\fill
  \caption{Example of code duplication}\label{fig:ex_nonopt}
\end{figure*}

\begin{comment}
\subsection{Extraction Algorithm Analysis}
The control flow extraction algorithm presented above
does not necessarily need any formal reasoning as it is not included in the \ac{tcb}
for this chapter.
However, showing that it \emph{terminates} and is \emph{sound} still has worth.%
\index{termination}%
\index{soundness}
Thus, a proof of correctness for a modified version
of the syntactic control flow data structure,
excluding if-then-else statements and using direct recursion,
was formulated in Isabelle/HOL.
As a complement,
this subsection presents informal proofs of termination and soundness
for the extraction algorithm.
It also provides an example where the algorithm is not optimal
due to syntactic duplication caused by the limited branch representation.

\subsubsection{Termination}
Showing termination is slightly tricky as the two main functions involved,
\textsc{cfgToScf} and \text{stmts}, are mutually recursive.%
\index{recursion!mutual}
However, if a separate strictly-decreasing \emph{measure}%
\index{recursion!measure}
can be shown for each function, termination as a whole will then follow.

\begin{proof}
  The measure for \textsc{cfgToScf} is based on its subgraph argument.
  Every subcall will have a strictly smaller number of blocks
  in its supplied subgraph.
  This can be seen as, if the entire graph is strongly connected
  such that all of its blocks are in one single \ac{scc},
  the algorithm will remove the back edges to the entry block
  and then recompute the \acp{scc}.
  By the definition of strong connection,%
  \index{strongly connected}
  that operation ensures there will be two or more smaller \acp{scc}
  for the modified graph.
  As any further calls to \textsc{cfgToScf} will take one of those \acp{scc},
  the reduction in blocks is guaranteed and thus so is \text{cfgToScf}'s termination.
  
  Termination of function \texttt{stmts} instead relies on the fact
  that it is always called under the condition $b\ll b_j$ (except when $b=b_j$).
  That precondition ensures that all paths starting from~$b$ that reach the exit
  must necessarily go through~$b_j$.
  As the function recursively explores the children of~$b$,
  having base cases when there are no more blocks to explore,
  the only possibility for non-termination is a loop occurring before~$b_j$.
  That loop may be either a single-block loop or \iac{scc} with more than one element.
  Neither case results in a direct recursive call of \texttt{stmts},
  thus ensuring termination.
\end{proof}

\subsubsection{Soundness}
The proof of soundness for control flow extraction,
showing that the semantics%
\index{semantics}
of \iac{cfg} is equivalent to those of \iac{scf},
is roughly as follows.
\begin{proof}
  This informal proof is expressed using potentially-infinite \emph{execution paths}.%
  \index{execution!path}
  As an example, one possible execution path for \cref{fig:ex_cf_cfg}
  is $[0, 7, 9, 6]$.
  The semantics of both \iac{cfg} and a term of type $\scf(B,E_F)$%
  \index{semantics}
  can be expressed in terms of such execution paths.
  With~$\Pi$ as the type of execution paths,%
  \nomenclature{$\Pi$}{Type of execution paths}
  $\sem_\mathrm{CFG}:B\rightarrow\powerset(\Pi)$ and
  $\sem_\mathrm{SCF}:\scf(B,E_F)\rightarrow\powerset(\Pi)$ respectively denote
  \ac{cfg} and \ac{scf} semantics.
  The first function takes \iac{cfg} entry point as input
  and ``runs'' the \ac{cfg} from there.
  The second function takes \iac{scf} as input and ``runs'' it.
  Soundness for function \textsc{cfgToScf} is defined as follows:
  \begin{equation}
    \forall b\cdot\sem_{CFG}(b)=\sem_\mathrm{SCF}(\textsc{cfgToScf}(\var{cfg},b)).
  \end{equation}
    Meanwhile, for \textsc{stmts}, the definition is:
  \begin{equation}
    \forall b~b_j~f\cdot b\ll b_j\longrightarrow\takeuntil(\sem_\mathrm{CFG}(b),b_j)
    =\sem_\mathrm{SCF}(\textsc{stmts}(b,b_j,f)).
  \end{equation}
  Function $\takeuntil$ takes a set of execution paths
  and returns all the path prefixes
  that come before the first occurrence of joint block~$b_j$.

  The proof itself is done by induction over \textsc{cfgToScf},%
  \index{induction}
  which produces a mutual induction scheme due to the mutual recursion involved.
  
  The first step considers the \emph{sequentialization}
  of terms $\textsc{stmts}(b_i,b_{i+1},t)$.
  The induction hypothesis provides soundness for each of those terms.
  As $b_0\ll b_1\ll\dotsb$, the semantics of the \ac{cfg}%
  \index{semantics}
  produces execution paths that can be split up into multiple fragments,
  $[\pi^0_0, \pi^0_1,\dotsc]$.
  Each fragment~$\pi^0_i$ starts at~$b_i$
  and consists of the prefix of the execution path
  up until the first occurrence of $b_{i+1}$.
  The result returned by \textsc{cfgToScf} is of the form~$c_0;c_1;\dotso$,
  which also consists of fragments in the form $[\pi^1_0,\pi^1_1,\dotsc]$.
  The induction hypothesis shows that for all~$i$, $\pi^0_i=\pi^1_i$.
  
  The next step considers the soundness of \textsc{stmts},%
  \index{soundness}
  which is largely straightforward.
  In the trivial case where $b=b_j$, both semantics are~$[]$, the empty path.%
  \index{semantics}
  That is,
  \begin{equation}
    \takeuntil(\sem_\mathrm{CFG}(b),b)=\sem_\mathrm{SCF}(\textsc{stmts}(b,b,f))=
    \sem_\mathrm{SCF}(\ASkip)=\{[]\}.
  \end{equation}
  The case for loops is more intricate.%
  \index{loop}
  The semantics of a loop with entry point~$b_e$%
  \index{loop!entry}
  is defined as the set of paths from~$b_e$ that can be split into three subpaths.
  The first subpath consists of the loop iterations,
  a list of any number of paths from~$b_e$ to~$b_e$.
  The second subpath is a path from~$b_e$ to an exit.
  The third subpath is a \emph{resume}, a path from an exit to the joint~$b_j$.%
  \index{joint}
  The algorithm produces a loop only if~$b_e$ is in a non-trivial \ac{scc}.
  Under that condition, the set of paths between~$b_e$ and the joint
  is exactly equal to the semantics of a loop.
\end{proof}
\end{comment}

%\subsection{Memory Region and Region Relation Generation}\label{sse:mem_reg}
%This process primarily relies on unverified symbolic execution in the generated code.
% wanted to discuss Z3 usage a little but I don't have time.

\subsection{Symbolic Execution}\label{sse:syntax_symb}
In \cref{sse:cfg_extract}, the semantics of assembly were expressed%
\index{semantics!assembly}
in terms of control flow between basic blocks.
This section now covers the symbolic execution of those individual blocks.
The Haskell symbolic execution engine
takes as input a data structure of type $\scf(B,E_F)$,
which is formulated over basic blocks,
and produces $\scf(\powerset(\asp),\esp)$,%
\nomenclature{$\asp$}{Type of assignments}
which is formulated over sets of assignments.
It keeps track of all used memory regions,
both the actual regions used by instructions as well as merged regions,
in order to supply those regions as part of an \ac{fmuc}.

\subsubsection{Avoiding Memory Aliasing}
Because symbolic execution uses symbolic state,
the relations of enclosure, separation, and overlap,
defined in \cref{sse:memory_aliasing}, must be determined for symbolic expressions.
Unfortunately, there is no single solution, no one decision procedure,
that can determine these properties for all symbolic expressions.

As an example of the potential issues that can occur,
take the completely symbolic regions $r_0=\region{a_0}{s_0}$
and $r_1=\region{a_1}{s_1}$.
Without additional information, we cannot determine any relations for these regions.
If they are \emph{possibly} different then they must be treated as different regions,
while if they \emph{necessarily} overlap
then they must be treated as a single merged region.

To deal with such symbolic issues,
the three aforementioned relations of enclosure, separation, and overlap
are formulated as \ac{smt} problems.
\Ac{smt} formulations generally require negating the desired condition
and then proving that the resultant problem is unsatisfiable.
Thus, for separation, we have the below definition.
\begin{definition}[\ac{smt} separation]
  Two symbolic regions $r_0=\region{a_0}{s_0}$ and $r_1=\region{a_1}{s_1}$
  are necessarily separate if and only if, for any address~$a$,
  the following \ac{smt} problem is unsatisfiable:
  \begin{equation*}
  a_0\leq a<a_0+s_0\wedge a_1\leq a<a_1+s_1.
  \end{equation*}
\end{definition}
Overlap may occur if two regions are not necessarily separate.
\begin{definition}[\ac{smt} overlap]
  The regions~$r_0$ and~$r_1$ necessarily overlap if and only if
  the following \ac{smt} problem is unsatisfiable:
  \begin{equation*}
  (a_0<a_1\vee a_0\geq a_1+s_1)\wedge(a_1<a_0\vee a_1\geq a_0+s_0).
  \end{equation*}
\end{definition}
Enclosure is 
\begin{definition}[\ac{smt} enclosure]
  Region $r_0$ is enclosed by $r_1$ if and only if,
  for any address~$a$, the following \ac{smt} problem is unsatisfiable:
  \begin{equation*}
  a_0\leq a<a_0+s_0\wedge(a<a_1\vee a \geq a_1 + s_1).
  \end{equation*}
\end{definition}
These \ac{smt} problems can be solved by Z3~\citep{de2008z3}
for a wide range of expressions over bitvectors
using the \texttt{QF\_UFBV} theory~\citep{barrett2017smt}.
Z3 is also used in this work for determining the sign of two values
in the region merge algorithm, originally presented in \cref{def:merge}.
Additionally, reads of overlapping regions may require merging
and separation analysis as described in \cref{sse:memory_aliasing},
so they also rely on Z3.

\subsection{Invariant Generation}\label{sse:inv_gen}
Invariants, formalized as sets of assignments of the aforementioned type~$\asp$,
\index{invariant}
are generated by starting from a precondition for the entry point of the function
and \emph{propagating} it throughout.%
\index{invariant!propagation}

The initial precondition of the function as a whole is generated
by including initial symbolic values for all registers that are read
before they are written as well as all used memory regions
that are not enclosed in another.
The concrete initial value of the instruction pointer, \inlineasm{rip},
must also be included,
and the (symbolic) address to return to after function completion
must be indicated as stored on the stack.
In Haskell, the conditions in question are represented as sets of assignments.
\begin{example}
  To reuse \cref{ex:simple}, its initial precondition would be:
  \begin{equation}
    \phi=\{\mathrip\coloneqq\mathtt{a0},
    \mathrsp\coloneqq\rspo,
    \region{\mathrsp-8}{8}\coloneqq v_0,
    \region{\mathrsp}{8}\coloneqq\retaddr\}.
  \end{equation}%
  \nomenclature{$\phi$}{Denotes a generated invariant}
\end{example}
Propagation requires performing \emph{substitution},%
\index{invariant!substitution}
which is defined over assignments, state parts, and expressions,
all with respect to invariant~$\phi$.
\begin{subequations}
  \begin{align}
    \subst(\phi,\var{sp}\coloneqq v) &= \subst(\phi,\var{sp})\coloneqq\subst(\phi,v)\\
    \subst(\phi,\var{sp}) &= \text{if }\exists v\cdot(\var{sp},v)\in\phi
    \text{ then }v\text{ else }\var{sp} \\
    \subst(\phi,e_0\bop e_1) &= \text{if }\exists v\cdot(e_0\bop e_1,v)\in\phi
    \text{ then }v\text{ else }\subst(\phi,e_0)\bop\subst(\phi,e_1) \\
    \text{unary ops} &= \dotso \notag \\
    \text{ternary ops} &= \dotso \notag
  \end{align}
\end{subequations}%
\nomenclature{$\bop$}{Denotes an arbitrary binary operator}

\Cref{algo:prop} performs invariant propagation.
Each block is modified by applying all applicable substitutions
with respect to~$\phi$.
Invariant~$\phi$ is then modified based off of the semantics of the block.%
\index{semantics}
Treating~$\alpha$ as the set of assignments in the block,
$\phi$ is modified by taking the subset of substitutions
where the substitutees are overwritten by~$\alpha$
and combining them with the subset of substitutions
that were completely unmodified by any assignment in~$\alpha$:
\begin{equation}
  \post(\phi,\alpha)\footnote{%
    This is a different $\post$ from that used to identify \ac{cfg} block children.
  }\equiv\{(v,e)\mid(v\coloneqq e\in\alpha\wedge(v,\_)\in\phi)
  \vee((v,e)\in\phi\wedge(v,e)\text{ is unmodified by }\alpha)\}.
\end{equation}
\begin{example}
  Once again consider \cref{ex:simple}.
  Propagation of the initial precondition through the single basic block
  produces the following postcondition:
  \begin{equation}
    \begin{split}
      \phi=\{\mathrip &\coloneqq\retaddr, \\
      \mathrsp &\coloneqq\rspo+8, \\
      \region{\rspo-8}8 &\coloneqq\mathtt{0xAABBCCDD}\concat
      \takebits{31,16}v_0\concat\mathtt{0xEEFF}, \\
      \region{\rspo}8 &\coloneqq\retaddr\}.
    \end{split}
  \end{equation}
\end{example}
\begin{algorithm}
  \caption{Invariant propagation}\label{algo:prop}
  \begin{algorithmic}
    \Require{Input is of type $\scf(\powerset(\asp),\esp)$}
    \Ensure{Output is a tuple of possibly-updated~$\phi$
      and \ac{scf} updated with current~$\phi$:
      $\asp\times\scf(\powerset(\asp),\esp)$}
    \Function{prop}{$\phi,\ABB~\alpha$}
      \State $\phi'\gets\post(\phi,\subst(\phi,\alpha))$
      \State\Return $(\phi',\ABB~(\alpha\text{ annotated with }\phi))$
    \EndFunction
    \Function{prop}{$\phi,\alpha_0\ASeq \alpha_1$}
      \State $(\phi',\alpha_0')\gets\Call{prop}{\phi,\alpha_0}$
      \State $(\phi'',\alpha_1')\gets\Call{prop}{\phi',\alpha_1}$
      \State\Return $(\phi'',\alpha_0'\ASeq \alpha_1')$
    \EndFunction
    \Function{prop}{$\phi,\AIf~f~\AThen~\alpha_0~\AElse~\alpha_1~\AFi$}
      \State $(\phi_0,\alpha'_0)\gets\Call{prop}{\phi,\alpha_0}$
      \State $(\phi_1,\alpha'_1)\gets\Call{prop}{\phi,\alpha_1}$
      \State $\phi'\gets\phi_0\cap\phi_1$
      \State\Return $(\phi',\AIf~\subst(\phi,f)~\AThen~
        \alpha'_0~\AElse~\alpha'_1~\AFi)$
    \EndFunction
    \Function{prop}{$\phi,\AWhile~\alpha~\AOd$}
      \State $(\phi',\alpha')\gets\Call{prop}{\phi,\alpha}$
      \If{$\phi\subseteq\phi'$}%
        \nomenclature{$\subseteq$}{Indicates subset relation}
        \State\Return $(\phi,\AWhile~\alpha'~\AOd)$
      \Else
        \State\Return $\Call{prop}{\phi\cap\phi',\AWhile~\alpha~\AOd}$
      \EndIf
    \EndFunction
    \Function{prop}{$\phi,\AWhileResume~\var{resumes}$}
      \ForAll{$\alpha_i\in\var{resumes}$}
        \State $(\phi'_i,\alpha'_i)\gets\Call{prop}{\phi,\alpha_i}$
      \EndFor
      \State $\phi''\gets\bigcap\phi'$
      \State\Return $(\phi'',\AWhileResume~\Call{zip}{i,\alpha'})$
    \EndFunction
    \Function{prop}{$\phi,\alpha$}\Comment{Default case}
      \State\Return $(\phi,\alpha)$
    \EndFunction
  \end{algorithmic}
\end{algorithm}
Invariant propagation is straightforward for sequencing and if statements,
with sequencing simply chaining invariant propagation
and if statements producing an invariant that is the common result
of propagating the initial invariant down both branches.

In contrast, a loop with body~$\alpha$ requires continual propagation
until the invariant~$\phi$ stabilizes, possibly by becoming~$\varnothing$.
This stabilization is identified
by checking if~$\phi$ is a subset of its propagated self.
If it is, then \textsc{prop} returns the propagated~$\phi$
and a new loop with the propagated body.
Otherwise,
the original loop is propagated again with the intersection of~$\phi$
and its propagated self.
This process effectively computes the loop invariant
as the greatest subset of the initial invariant
that is preserved by execution of the loop body.
For loops that have multiple exits,
each exit's resume is propagated with the invariant at the point of exit evaluation.
In a similar fashion to the process for if statements,
the invariants that result from individual resume propagation
are intersected to produce a singular invariant for all resumes,
which is then returned along with all of the propagated resumes.

\section{FMUC Verification}\label{se:fmuc_ver}
This section presents verification of \iac{fmuc} as shown in \cref{fig:verify},
one of the primary contributions for this chapter as mentioned in its preamble.
Both the \ac{fmuc} and the original assembly are loaded into Isabelle/HOL,
where the memory usage theorem is then proven using the proof ingredients
provided by the \ac{fmuc}.
By this method, which requires a step function
that models the semantics of the assembly instructions
and a process to apply it repeatedly,
the \ac{fmuc}'s memory usage Hoare triple can be verified.

\begin{figure*}
  \centering
  \begin{tikzpicture}[->, >=stealth, every node/.style={
    draw,
    rounded corners,
    text width=3cm,
    text centered,
    minimum height=1.5cm
  }]
    \node (a) {Isabelle/HOL};
    \node[above left=of a] (b) {Assembly};
    \node[below left=of a] (c) {Certificate};
    \node[right=1.5cm of a] (d) {OK/unproven};
    
    \draw (b) -- (a.north west);
    \draw (c) -- (a.south west);
    \draw (a) -- (d);
  \end{tikzpicture}
  \caption{Overview of \acs*{fmuc} verification}\label{fig:verify}
\end{figure*}

\subsection{Syntactic Control Flow in Isabelle/HOL}\label{isabelle_scf}
As described previously, \acl{scf} is a representation of the control flow of a function
in terms of syntactic structures such as basic blocks,
loops, and if-then-else statements.
While very similar to the \ac{scf} used when generating \acp{fmuc},
there are some modifications that must be made when the generated \acp{scf}
are to be loaded into Isabelle/HOL.
These modifications are required due to subtle differences in the semantics
of the generating tool versus the verifying tool,
and are required to properly support the Hoare rules
described in \cref{scf_hoare} below.

In the Isabelle/HOL representation,
there are no \texttt{Break}s or \texttt{Continue}s;
any occurrences of such statements are translated to \texttt{Skip}.
This does mean that programs that cannot be easily transformed
such that that translation does not modify the overall semantics
are not easily handled in this framework.
However, none of functions encountered in the case study presented in \cref{se:xen}
had that issue, so it does not appear to be a significant drawback.

Additionally, loops in the Isabelle/HOL \ac{scf} syntax
do rely on a explicit exit condition.
This condition is simply the precondition of the entry block of the loop
as generated using the methodology in \cref{sse:inv_gen}.

Another important difference is that basic blocks in Isabelle
take the form $\ABB~\Block*{n}{a}{i}$,
where~$n$ indicates the number of instructions in the block,~$a$ is the address
of the last instruction in the block, and~$i$ is an ID
that uniquely identifies the block in the current \ac{scf}.
This style is used to assist with the symbolic execution methodology described in
\cref{sse:syntax_ver_symb}.

Finally, to properly handle function calls in the Isabelle/HOL syntax,
the analyzed \acp{cfg} are preprocessed prior to performing extraction
in order to isolate \inlineasm{call} instructions into their own basic blocks.
% Freek did the preprocessing but I did all the rest.
These single-instruction blocks are then translated into \texttt{Call}~$f$ entries
in the Isabelle/HOL \ac{scf}, where~$f$ is the textual label of the function called.
This allows for proper matching with the Hoare rules presented below.

\subsection{Symbolic Execution}\label{sse:syntax_ver_symb}
\Cref{cfg_symb_exec} previously presented a formal symbolic execution engine
based on the machine model described in \cref{se:machine_model}.
It provides a function $\run$ that describes the symbolic execution of
blocks in a control flow graph.

The formal function for block-level symbolic execution presented in this chapter,%
\index{symbolic execution}
by contrast, is a \emph{transition relation}%
\index{transition relation} formulated as
$\execblock:\mathbb{N}\times W\times\mathbb{N}\times S\times S\rightarrow\mathbb{B}$.%
\nomenclature{$\mathbb{N}$}{Type of natural numbers}%
\nomenclature{$W$}{Type of 64-bit words}
Its inputs are the number of instructions left to execute in the block,
the address of the last instruction in the block, the block's ID,
the current state~$\sigma$, and an ending state~$\sigma'$.
Its result is true if and only if execution
starting from the current instruction in state~$\sigma$
and running to the ending address can produce state~$\sigma'$.
The other arguments are used to ensure termination and block matching.
Undefined behavior, such as null-pointer dereferencing,
is modeled by relating the state in which it occurs to any successor state
supplied with it.

The internal step function has type $\step:A\times\mathbb{N}\times S\rightarrow S$,%
\nomenclature{$A$}{Type of instructions}
with its first argument being an instruction, its second being the size of the instruction,
and its third being the current state.
The function returns the state after instruction execution,
incrementing \inlineasm{rip} by the supplied size
if it was not changed by a control-flow instruction instead.

The $\execblock$ function is used as part of a function for \ac{scf} execution,
$\exec:\var{SCF}\times S\times S\rightarrow\mathbb{B}$.
In case of loops, that function is defined using a least-fixed-point construction.
Thus, if there are infinite loops present,
the function will have no related successor states.
The function is not actually executed when used in a proof, however;
it exists to allow creation of the Hoare rules shown below in \cref{scf_hoare}.

Unlike the symbolic execution for generation,
this symbolic execution methodology is implemented fully in Isabelle/HOL,
meaning that every rewrite rule has been formally proven correct.

\subsection{Per-Block Verification}\label{sse:per-block}
The verification methodology presented here occurs
by first verifying the functionality of each basic block in the corresponding function.
This is done for each block by proving the lemma shown below,
using the $\exec$ function from the previous section:
\begin{lemma}[Block Verification]\label{per-block-lemma}
  \begin{equation*}
    P(\sigma)\longrightarrow\execblock(n,a,i,\sigma,\sigma')\wedge
    Q(\sigma')\wedge\usage(M(\sigma),\sigma,\sigma')
  \end{equation*}
\end{lemma}
Every generated version of this lemma is discharged
with an Isabelle/HOL proof method written in Eisbach~\citep{matichuk2016eisbach},
Isabelle's proof automation language.
For each block, the method takes the block-related proof ingredients
from the \ac{fmuc} and runs symbolic execution
to prove the postcondition and thus establish the memory usage for the block.
The open variables $P$, $Q$, $n$, $a$, $i$, and $M$ are all provided by the \ac{fmuc}.
No user interaction is required outside of cases where semantics
for specific instructions are unavailable or the Isabelle libraries in use
do not have the right simplification lemmas for automatic reasoning.
Those cases are rare and become rarer as more relevant lemmas are developed,
so for basic blocks, the proof is essentially automated.

\subsection{Function Body Verification}
While symbolic execution works well to establish memory usage on the level of basic blocks,
the goal of this verification effort is to formally establish memory usage
on the function level. This section describes that process,
which occurs after the individual blocks have had their semantics and memory usage derived
and relies on Hoare logic (described in \cref{se:hoare}).

\subsubsection{Hoare Rules}\label{scf_hoare}
The Hoare triple formulation used for this work, $\htriple*{P}f{Q}M$,
resembles traditional Hoare triples a bit more than the version from \cref{ch:cfg},%
\index{Hoare!triple}
as rather than a halting condition
it takes a syntactic representation of the program, an \ac{scf}.
Unlike traditional Hoare triples, however,
it also explicitly contains the set of memory regions,~$M$,
that enumerate the areas of memory read and written by the program
the \ac{scf} encodes.
Syntactic structure is required because Hoare logic is a syntax-guided approach.
\begin{definition}[Hoare triple for \ac{scf}]\label{def:usage}
  \begin{equation}
    \htriple*{P}f{Q}{M}\equiv
    \forall\sigma~\sigma'\cdot P(\sigma)\wedge\exec(f,\sigma,\sigma')\longrightarrow
    Q(\sigma')\wedge\usage(M,\sigma,\sigma')
  \end{equation}
  The above equation states the following:
  if precondition~$P$ holds on the initial state~$\sigma$
  and~$\sigma'$ can be obtained by executing~$f$,
  postcondition~$Q$ holds on the produced state
  and the values stored in all memory regions outside set~$M$ are preserved.
\end{definition}
\begin{example}
  \todo{need example?}
\end{example}
While \cref{def:usage} focuses on the regions written to,
the regions read must also be included as symbolic execution
relies on those regions being included.
Without them, proofs that require symbolically executing
the related instructions will not complete.
\begin{figure*}
  \centering
  \setlength\parskip{1em}
  \subcaptionbox{Introduction rule\label{fig:intro-rule}}{
    \AxiomC{$\begin{multlined}
      \forall\sigma~\sigma'\cdot P(\sigma)\longrightarrow \\
      \execblock(n,a,i,\sigma,\sigma')\wedge{} \\
      Q(\sigma')\wedge\usage(M(\sigma),\sigma,\sigma')
    \end{multlined}$}
    \AxiomC{$M'=\{r\mid\exists\sigma\cdot P(\sigma)\wedge r\in M(\sigma)\}$}
    \BinaryInfC{$\htriple*{P}{\ABB~\Block*{n}{a}{i}}{Q}{M'}$}
    \DisplayProof
  }

  \subcaptionbox{Sequence rule}{
    \AxiomC{$\htriple*{P}f{Q}{M_1}$}
    \AxiomC{$\htriple*{Q}g{R}{M_2}$}
    \AxiomC{$M=M_1\cup M_2$}
    \TrinaryInfC{$\htriple*{P}{f\ASeq g}{R}{M}$}
    \DisplayProof
  }

  \subcaptionbox{Conditional rule}{
    \AxiomC{$\htriple*{P\wedge B}f{Q_1}{M_1}$}
    \AxiomC{$\htriple*{P\wedge\neg B}g{Q_2}{M_2}$}
    \AxiomC{$Q_1\vee Q_2\longrightarrow Q$}
    \AxiomC{$M=M_1\cup M_2$}
    \QuaternaryInfC{$\htriple*{P}{\AIf~B~\AThen~f~\AElse~g~\AFi}{Q}{M}$}
    \DisplayProof
  }

  \subcaptionbox{While rule\label{fig:hoare-loop}}{
    \AxiomC{$\htriple*{I\wedge B}f{I'}{M}$}
    \AxiomC{$I'\longrightarrow I$}
    \AxiomC{$I\wedge\neg b\longrightarrow Q$}
    \TrinaryInfC{$\htriple*{I}{\texttt{While}~B\texttt{ DO }f\texttt{ OD}}{Q}{M}$}
    \DisplayProof
  }
  \hfill
  \subcaptionbox{Skip rule}{
    \AxiomC{$M=\varnothing$}
    \AxiomC{$P\longrightarrow Q$}
    \BinaryInfC{$\htriple*{P}\ASkip{Q}{M}$}
    \DisplayProof
  }\hspace*\fill

  \subcaptionbox{Resume rule}{
    \AxiomC{$\forall {0\leq j\leq n}\cdot\htriple*{P}{a_j}{Q_j}{M_j}$}
    \AxiomC{$(\bigvee_{0\leq j\leq n} Q_j)\longrightarrow Q$}
    \AxiomC{$M = \bigcup_{0\leq j\leq n}M_j$}
    \TrinaryInfC{$\htriple*{P}{\AWhileResume\{(i_0,a_0),\dotsc,(i_n, a_n)\}}{Q}{M}$}
    \DisplayProof
  } 

  \hspace*\fill
  \subcaptionbox{Precondition weakening\label{fig:hoare-weaken}}{
    \AxiomC{$P\longrightarrow P'$}
    \AxiomC{$\htriple{P'}b{Q}{M}$}
    \BinaryInfC{$\htriple{P}b{Q}{M}$}
    \DisplayProof
  }
  \hfill
  \subcaptionbox{Postcondition strengthening}{
    \AxiomC{$Q'\longrightarrow Q$}
    \AxiomC{$\htriple*{P}b{Q'}{M}$}
    \BinaryInfC{$\htriple*{P}b{Q}{M}$}
    \DisplayProof
  }\hspace*\fill
  \caption{Hoare rules for memory usage}\label{fig:rules}
\end{figure*}
While the introduction rule for basic blocks
is the ultimate target of our Hoare rule application process,
the rest of the rules are required to decompose the syntax above the level of blocks.
The remainder of \cref{fig:rules} describes those additional rules.
The sequence, conditional, and resume-rules are straightforward:
the ultimate memory usage is the union of the memory usage of their constituents.
Note that the sequence rule is sound only because the memory predicates
are independent of state as discussed in \cref{sse:per-block}.

The while rule is based on a loop invariant,~$I$.
If the memory usage of one iteration of loop body~$f$
is constrained to set of memory regions~$M$,
then so is the memory usage of every other iteration.
This may sound counterintuitive,
so consider a simple C-like loop that starts from $i=0$ and iterates while $i<10$.
The body of this example loop contains single-byte array assignment operations
along the lines of $a[i]=v$.
Verification of the loop requires the loop invariant $I(\sigma)=i(\sigma)<10$.
The \ac{fmuc} of the loop body will have, as a state-dependent set of memory regions, $M(\sigma)=\{\region{a+i(\sigma)}1\}$, which is a single region of one byte.
If the Hoare logic introduction rule were to be applied to the block
that is the body of the loop, the result would be as follows:
\begin{subequations}
  \begin{align}
    M' &= \{r\mid\exists\sigma\cdot I(\sigma)\wedge r\in M(\sigma)\} \\
    &= \{r\mid\exists\sigma\cdot i(\sigma)<10\wedge r=\region{a+i(\sigma)}1\} \\
    &= \{\region{a'}1\mid a\leq a'\le a+10\}
  \end{align}
\end{subequations}
The set~$M'$ contains the regions of memory used by the entire loop,
not just one iteration.
This is because the introduction rule applies the state-dependent set of memory regions
to any state that satisfies the invariant.
This shows that the strength of the generated invariants directly influences the tightness of the overapproximation of memory usage.
A weaker invariant, such as $i<20$, would result in a larger set of memory regions.

\subsubsection{The Introduction Rule}
This rule, depicted in \cref{fig:intro-rule},
is the rule that ties the per-block verification to the function-body verification.
The first assumption requires the symbolic execution method be run
from a universally quantified initial symbolic state~$\sigma$ that satisfies the precondition.
As long as any resulting state~$\sigma'$ satisfies the postcondition~$Q$,
the set of memory regions~$M$ generated for the block should be correct.

The second assumption is required because of an important subtlety:
the regions generated in the~\ac{fmuc} are state dependent.
Consider the example in \cref{fig:fmuc}.
Memory region set~$M$ is actually a function based on the initial state of the block:
its regions depend on the values stored in memory.
However, it makes no sense to express the regions used by individual blocks
within a larger function in terms of their own individual initial state alone.
If a region of a basic block somewhere within a function body depends on
something such as the value of register $\mathrdi$ at the start of that block,
then it would be unsound to express that memory region in terms of $\rdio$,
the value of $\mathrdi$ at the start of the function.
As such,
the Hoare triples are defined over a state-independent set of memory regions,~$M'$.
That set is obtained for each block by taking the generated state-dependent set of memory regions
and applying that set to any state that satisfies the current invariant.


\subsubsection{Verification Condition Generation}\label{sse:vcg}
The \ac{vcg} presented here is also a set of proof methods
implemented in Eisbach.
It is designed to automatically apply the proper Hoare logic rules
as much as possible,
driven by the formal \ac{scf} provided by the \ac{fmuc}.
While it does allow for additional user interaction as necessary,
my experience with function bodies without loops
has been that minimal further interaction is required.
For the case where loops are present,
the \ac{vcg} provides an alternate \lstinline|vcg_while| method
that relies on the loop rule presented in \cref{fig:hoare-loop}.
That loop rule is structured such that
the majority of work required to support the loops
is identifying the preconditions of their exit blocks
and then supplying their disjunction to \lstinline|vcg_while|.
This method relies on application of the weakening rule
presented in \cref{fig:hoare-weaken}
to show that the postcondition of the block before entry implies the loop invariant.

Without exception, each of the proofs we produced
could be finished using standard, off-the-shelf Isabelle/HOL methods,
though finishing them was not always an automatic process.
The part that is usually the most involved,
defining the invariants (as seen in the previous chapter)
is taken care of by the \ac{fmuc} generation.

\subsection{Composition}\label{sse:fmuc_comp}
In order to achieve a scalable verification methodology,
it must support some form of compositionality.

Consider the body of an already-verified function~$f$
with the following Hoare triple:
\begin{equation*}
  \htriple*{P_f}f{Q_f}{M_f}.
\end{equation*}
In order to reuse that function's proof in a compositional fashion,%
\index{function!composition}
it is treated as a black box.%
\index{function!black box}
Now consider the assembly of a function~$g$ that calls~$f$:
\begin{lstlisting}[style=x64, gobble=2, numbers=none]
  a0: push rbp
  a1: call f
  a2: pop  rbp
  a3: ret
\end{lstlisting}
$P$ and~$Q$ are the pre- and postconditions just before executing \inlineasm{call}
and just after it returns.
$P$ contains the equality $\readmem{\rspo^g-8}{8}=\rbpo^g$,
expressing that~$g$ has pushed the frame pointer \inlineasm{rbp}%
\index{frame!pointer}
into its own local stack frame.%
\index{stack!frame}
The ultimate postcondition of~$g$
expresses that the callee-saved register \inlineasm{rbp} is properly restored:%
\index{register!callee-saved}
$\mathrbp=\rbpo^g$.
That operation is indeed performed by \inlineasm{pop rbp}.
In order to prove proper restoration of \inlineasm{rbp},
a proof that function~$f$ did not overwrite any byte in the region%
\index{memory!region}
$\region{\rspo^g-8}{8}$ is required.
The proof must also show that~$f$ does not overwrite region $\region{\rspo^g}{8}$,
which stores the address~$g$ returns to.
That proof would be specific to this particular instance of calling~$f$.

Of course,~$g$ may not be the only function that calls~$f$.
It may even be called multiple times by the same function.
Every call has specific requirements on which memory regions must be preserved,
based on the calling context.
Thus, to be able to verify function~$f$ once
but reuse its proof for each call,
the proof must at least contain an overapproximation
of the memory written to by function~$f$, .
This is exactly what \emph{separation
logic}~\citep{o2001local,reynolds2002separation,krebbers2017essence}%
\index{separation logic}
requires.
As described in \cref{sse:composition},
separation logic provides a \emph{frame rule} for compositional reasoning.%
\index{separation logic!frame rule}
Informally, this rule states that, if a program can be confined
to a certain part of state, properties of that program will carry over
when the program is used as part of a bigger system.

In order to achieve that same behavior specifically for memory usage verification,%
\index{memory!usage}
we developed the frame rule presented in \cref{fig:composition}.
This rule is used to prove that the memory usage of a caller function~$g$
is equal to the memory it itself uses, \emph{plus} the memory used by function~$f$.
It must have the following four assumptions.
First, that~$f$ has been verified for memory usage,
with~$M_f$ denoting the memory regions~$f$ uses.%
\index{memory!region}
Second, that precondition~$P$ can be split up into two parts:
precondition~$P_f$, required to verify~$f$, and a separate part~$\psep$.
The separate part is specific to the specific call of the function
where the frame rule is applied.%
\index{separation logic!frame rule}
In the example above,~$P_\mathrm{sep}$ must contain the equality
$\region{\rspo^g-8}{8}=\rbpo^g$.
Third, the correctness of~$M_f$, the set of memory regions,
should suffice to prove that~$\psep$ is preserved.
This effectively means that, for the above example,~$M_f$
should not overlap with the two regions of~$g$.
Fourth and finally,~$\psep$ and~$Q_f$ should imply postcondition$~Q$.
\begin{figure*}
  \begin{prooftree}
    \AxiomC{$\htriple*{P_f}{f}{Q_f}{M_f}$}
    \AxiomC{$P\longrightarrow P_f\wedge\psep$}
    \AxiomC{$\forall s~s'\cdot\begin{array}{l}
      \usage(M_f,s,s')\wedge{} \\
      \psep(s)\longrightarrow\psep(s')
    \end{array}$}
    \AxiomC{$Q_f\wedge\psep\longrightarrow Q$}
    \QuaternaryInfC{$\htriple*{P}{\ACall f}{Q}{M_f}$}
  \end{prooftree}
  \caption{Frame rule for composition of memory usage}\label{fig:composition}
\end{figure*}

In practice, many functions will not be part of the assembly code under verification, such as dynamic library or system calls.
Those cases necessitate generating the assumptions required
to proceed with verification.
The following box notation supports those cases:
\begin{equation}
  \htriple*P{\fbox f}Q{M_f}\equiv
  \exists P_f~Q_f~\psep\cdot
  \text{all four assumptions of the frame rule are satisfied}.
\end{equation}
This assumption informally expresses that function~$f$ has been verified.
Its memory usage~$M_f$ is assumed to suffice to prove that
the states that satisfy precondition~$P$ lead to the states that satisfy
postcondition~$Q$.

\section{Full Example}\label{se:syntax_example}
This section presents an execution of the entire toolchain
on the example given in \cref{fig:example2-c}
as a summary of \cref{se:fmuc_gen,se:fmuc_ver}.
The~C code is provided solely for presentation,
as the only inputs to the \ac{fmuc} generation
are the assembly created by disassembling the corresponding binary%
\index{assembly}%
\index{assembly!dis-}%
\index{binary}
and a basic configuration file indicating which functions to analyze.
\Cref{fig:example2-scf} presents the generated \ac{scf}.
The example has one loop, which starts at instruction address \texttt{0x120}.%
\index{loop}
\begin{figure}
  \begin{subfigure}[b]{.53\linewidth}
    \begin{lstlisting}[style=C, gobble=6]
      int main(int argc, char* argv[]) {
          int* a = (int*)argv;
          int* b = (int*)(argv + 4);
          *(int*)(argv + 2) = *a + *b;
          *(char*)argv = 'a';

          int array[argc];
          for (int i = 0; i < argc; i++)
              array[i] = argv[i][0] * 2;

          if (is_even(argc))
              return array[argc];
          return array[0];
      }
    \end{lstlisting}
    \caption{C code}\label{fig:example2-c}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{.46\linewidth}
    \begin{equation*}
      \begin{array}{l}
        \ABB~\Block{1149}{120b}\ASeq* \\
        \AWhile \\
        \ind{2ex}\ABB~\Block{123e}{1244}\ASeq* \\
        \ind{2ex}\AIf~\var{SF}\neq\var{OF}~\AThen~\ABB~\Block{120d}{123a} \\
        \ind{4ex}\AElse~\ABreak~\AFi \\
        \AOd\ASeq* \\
        \ABB~\Block{1246}{1249}\ASeq* \\
        \ABB~\Block{124b}{124b}\ASeq* \\
        \ABB~\Block{1250}{1252}\ASeq* \\
        \AIf~\var{ZF}~\AThen~\ABB~\Block{1263}{1267} \\
        \ind{2ex}\AElse~\ABB~\Block{1254}{1261}~\AFi\ASeq* \\
        \ABB~\Block{1269}{1279}\ASeq* \\
        \AIf~\var{ZF}~\AThen~\ABB~\Block{1280}{1285} \\
        \ind{2ex}\AElse~\ABB~\Block{127b}{127b}~\AFi
      \end{array}
    \end{equation*}
    \caption{Syntactic control flow for the assembly}\label{fig:example2-scf}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \begin{align*}
      M &=\{r_0=\region{\rspo}8, r_1=\region{\fso+40}8, r_2=\region{\rsio+36}4,
            r_3=\region{\rspo-8}8,\dotsc\} \\
      \var{MRR} &= \{r_0,r_1,r_2,r_3,\dotsc,r_{12}\}\text{ are separate}
    \end{align*}
    \caption{Some memory regions and their relations for block $\Block{123e}{1244}$}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \begin{equation*}
      P_\mathtt{123e}(\sigma)=\begin{aligned}
        \mathrip            &= \mathtt{0x123e} \\
        \mathrbp            &= \rspo-8 \\
        \mathrdi            &= \rdio \\
        \mathrsp            &= \rspo-(88+16*((15+4*
          \sextend(\takebits{31,0}\rdio)/16)) \\
        \readmem{\rspo-40}8 &= \rspo-(85+16*((15+4*
          \sextend(\takebits{31,0}\rdio))/16))\gg 2\ll 2 \\
        \readmem{\rspo-48}8 &= \sextend(\takebits{31,0}\rdio)-1 \\
        \readmem{\rspo-56}8 &= \rsio+32
      \end{aligned}
    \end{equation*}%
    \nomenclature{$\gg$}{Performs unsigned right shift when used with word values}%
    \nomenclature{$\ll$}{Performs left shift when used with word values}
    \caption{Invariant for address \texttt{0x123e}
      (only 7 out of 23 equalities shown)}\label{fig:example2-inv}
  \end{subfigure}
  \\[1em]
  \begin{subfigure}[b]{.38\linewidth}
    \begin{equation*}
      \htriple*{P_\mathtt{124b}}{\fbox{\texttt{is\_even}}}
      {P_\mathtt{1250}}{M_\mathtt{is\_even}}
    \end{equation*}
    \caption{Assumption due to call of \lstinline|is_even|}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{.56\linewidth}
    \begin{lstlisting}[gobble=6]
      apply check_scf_step+
      apply (check_scf_while \<open>P_123e || P_1246\<close>)
      apply check_scf_step+
    \end{lstlisting}
    \caption{Isabelle proof code (manual effort)}\label{fig:manual}
  \end{subfigure}
  \caption{Application of entire methodology on example}
\end{figure}
Zooming in on $\ABB~\Block{123e}{1244}$, we see from \cref{fig:example2-inv}
that the \ac{fmuc} provides 13 regions, of which four are shown.
Region~$r_0$ stores the return address
while region~$r_1$ depends on the segment register~\inlineasm{fs}
and stores the canary value\index{stack!canary}
used to detect stack buffer overflows~\citep{cowan1998stackguard}.%
\index{stack!buffer overflow}
Region~$r_2$ is based on the pointer passed as the second argument to the function,
and region~$r_3$ is part of the stack frame.
The generated \acp{mrr} assume that all these regions are separate.

The precondition assigned to $\ABB~\Block{123e}{1244}$
is effectively a loop invariant (see \cref{fig:example2-inv}).%
\index{loop!invariant}
The frame pointer \lstinline|rbp|%
\index{frame!pointer}
is equal to the original stack pointer minus eight.%
\index{stack!pointer}
Register \lstinline|rdi| has not been touched.
Some of the more complex assignments are also shown,
such as the current value of the stack pointer.
In total, the loop invariant provides information
on 11 registers and 12 memory locations for this basic block.%
\index{basic block}
The process of verification shows that,
for any state satisfying this invariant,
executing one iteration of the loop body
will result in a state that again satisfies the loop invariant.
The only interactions required in verifying the \ac{fmuc} of the entire function are:
\begin{enumerate*}
  \item showing that the postcondition after $\ABB~\Block{1149}{120b}$
  implies the loop invariant, and
  \item showing that, in the case of a break, the postcondition of the loop body
  implies the precondition of $\ABB~\Block{1246}{1249}$.
\end{enumerate*}
This amounts to two manually written lines of Isabelle proof code.  

To demonstrate the black-box functionality from \cref{sse:fmuc_comp},
\lstinline|is_even| was treated as external to the example's analysis.
This resulted in the generation of an assumption
stating that the memory usage of \lstinline|is_even| suffices to show that
the invariant for the call site (instruction address $\mathtt{124b}$)
implies the invariant for the instruction address immediately following,
$\mathtt{1250}$.
This means that $M_\mathtt{is\_even}$
is assumed to not overlap with regions~$a$ through~$d$, among other things.

\Cref{fig:manual} shows the sole manual effort required
to prove the \ac{fmuc} for this function.
All it involves is calling the proper predefined Eisbach~\citep{matichuk2016eisbach}
proof methods.
The first proof method applied is \lstinline|check_scf_step|,
which applies Hoare rules and proves correctness of all memory usage
up until the loop.
Following that, the proof method for dealing with loops
is called with the invariant formed from the disjunction
of the precondition of the loop's entry block
and the precondition of the loop's exit block,
both identified from the generated \ac{scf}.
As the last manual step, \lstinline|check_scf_step| is once again called
and is able to verify the remainder of the function.

Finally, note that, without any assumptions,
the function could overwrite its own return address at various places.
The \acp{mrr} are strong enough to exclude this scenario.
Those relations thus form the preconditions
under which a return-address exploit is impossible.%
\index{return-address exploit}
For example, they assume that regions~$a$ and~$c$ are separate.
This means that the address stored in argument \lstinline[style=C]|argv|
(mapped to $\rsio$ on the assembly level)
is not allowed to point to a region
within the stack frame of the \lstinline[style=C]|main| function.

\section{Application: Xen Project}\label{se:xen}
The Xen Project~\citep{chisnall2008definitive}%
\index{Xen}
is a mature, widely-used \ac{vmm}, also known as a \emph{hypervisor}.%
\index{hypervisor}
Hypervisors provide a method of managing multiple
\acp{vm} (called domains in the Xen documentation) on a physical host.%
\index{domain}
%Xen has support for hardware-assisted virtualization, referred to as \acp{hvm}.
% Relevant because of QEMU

The Xen hypervisor is a suitable case study because of its security relevance%
\index{Xen}
and its complex build process involving real production code.
Security is a significant issue in environments where hypervisors are used,
such as the \ac{aec2}, Rackspace Cloud, and many other cloud service providers.
For example, when one or more hosts support guest domains
for any number of distinct users,
ensuring isolation of the domains is important.

The Xen build process produces multiple binaries
that contain functions not present in the Xen source itself.
This is due to the inclusion of external static libraries and programs.
Xen version 4.12 was compiled with \ac{gcc} 8.2 via the standard Xen build process.
This build process uses various optimization levels,
ranging from~\texttt{O1} to~\texttt{O3}.
The version of \texttt{objdump} used to disassemble the compiled binaries was 2.31.1.%
\index{\texttt{objdump}}%
\index{assembly!dis-}

The verification effort presented here
covered three of the binaries produced by the Xen build process:
\lstinline|xenstore|, \lstinline|xen-cpuid|, and \lstinline|qemu-img-xen|.
The \lstinline|xenstore| binary is involved in the functionality of
XenStore\fturl{https://wiki.xen.org/wiki/XenStore},
a hierarchical data structure shared amongst all Xen domains.
This sharing allows for the possibility of inter-domain communication,
though in general XenStore is intended for simple configuration information.
A smaller program than \lstinline|xenstore|, \lstinline|xen-cpuid|
provides functionality similar to that of the
\lstinline|cpuid| utility\fturl{https://linux.die.net/man/1/cpuid}.
This utility queries the underlying processors
and displays information about the features they support.
Such functionality is important for Xen
as it supports migrating domains
between processors with different variants of the same \ac{isa}~\citep{cpuid-masking}.
The third binary used, \lstinline|qemu-img-xen|,
consists of over three hundred functions
that are not present in the Xen source code.
It provides some of the functionality of \ac{qemu}.
\Ac{qemu} is a free, open-source emulator\fturl{https://www.qemu.org/}.%
\index{emulator}
Xen uses it to emulate \acp{dm}, which provide interfaces for hardware storage.

\begin{table*}
  \sisetup{table-format=5.0, table-number-alignment=right}
  \centering
  \begin{tabular}{lrSSS}
    \toprule
    Binaries & Function Count & {Instruction Count} & Loops &
      {Manual Lines of Proof} \\
    \midrule
    \lstinline|xenstore| & 2/6 & 100 & 0 & 6 \\
    \lstinline|xen-cpuid| & 2/3 & 210 & 2 & 39 \\
    \lstinline|qemu-img-xen| & 247/343 & 11942 & 64 & 1002 \\
    Total & 251/352 & 12252 & 65 & 1047 \\
    \bottomrule
  \end{tabular}
  \caption{Verified Xen Functions}\label{func-counts}
\end{table*}
\begin{figure*}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.98\linewidth,
      height=\linewidth,
      ybar,
      ylabel=Counts and Percentages,
      bar width=0.3,
      nodes near coords, % causes build failure when combined with symbolic x coords
      point meta=y/3.52, % can't get \% shown right
      enlarge y limits={value=0.2, upper},
      ymin=-20,
      xticklabels={
        Verified,
        Indirection,
        \begin{tabular}{c}Address\\Computation\end{tabular},
        \texttt{repz cmps},
        Recursion,
        \acs{scf} explosion
      },
      xtick=data
    ]
    \addplot coordinates {
      (0, 251)
      (1, 66)
      (2, 19)
      (3, 10)
      (4, 2)
      (5, 4)
    };
    \end{axis}
  \end{tikzpicture}
  \caption{Analzyed Xen functions compared to unverified features}
  \label{fig:unverified}
\end{figure*}

This methodology is currently capable of dealing with \xenpercentage\
of the functions present in the aforementioned binaries (see \cref{fig:unverified}).
The supported features include (nested) loops,
subcalls, variable argument lists, jumps into other function bodies,
string instructions with the \texttt{rep} prefix, and \ac{simd} instructions.
There is no particular limit on function size.
The average number of instructions per function analyzed is 49.
Some of the functions analyzed have over 300 instructions and over 100 basic blocks.

There are five categories of features not currently supported.
The first and most common, previously mentioned in \cref{sse:cfg_extract},
is \emph{indirection}, accounting for \SI{19}{\percent}.%
\index{indirection}
Indirection involves a call or jump instruction
that loads the target address from a register or memory location
rather than using a static value.
Switch statements and certain uses of \texttt{goto}
are the most common causes of indirect jumps.
Indirect calls generally result from usage of function pointers.
For example, the \lstinline|main| functions of all three verified binaries
used switch statements in loops in the process of parsing command line options.
These statements introduced indirect branches.

The second category involves issues related to generating the \acp{mrr}.
This step requires solving linear arithmetic over symbolically computed addresses.%
\index{linear arithmetic}
Sometimes, addresses are computed using a combination of arithmetic operators%
\index{operator!arithmetic}
with bitwise logical operators.%
\index{operator!bitwise}
In some of these cases, our translation to Z3 does not produce an answer.%
\index{Z3}
As an example, function \texttt{qcow\_open}
uses the rotate-left function to compute an address.
As another example, function \texttt{AES\_set\_encrypt\_key}
produces addresses that are obtained via combinations of bit-shifting,
bit masking, and \texttt{xor}-ing.
For these cases, separation and enclosure relations cannot be generated.

The instruction \texttt{repz cmps} is currently not supported for technical reasons.
It is the assembly equivalent of the function \texttt{strncmp},
but instead writes its result to a flag.
Various other string-related instructions with the \texttt{rep} prefix are supported,
however.

Functions with \emph{recursion}, a minority in systems code, are also not supported
as they are not well-suited to automation in this framework.
The two recursive functions encountered in the analyzed Xen binaries
both perform file-system-like tasks.
Functions \lstinline|do_chmod| and \lstinline|do_ls|
are similar to the permission-setting \lstinline|chmod| utility
and the directory-displaying \lstinline|ls|, respectively.

The final category is functions whose \ac{scf} explodes.
The issue can occur when the pattern in \cref{fig:ex_nonopt} shows up extensively
or when while loops have multiple entry points.

\Cref{func-counts} provides an overview of the verification effort.
The table shows the absolute counts of functions verified
as well as the total number of instructions for those functions.
Alongside that information is the number of functions with loops
that were verified and how many manual lines of proof were required in total.
The vast majority of those manual proof lines were related to the loop count.
Meanwhile, a comparison with those functions not verified
can be found in \cref{fig:unverified}.

\section{Summary}
This chapter presented an approach to formal verification of memory usage
for functions in a disassembled program,
an overapproximation of the memory used by that assembly code.
The approach automatically generates \iac{fmuc} that includes
\begin{enumerate*}
  \item a set of memory regions read from and written to,
  \item preconditions necessary for formal verification,
  \item postconditions that express sanity constraints over the function
  (the return address has not been overwritten,
  callee-saved registers are restored, etc.), and
  \item proof ingredients.
\end{enumerate*}
The certificate is loaded into a theorem prover, where it can be verified.
The proof ingredients, combined with custom proof methods,
provide a large degree of automation.
They deal with memory aliasing and provide both the control flow of the function
as well as invariants.

The approach was applied to three binaries produced by the Xen hypervisor build process.
They contain, among other things, nested loops,
complex data structures, variadic functions,
and both internal and external function calls.
A certificate could be generated and verified
for \xenpercentage\ of the functions from those binaries.
The amount of user interaction was roughly 85 lines of proof code
per \num{1000} lines of assembly code.
The greatest issue was indirect branching,
which could be found in 19\% of the functions examined.
